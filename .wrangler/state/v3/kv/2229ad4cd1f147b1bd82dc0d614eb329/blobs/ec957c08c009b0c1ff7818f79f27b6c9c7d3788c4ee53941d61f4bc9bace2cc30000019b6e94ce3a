[{"id":"228754181994609664","type":"news","url":"https://www.aibase.com/zh/news/24135","title":"​法拉第未来计划更名，引入人工智能以提升品牌形象","description":"法拉第未来（Faraday Future）近日宣布，将于2026年2月13日召开特别股东大会，届时将就多项提案进行投票。公司特别强调，股东们应支持所有提案，尤其是涉及公司名称变更的部分。 根据提案，公司计划将现有名称 “Faraday Future Intelligent Electric” 更改为 “Faraday Future AI Electric Vehicle Inc.”（法拉第未来人工智能电动汽车公司）。这一新名称旨在彰显法拉第未来在智能电动汽车(EV)领域的领先地位，并体现其在人工智能驱动的创新解决方案与智慧交通生态方面的长期承诺。法拉第未来表示，新名称将更好地反映公司在人工智能与智能出行、车辆系统及用户体验等核心技术上的战略融合。 此外，更名还将与公司新的股票代码 “FFAI” 相呼应，提升品牌形象的一致性。这一系列举措意在帮助公司在竞争激烈的电动车市场中脱颖而出。 在股东大会上，公司还将提出股份授权提案，计划将普通股的授权股数量增加至312，285，439股，并将优先股的授权股份数量增加至24，087，265股。这意味着，法拉第未来的普通股和优先股总授权股份将增至336，372，704股，增幅达到约34%。这一提案的通过将为公司的进一步发展提供必要的资金支持。 法拉第未来期待通过此次特别股东大会，获得股东们的全力支持，以实现公司的未来愿景和发展战略。 划重点: 🌟 法拉第未来计划于2026年召开特别股东大会，提案将公司名称更改为 “法拉第未来人工智能电动汽车公司”。 📈 公司还提出增加普通股和优先股的授权数量，旨在支持未来的发展需求。 🤖 新名称将突出公司在人工智能和智能电动汽车领域的战略重点，提升品牌形象一致性。","published_date":"2025-12-30T08:57:02.239Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>法拉第未来（Faraday Future）近日宣布，将于2026年2月13日召开特别股东大会，届时将就多项提案进行投票。公司特别强调，股东们应支持所有提案，尤其是涉及公司名称变更的部分。</p><p>根据提案，公司计划将现有名称 “Faraday Future Intelligent Electric” 更改为 “Faraday Future AI Electric Vehicle Inc.”（法拉第未来人工智能电动汽车公司）。这一新名称旨在彰显法拉第未来在智能电动汽车(EV)领域的领先地位，并体现其在人工智能驱动的创新解决方案与智慧交通生态方面的长期承诺。法拉第未来表示，新名称将更好地反映公司在人工智能与智能出行、车辆系统及用户体验等核心技术上的战略融合。</p><p>此外，更名还将与公司新的股票代码 “FFAI” 相呼应，提升品牌形象的一致性。这一系列举措意在帮助公司在竞争激烈的电动车市场中脱颖而出。</p><p>在股东大会上，公司还将提出股份授权提案，计划将普通股的授权股数量增加至312，285，439股，并将优先股的授权股份数量增加至24，087，265股。这意味着，法拉第未来的普通股和优先股总授权股份将增至336，372，704股，增幅达到约34%。这一提案的通过将为公司的进一步发展提供必要的资金支持。</p><p>法拉第未来期待通过此次特别股东大会，获得股东们的全力支持，以实现公司的未来愿景和发展战略。</p><blockquote><p>划重点:</p><p>🌟 法拉第未来计划于2026年召开特别股东大会，提案将公司名称更改为 “法拉第未来人工智能电动汽车公司”。  </p><p>📈 公司还提出增加普通股和优先股的授权数量，旨在支持未来的发展需求。  </p><p>🤖 新名称将突出公司在人工智能和智能电动汽车领域的战略重点，提升品牌形象一致性。</p></blockquote>"}},{"id":"228754181994609665","type":"news","url":"https://www.aibase.com/zh/news/24134","title":"教育部:加快建设自主可控教育大模型，构建全域贯通的数据支撑体系","description":"教育部教育数字化专家咨询委员会主任杨宗凯在30日的新闻发布会上强调，要在“十五五”时期及未来持续保持全球领先地位，关键在于实现平台、数据、AI工具与政策的高度一致性，以此深度重塑教育新生态。 杨宗凯指出，深化改革的首要任务是深入推进集成化，通过持续建设国家教育大数据中心，构建起覆盖招生、教学、管理到就业的全域贯通体系，在实现数据“一数一源”高效共享的同时，加快研发中国自主可控的教育人工智能大模型，并辅以严谨的安全伦理长效机制。在全面推进智能化方面，应坚持“智能向善”与立德树人的核心理念，将AI技术融入教育教学全要素，以智能化升级推动大规模因材施教，培养适应时代需求的创新型人才。 [图片: AI，人工智能 https://pic.chinaz.com/picmap/202412271635326771_0.jpg] 此外，提升育人效能的核心在于师生数字素养的全面进阶。杨宗凯提出要将数字化赋能教师发展作为长期任务，统筹大中小学一体化的人工智能通识教育，并将数字胜任力正式纳入学生综合素质评价。最后，我国将大力推进教育数字化国际化，通过建强国家智慧教育平台国际版、推动世界数字教育联盟实体化运行等举措，深度参与全球治理，在国际数字教育议题中贡献中国智慧并提升话语权。","published_date":"2025-12-30T08:41:33.164Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>教育部教育数字化专家咨询委员会主任杨宗凯在30日的新闻发布会上强调，要在“十五五”时期及未来持续保持全球领先地位，关键在于实现平台、数据、AI工具与政策的高度一致性，以此深度重塑教育新生态。</p><p>杨宗凯指出，深化改革的首要任务是深入推进集成化，通过持续建设国家教育大数据中心，构建起覆盖招生、教学、管理到就业的全域贯通体系，在实现数据“一数一源”高效共享的同时，加快研发中国自主可控的教育人工智能大模型，并辅以严谨的安全伦理长效机制。在全面推进智能化方面，应坚持“智能向善”与立德树人的核心理念，将AI技术融入教育教学全要素，以智能化升级推动大规模因材施教，培养适应时代需求的创新型人才。</p><p style=\"text-align: center\"><img src=\"https://pic.chinaz.com/picmap/202412271635326771_0.jpg\" title=\"AI，人工智能 (图片来源：AI合成)\" alt=\"AI，人工智能\"></p><p>此外，提升育人效能的核心在于师生数字素养的全面进阶。杨宗凯提出要将数字化赋能教师发展作为长期任务，统筹大中小学一体化的人工智能通识教育，并将数字胜任力正式纳入学生综合素质评价。最后，我国将大力推进教育数字化国际化，通过建强国家智慧教育平台国际版、推动世界数字教育联盟实体化运行等举措，深度参与全球治理，在国际数字教育议题中贡献中国智慧并提升话语权。</p>"}},{"id":"228754181994609666","type":"news","url":"https://www.aibase.com/zh/news/24132","title":"腾讯混元发布1.5版开源翻译模型:端侧部署性能跃升，效果比肩超大型闭源模型","description":"腾讯混元今日宣布正式开源其翻译模型1.5版本。本次更新共包含两个不同尺寸的模型: Tencent-HY-MT1.5-1.8B 和 Tencent-HY-MT1.5-7B ，旨在通过 极致 的效率与领先的翻译质量，重新定义端云协同的翻译体验。 [图片: image.png https://upload.chinaz.com/2025/1230/6390270667310847734235147.png] 核心亮点:端侧部署与卓越性能 本次发布的 1.8B 模型 表现尤为亮眼。作为一款面向手机等消费级设备设计的轻量化模型，它在经过量化处理后，仅需 1GB 内存 即可实现离线流畅运行。 极致 速度 :处理50个 tokens 的平均耗时仅为 0.18秒 ，远快于主流商用翻译 API 的0.4秒。 跨级表现 :在 FLORES-200等 权威 测试集中，其效果达到了 Gemini-3.0-Pro 等超大尺寸闭源模型的90分位水平，全面超越了中等尺寸开源模型。 [图片: image.png https://upload.chinaz.com/2025/1230/6390270669017117825709840.png] 全面覆盖:从主流语种到方言民汉 混元翻译模型1.5支持包括中、英、日、法等 33个全球语种 的互译，并特别加强了对捷克语、爱沙尼亚语、冰岛语等小语种的支持。此外，模型还涵盖了 5种国内民汉语言及方言 ，极大拓宽了 AI 翻译的应用边界。 功能进化:更具实操性的翻译体验 针对实际应用场景，1.5版本在三个维度进行了重磅升级: 自定义术语库 :用户可针对医学、法律、金融等专业领域上传术语表，确保专业词汇翻译的一致性。 上下文理解 :具备先进的长文本对话理解能力，能基于前文语境优化后续结果，避免语义断裂。 格式保持能力 :通过精准的指令遵循，模型可以在翻译后完美保持原始文本（如网页、代码、Markdown）的格式。 技术突破:大模型引导小模型 HY-MT1.5-1.8B 之所以能以小博大，得益于腾讯采用的 On-Policy Distillation（大尺寸模型蒸馏） 策略。由7B 尺寸的“老师”模型实时引导“学生”模型，帮助其从预测偏移中学习，而非单纯死记硬背答案，从而显著提升了小模型的逻辑与翻译能力。 开发者生态:多平台全面支持 目前，腾讯混元翻译模型1.5均已在腾讯混元官网上线，并在Github及HuggingFace开源社区开放下载。模型已适配 Arm、高通、Intel 及沐曦等主流计算平台。 从腾讯会议到企业微信，腾讯混元翻译技术已在内部多个高并发场景落地。随着1.5版本的开源，腾讯正进一步推动高质量 AI 翻译技术走向普惠，为全球开发者提供更具性价比的翻译方案。","published_date":"2025-12-30T07:51:39.028Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>腾讯混元今日宣布正式开源其翻译模型1.5版本。本次更新共包含两个不同尺寸的模型:<strong>Tencent-HY-MT1.5-1.8B</strong> 和 <strong>Tencent-HY-MT1.5-7B</strong>，旨在通过<span>极致</span>的效率与领先的翻译质量，重新定义端云协同的翻译体验。</p><p style=\"text-align:center\"><img src=\"https://upload.chinaz.com/2025/1230/6390270667310847734235147.png\" title=\"image.png\" alt=\"image.png\"></p><h3>核心亮点:端侧部署与卓越性能</h3><p>本次发布的 <strong>1.8B 模型</strong> 表现尤为亮眼。作为一款面向手机等消费级设备设计的轻量化模型，它在经过量化处理后，仅需 <strong>1GB 内存</strong> 即可实现离线流畅运行。</p><ul><li><p><strong><span>极致</span>速度</strong>:处理50个 tokens 的平均耗时仅为 <strong>0.18秒</strong>，远快于主流商用翻译 API 的0.4秒。</p></li><li><p><strong>跨级表现</strong>:在 FLORES-200等<span>权威</span>测试集中，其效果达到了 Gemini-3.0-Pro 等超大尺寸闭源模型的90分位水平，全面超越了中等尺寸开源模型。</p></li></ul><p style=\"text-align:center\"><img src=\"https://upload.chinaz.com/2025/1230/6390270669017117825709840.png\" title=\"image.png\" alt=\"image.png\"></p><h3>全面覆盖:从主流语种到方言民汉</h3><p>混元翻译模型1.5支持包括中、英、日、法等 <strong>33个全球语种</strong> 的互译，并特别加强了对捷克语、爱沙尼亚语、冰岛语等小语种的支持。此外，模型还涵盖了 <strong>5种国内民汉语言及方言</strong>，极大拓宽了 AI 翻译的应用边界。</p><h3>功能进化:更具实操性的翻译体验</h3><p>针对实际应用场景，1.5版本在三个维度进行了重磅升级:</p><ol><li><p><strong>自定义术语库</strong>:用户可针对医学、法律、金融等专业领域上传术语表，确保专业词汇翻译的一致性。</p></li><li><p><strong>上下文理解</strong>:具备先进的长文本对话理解能力，能基于前文语境优化后续结果，避免语义断裂。</p></li><li><p><strong>格式保持能力</strong>:通过精准的指令遵循，模型可以在翻译后完美保持原始文本（如网页、代码、Markdown）的格式。</p></li></ol><h3>技术突破:大模型引导小模型</h3><p>HY-MT1.5-1.8B 之所以能以小博大，得益于腾讯采用的 <strong>On-Policy Distillation（大尺寸模型蒸馏）</strong> 策略。由7B 尺寸的“老师”模型实时引导“学生”模型，帮助其从预测偏移中学习，而非单纯死记硬背答案，从而显著提升了小模型的逻辑与翻译能力。</p><h3>开发者生态:多平台全面支持</h3><p>目前，腾讯混元翻译模型1.5均已在腾讯混元官网上线，并在Github及HuggingFace开源社区开放下载。模型已适配 Arm、高通、Intel 及沐曦等主流计算平台。</p><p>从腾讯会议到企业微信，腾讯混元翻译技术已在内部多个高并发场景落地。随着1.5版本的开源，腾讯正进一步推动高质量 AI 翻译技术走向普惠，为全球开发者提供更具性价比的翻译方案。</p>"}},{"id":"228754181994609667","type":"news","url":"https://www.aibase.com/zh/news/24131","title":"三星 Exynos 2600 芯片助力 AI 技术飞跃，模型体积缩减 90%！","description":"近日，韩国媒体 ETNews 报道称，三星新一代 Exynos2600芯片将整合 Nota 公司的 AI 模型优化技术。这一举措旨在在保持模型高精度的同时，神奇地将 AI 模型的体积缩小90% 以上，为移动设备的 AI 应用铺平道路。 据悉，Nota 是一家专注于 AI 优化技术的企业，近日与三星签署了技术供应协议。这是 Nota 继 Exynos2500后再次获得三星旗舰芯片的订单，充分说明其在 AI 技术领域的实力和影响力。Nota 的核心技术基于其自主研发的 “NetsPresso” 平台，该平台能够在不同的硬件环境中高效优化和部署 AI 模型，确保其推理准确度不会受到影响。 [图片: image.png https://upload.chinaz.com/2025/1230/6390270632448222396688086.png] 此次合作的目标是支持大规模生成式 AI 模型在 Exynos2600上的流畅运行。得益于 Nota 的优化技术，未来的移动设备将能够在无需互联网连接的情况下，直接在本地处理复杂的生成式 AI 任务。这将极大提升用户体验，尤其是在需要快速响应和处理的应用场景中。 除了为 Exynos2600提供 AI 模型优化方案，Nota 还将深度参与三星电子下一代 AI 模型优化工具链 “Exynos AI Studio” 的开发工作。该工具旨在通过自动化优化管道，改善开发者在 Exynos 平台上部署 最新 AI 模型的流程，从而降低开发难度、缩短上市时间。这对于广大开发者而言，无疑是一个利好消息，让 AI 技术的普及变得更加迅速和便捷。 总的来看，三星 Exynos2600芯片的推出不仅是硬件的升级，更是移动 AI 技术的一次重大飞跃。随着 AI 技术的不断进步，未来我们将看到更多智能设备的崛起，极大地丰富我们的生活。","published_date":"2025-12-30T07:45:51.514Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>近日，韩国媒体 ETNews 报道称，三星新一代 Exynos2600芯片将整合 Nota 公司的 AI 模型优化技术。这一举措旨在在保持模型高精度的同时，神奇地将 AI 模型的体积缩小90% 以上，为移动设备的 AI 应用铺平道路。</p><p>据悉，Nota 是一家专注于 AI 优化技术的企业，近日与三星签署了技术供应协议。这是 Nota 继 Exynos2500后再次获得三星旗舰芯片的订单，充分说明其在 AI 技术领域的实力和影响力。Nota 的核心技术基于其自主研发的 “NetsPresso” 平台，该平台能够在不同的硬件环境中高效优化和部署 AI 模型，确保其推理准确度不会受到影响。</p><p style=\"text-align:center\"><img src=\"https://upload.chinaz.com/2025/1230/6390270632448222396688086.png\" title=\"image.png\" alt=\"image.png\"></p><p>此次合作的目标是支持大规模生成式 AI 模型在 Exynos2600上的流畅运行。得益于 Nota 的优化技术，未来的移动设备将能够在无需互联网连接的情况下，直接在本地处理复杂的生成式 AI 任务。这将极大提升用户体验，尤其是在需要快速响应和处理的应用场景中。</p><p>除了为 Exynos2600提供 AI 模型优化方案，Nota 还将深度参与三星电子下一代 AI 模型优化工具链 “Exynos AI Studio” 的开发工作。该工具旨在通过自动化优化管道，改善开发者在 Exynos 平台上部署<span>最新</span> AI 模型的流程，从而降低开发难度、缩短上市时间。这对于广大开发者而言，无疑是一个利好消息，让 AI 技术的普及变得更加迅速和便捷。</p><p>总的来看，三星 Exynos2600芯片的推出不仅是硬件的升级，更是移动 AI 技术的一次重大飞跃。随着 AI 技术的不断进步，未来我们将看到更多智能设备的崛起，极大地丰富我们的生活。</p>"}},{"id":"228754181994609668","type":"news","url":"https://www.aibase.com/zh/news/24130","title":"端侧AI翻译新突破:腾讯混元1.5版开源，手机也能实时多语种互译","description":"据AIbase报道，腾讯近日正式发布其混元翻译模型（HY-MT）的1.5版本，并宣布将其开源。这是一个支持33种语言互译的强大解决方案，旨在为移动设备和高效的端侧部署提供卓越的翻译能力。 [图片: QQ20251230-153609.png https://upload.chinaz.com/2025/1230/6390270589390446664756175.png] 新发布的模型包含两个版本: Tencent-HY-MT1.5-1.8B 和 Tencent-HY-MT1.5-7B 。其中，1.8B版本经过特别优化，经过量化处理后，仅需1GB内存即可在手机等消费级设备上实现离线实时翻译。其推理速度极为出色，处理50个tokens仅需0.18秒，远超主流商用翻译API，使其在即时翻译场景下具备强大的实用性。 性能方面，混元翻译模型1.5在 权威 的WMT25和Flores200测试集上表现突出。值得注意的是，轻量级的1.8B模型的表现已达到了谷歌超大规模闭源模型Gemini-3.0-Pro的90分位水平，在翻译质量和响应效率之间取得了 极佳 的平衡。而7B版本则在前一版本的基础上显著提升了翻译准确率，并有效减少了译文中不必要的注释干扰和语种混杂问题。 该模型不仅技术领先，功能也十分全面。它支持用户根据行业特点自定义术语库，确保专业领域翻译的准确性和一致性。同时，其优秀的上下文理解能力使其在处理长文本和多轮对话时，能够保持翻译的连贯性和流畅性，显著提升用户体验。 腾讯混元翻译模型已经在国际机器翻译比赛中斩获多个奖项，并在腾讯会议、企业微信等核心业务中得到应用，证明了其强大的竞争力。 混元官网：https://hunyuan.tencent.com/modelSquare/home/list Github链接：https://github.com/Tencent-Hunyuan/HY-MT HuggingFace链接：https://huggingface.co/collections/tencent/hy-mt15","published_date":"2025-12-30T07:38:57.533Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>据AIbase报道，腾讯近日正式发布其混元翻译模型（HY-MT）的1.5版本，并宣布将其开源。这是一个支持33种语言互译的强大解决方案，旨在为移动设备和高效的端侧部署提供卓越的翻译能力。</p><p style=\"text-align:center\"><img src=\"https://upload.chinaz.com/2025/1230/6390270589390446664756175.png\" title=\"QQ20251230-153609.png\" alt=\"QQ20251230-153609.png\"></p><p>新发布的模型包含两个版本:<strong>Tencent-HY-MT1.5-1.8B</strong> 和 <strong>Tencent-HY-MT1.5-7B</strong>。其中，1.8B版本经过特别优化，经过量化处理后，仅需1GB内存即可在手机等消费级设备上实现离线实时翻译。其推理速度极为出色，处理50个tokens仅需0.18秒，远超主流商用翻译API，使其在即时翻译场景下具备强大的实用性。</p><p>性能方面，混元翻译模型1.5在<span>权威</span>的WMT25和Flores200测试集上表现突出。值得注意的是，轻量级的1.8B模型的表现已达到了谷歌超大规模闭源模型Gemini-3.0-Pro的90分位水平，在翻译质量和响应效率之间取得了<span>极佳</span>的平衡。而7B版本则在前一版本的基础上显著提升了翻译准确率，并有效减少了译文中不必要的注释干扰和语种混杂问题。</p><p>该模型不仅技术领先，功能也十分全面。它支持用户根据行业特点自定义术语库，确保专业领域翻译的准确性和一致性。同时，其优秀的上下文理解能力使其在处理长文本和多轮对话时，能够保持翻译的连贯性和流畅性，显著提升用户体验。</p><p>腾讯混元翻译模型已经在国际机器翻译比赛中斩获多个奖项，并在腾讯会议、企业微信等核心业务中得到应用，证明了其强大的竞争力。</p><ul><li><p>混元官网：https://hunyuan.tencent.com/modelSquare/home/list</p></li><li><p>Github链接：https://github.com/Tencent-Hunyuan/HY-MT</p></li><li><p>HuggingFace链接：https://huggingface.co/collections/tencent/hy-mt15</p></li></ul><p><br></p>"}},{"id":"228754181994609669","type":"news","url":"https://www.aibase.com/zh/news/24129","title":"Zara 借助 AI 技术革新时尚摄影，传统行业面临挑战","description":"西班牙快时尚品牌 Zara 正在积极运用人工智能（AI）技术，致力于对模特照片进行数字化编辑，以应对激烈的市场竞争。该公司在征得模特同意的基础上，利用她们的现有照片，通过 AI 为她们 “穿上” 新款服装，并将模特置于虚拟场景中。这一创新做法不仅大幅度降低了拍摄成本和时间，还重新定义了时尚摄影的制作流程。 与传统拍摄相比，Zara 的 AI 应用省去了许多实体拍摄所需的场地、设备以及专业团队。这种高效的运作方式为品牌节省了可观的开支。不过，行业内的专业人士面临着挑战。多位接受《City AM》采访的模特表示，她们在 AI 创作中获得的报酬与实际拍摄相同，但摄影师、化妆师及造型师等传统岗位却可能在这个过程中被排除，导致他们失去工作机会。 对于外界的质疑，Zara 方面强调，AI 技术将作为传统摄影的补充，而非完全取代。与此同时，Zara 的销售额已在去年11月降至六个月来的 最低 点，表明公司在转型中面临的压力。此外，Zara 并非 唯一 探索这一领域的品牌，其竞争对手 H&#x26;M 和 Zalando 也在今年推出了类似的 AI 计划，旨在为模特创建数字化形象。这一系列动作表明，时尚行业正在逐步迈向一个由技术驱动的新纪元。 划重点: 🌟 Zara 通过 AI 技术进行模特照片的数字化编辑，显著节省成本和时间。 📸 传统摄影师、化妆师等专业人士可能面临失业风险。 🚀 时尚行业正迎来技术驱动的转型，H&#x26;M 和 Zalando 等品牌也在探索类似 AI 计划。","published_date":"2025-12-30T07:26:56.265Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>西班牙快时尚品牌 Zara 正在积极运用人工智能（AI）技术，致力于对模特照片进行数字化编辑，以应对激烈的市场竞争。该公司在征得模特同意的基础上，利用她们的现有照片，通过 AI 为她们 “穿上” 新款服装，并将模特置于虚拟场景中。这一创新做法不仅大幅度降低了拍摄成本和时间，还重新定义了时尚摄影的制作流程。</p><p>与传统拍摄相比，Zara 的 AI 应用省去了许多实体拍摄所需的场地、设备以及专业团队。这种高效的运作方式为品牌节省了可观的开支。不过，行业内的专业人士面临着挑战。多位接受《City AM》采访的模特表示，她们在 AI 创作中获得的报酬与实际拍摄相同，但摄影师、化妆师及造型师等传统岗位却可能在这个过程中被排除，导致他们失去工作机会。</p><p>对于外界的质疑，Zara 方面强调，AI 技术将作为传统摄影的补充，而非完全取代。与此同时，Zara 的销售额已在去年11月降至六个月来的<span>最低</span>点，表明公司在转型中面临的压力。此外，Zara 并非<span>唯一</span>探索这一领域的品牌，其竞争对手 H&#x26;M 和 Zalando 也在今年推出了类似的 AI 计划，旨在为模特创建数字化形象。这一系列动作表明，时尚行业正在逐步迈向一个由技术驱动的新纪元。</p><blockquote><p>划重点:</p><p>🌟 Zara 通过 AI 技术进行模特照片的数字化编辑，显著节省成本和时间。  </p><p>📸 传统摄影师、化妆师等专业人士可能面临失业风险。  </p><p>🚀 时尚行业正迎来技术驱动的转型，H&#x26;M 和 Zalando 等品牌也在探索类似 AI 计划。  </p></blockquote>"}},{"id":"228741521169218560","type":"news","url":"https://www.jiqizhixin.com/articles/2025-12-30-7","title":"吴恩达年终总结：2025是AI工业时代的黎明","description":"[图片: https://image.jiqizhixin.com/uploads/editor/dd6edcfb-d9fc-41ef-b7a1-bde98860ed18/1767078789691.png]2025 年已经走到了尾声。 关注 AI 圈的读者们都知道，今年是各路 AI 巨头神仙打架的一年，是人才大战架构重组极其频繁的一年，是大模型军备竞赛出奇白热化的一年，也是 AI 基础设施建设如火如荼的一年…… 在这精彩绝伦的一年的结尾，我们的老朋友：斯坦福大学计算机科学客座教授，前百度 AI 负责人，前谷歌大脑负责人吴恩达老师，发表了今年的保留节目：一封信，和一篇 2025 的人工智能领域年度总结。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/1cf3e875-a2a7-475b-a5cd-16f9e5ae571e/640.png] 年末寄语：三把金钥匙 元旦假期将至，学生们美妙的寒假以及春节假期也近在眼前。「永远不要停止学习」是假期前的老生常谈，尤其是希望在蓬勃进化的，高度竞争的人工智能领域内谋求发展机会的人。应当做什么，怎么做，吴恩达在今年的信中给出了他的见地。 以下是公开信全文： 亲爱的朋友们： 又一年，AI 以惊人的速度向前推进，为所有人 —— 包括刚进入这个领域的新手 —— 创造了前所未有的软件开发机会。事实上，许多公司现在最大的困扰之一，就是找不到足够多真正懂 AI 的工程师。 每年冬季假期，我都会留出一段时间来学习和动手构建项目，希望你们也能如此。这不仅能帮助我打磨已有技能、掌握新知识，也能实实在在地推动你的技术职业发展。 要真正具备构建 AI 系统的能力，我建议你做到三点： 系统地学习 AI 课程 持续动手构建 AI 系统 （可选）阅读研究论文 下面我解释为什么这三点都如此重要。 我常听到一些开发者建议别人：「别学了，直接上手做就行。」这是非常糟糕的建议！除非你已经身处一个经验丰富的 AI 开发者社群中，否则在没有理解 AI 基础的情况下贸然动手，很容易导致你重复发明轮子，或者更糟糕的是，把轮子重新发明得一团糟。 举个例子，在面试中，我见过不少候选人：自己重新发明了一套标准的 RAG 文档切分策略；重复实现了已经成熟的 Agentic AI 评估方法；写出了结构混乱、难以维护的 LLM 上下文管理代码。如果他们提前上过几门相关课程，就会更清楚哪些「积木」已经存在于行业中。他们当然仍然可以选择从零实现这些模块，甚至发明出比现有方案更好的方法，但至少能避免浪费数周时间走弯路。 因此，结构化学习至关重要。 而且说实话，我个人觉得上课非常有趣。与其看 Netflix，我更愿意随时打开一门优秀 AI 讲师的课程来学习。 同时，仅仅上课是不够的。有许多重要的经验，只有通过亲手实践才能真正学到。学习飞机是如何运作的理论，对于成为一名飞行员当然非常重要，但从来没有人只靠上课就学会开飞机。在某个时刻，真正坐进驾驶舱是不可或缺的！好消息是：随着高度智能化（highly agentic）的编程助手出现，动手构建的门槛已经比以往任何时候都低。而当你开始学习 AI 的各种构建模块时，它们常常会激发你对「还能做些什么」的新想法。如果我一时找不到项目灵感，我通常会去上几门课，或者读一些研究论文。这样坚持一段时间后，我总会冒出一大堆新的项目想法。而且，说实话，我觉得「做东西」本身真的很有趣，也希望你能体会到这种乐趣！ 最后，并不是每个人都必须这样做，但我发现如今就业市场上最强的一批候选人，几乎都会偶尔阅读研究论文。虽然在我看来，论文比课程难啃得多，但它们包含了大量尚未被翻译成更易理解形式的前沿知识。我会把读论文的优先级排在课程和实践之后，但如果你有机会提升阅读论文的能力，我仍然强烈建议你这样做。（你也可以看看我以前讲过的一段关于如何读论文的视频。）上课和动手构建对我来说很有趣，读论文则更像是一种「磨练」，但从论文中偶尔闪现的洞见，真的令人愉悦。 祝你度过一个美好的寒假，新年快乐。除了学习和创造，也希望你能多花时间陪伴亲人 —— 那同样非常重要！ Love， Andrew 年终总结：AI工业时代的黎明 2025年着实是精彩绝伦的一年。 作为每年的保留节目，吴恩达的年终总结都能带我们回顾全年最重要的人工智能事件和发展趋势。 2022年，是AI 的璀璨之年，生成文本、图像、视频、音乐和代码的系统即将到来，引发了关于创造力的未来问题的讨论。 2023年，是创新与焦虑的一年，生成式 AI 浪潮席卷了各行各业，其不断扩大的能力引发了智能机器可能会使人类过时的担忧。 2024年，是暴风雪般进步的一年，人工智能取得了突破性进展。智能代理系统提升了推理、使用工具和控制桌面应用程序的能力。小型模型迅速普及，其中许多比其前辈更强大且价格更低廉。 2025年，或将被铭记为 AI 工业时代的黎明。 让我们跟随吴恩达的视角，探索2025年最具代表性的AI大事。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/2d1114c2-2934-425f-9964-c01d52546612/640.png] 文章链接：https://www.deeplearning.ai/the-batch/issue-333/ 思考型模型解决更大的问题 去年年末，OpenAI 推出了首个推理模型 o1，将一种具备代理能力的推理工作流内嵌其中。今年 1 月，DeepSeek-R1 向世界展示了如何构建这种能力。结果是：数学与编程性能立刻提升，问题回答更准确，机器人能力更强，AI 智能体取得快速进展。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/111d44b9-f69e-4b7b-9a4b-ff91aa7d3498/640.png] 在 2025 年初，模型只有在被明确提示时才会执行推理策略。如今，大多数新的大语言模型都会默认这样做，从而在广泛任务上显著提升了性能。 最早的一批推理模型 通过 RL 训练 ，专门用于正确求解数学问题、准确回答科学问题，生成能通过单元测试的代码。例如，o1-preview 在 AIME 2024 上比其非推理前身 GPT-4o 高出 43 个百分点，在 GPQA Diamond上高出 22 个百分点；在 Codeforces 编程题中，其表现位于人类竞技选手的 第 62 百分位，而 GPT-4o 仅为 第 11 百分位。 当推理模型 学会使用诸如计算器、搜索引擎或 bash 终端等工具 时，表现会进一步提升。例如，在一项涵盖 100 个领域、考察多模态理解与技术专长的高难度测试中，带工具的 OpenAI o4-mini 达到 17.7% 的准确率，比不使用工具时高出 3 个多百分点。 机器人动作模型 也通过 RL 学会推理。例如，通过奖励 ThinkAct 达成目标位置，使其在机器人任务上的表现相较于不具备思考能力的模型（如 OpenVLA）提升了约 8%。 推理模型还帮助智能体 应对复杂问题 。例如，AlphaEvolve 使用 Google Gemini 反复生成、评估并修改代码，最终为现实世界问题产出了更快的算法。其中一个成果是，它提出了一个用于解释微生物耐药性的长期未解问题的假说；人类科学家几乎在同一时间独立提出并验证了相同假说。 推理能力显著提升了 LLM 的性能，但更优输出也伴随着成本。Gemini 3 Flash 在开启推理时运行 Artificial Analysis 的 Intelligence Index 基准共消耗 1.6 亿 tokens（得分 71），而关闭推理仅消耗 740 万 tokens（得分明显更低，为 55）。此外，生成推理 tokens 会延迟输出，这也给 LLM 推理服务商带来了更大的性能压力。不过，研究人员正在努力提高效率。Claude Opus 4.5 与 GPT-5.1 在高推理设置下取得了相同的 Intelligence Index 分数，但前者消耗 4800 万 tokens，后者则消耗 8100 万 tokens。 巨额薪酬吸引顶尖 AI 人才 领先的 AI 公司展开了一场激烈的人才争夺战，用堪比职业体育明星级别的薪酬，从竞争对手那里挖走顶尖人才。 7 月，Meta 发起大规模招聘 ，为新成立的 Meta Superintelligence Labs 组建团队，向来自 OpenAI、Google、Anthropic 等顶级 AI 公司的研究人员开出高达数亿美元的待遇。作为回应，Meta 的竞争对手反过来从 Meta 及彼此之间挖走关键员工，使 AI 人才的市场价值被推至前所未有的高度。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/771831a0-1a79-4592-bee4-b636197977ec/640.png] 据《华尔街日报》报道，在成功招募 Alexandr Wang 及其核心团队成员之后，Meta 首席执行官 Mark Zuckerberg 列出了一份「心愿清单」。 为了说服人们跳槽，Zuckerberg 甚至亲自登门拜访，有时还会带上自制的汤。这项努力成功招募了包括 OpenAI 的 Jason Wei 和 Hyung Won Chung 在内的人才，两人均为推理模型的核心研究者。 《华尔街日报》称，曾与 OpenAI 前 CTO Mira Murati 共同创立 Thinking Machines Lab 的 Andrew Tulloch，最初拒绝了 Meta 提出的方案，其中包括价值 15 亿美元 的奖金。几个月后，他改变主意，加入了 Meta。 Meta 还聘请了曾主管 Apple AI 模型的 Ruoming Pang。据彭博社报道，其薪酬方案在数年内累计高达数亿美元。Meta 的报价超过了 Apple 除 CEO 之外最高层管理者的薪酬，而 Apple 选择不予匹配。 在这场人员流动中，Microsoft AI CEO Mustafa Suleyman 从 Google 带走了 20 多名研究人员和工程师，其中包括工程副总裁 Amar Subramanya。 Elon Musk 的 xAI 从 Meta 挖走了十多名 AI 研究人员和工程师。Musk 抨击竞争对手的报价「疯狂」，并强调自己公司「极端以能力为导向」的文化，以及股权更具增长潜力。 随着 2026 年的到来，AI 招聘格局已发生巨大变化。据《华尔街日报》报道，为了抵御猎头挖角，OpenAI 提供了比竞争对手更高比例的股票型薪酬，加快了新员工期权的归属进度，并发放高达 150 万美元 的留任奖金。 尽管 2025 年出现了关于 AI 泡沫的讨论，但对于计划投入数百亿美元建设 AI 数据中心的公司来说，高薪是完全理性的选择： 如果你愿意在硬件上花这么多钱，为什么不拿出其中一小部分用于支付人才薪酬呢？ 数据中心建设狂潮席卷全球 头部 AI 公司纷纷宣布了庞大的建设计划，预计在未来几年内将豪掷数万亿美元，并消耗数吉瓦（GW）的电力。 仅今年一年，AI 行业的资本支出就突破了 3000 亿美元，其中大部分用于建设处理 AI 任务的新数据中心。这还仅仅是「前菜」，各大公司正在规划堪称宏伟的蓝图——建设规模堪比小镇、能耗相当于中型城市的设施。据麦肯锡预测，为了建设足够的算力以满足预期的推理和训练需求， 这场竞赛的成本到 2030 年可能高达 5.2 万亿美元 。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/351b599e-3e02-468f-a2c8-a4998e0b5595/640.png] OpenAI ： 1 月，OpenAI 启动了与甲骨文（Oracle）、软银（SoftBank）及阿联酋投资公司 MGX 合作的 5000 亿美元「星际之门」（Stargate）项目。公司最终宣布计划在全球建设 20 吉瓦的数据中心产能，并预测需求量将是该数字的 5 倍。OpenAI CEO 萨姆·奥特曼表示，希望最终能实现每周增加 1 吉瓦的产能。 Meta ： 2025 年在基础设施项目上投入约 720 亿美元，高管表示该数字在 2026 年还将大幅上升。其 Hyperion 项目包括在路易斯安那州农村地区建设一个价值 270 亿美元、容量为 5 吉瓦的数据中心。 微软 ： 2025 年全球数据中心项目支出达 800 亿美元，其中包括位于威斯康星州和亚特兰大的设施，它们将通过专用光纤网络连接，作为一个巨大的超级计算机运行。公司还承诺将其在欧洲的云和 AI 产能扩展至 200 个数据中心。 亚马逊 ： 预计 2025 年基础设施支出将达 1250 亿美元，2026 年还将投入更多。其耗资 110 亿美元的「雷尼尔计划」（Project Rainier）是位于印第安纳州的一个 2.2 吉瓦数据中心，将运行 50 万块 Amazon Trainium 2 芯片。此外，亚马逊计划在 2025 年至 2029 年间斥资约 140 亿美元扩建澳大利亚的数据中心，并在德国投资约 210 亿美元。 Alphabet （谷歌母公司）： 预计 2025 年基础设施支出高达 930 亿美元，高于此前预测的 750 亿美元。公司宣布了一项 400 亿美元的计划，到 2027 年在得克萨斯州增加 3 个数据中心。此外，还承诺在印度投入 150 亿美元，在德国宣布了约 60 亿美元的投资，并在澳大利亚、马来西亚和乌拉圭推出了新建或扩建项目。 尽管存在对 AI 泡沫的担忧，但基础设施建设热潮正在为原本不温不火的 经济带来实实在在的增长 。哈佛大学经济学家 Jason Furman 指出，2025 年上半年美国GDP的增长几乎全部来自数据中心和 AI 领域的投资。在此阶段，有证据支持这样一种观点：2025 年拉开了新工业时代的序幕。 智能体让代码编写更高效 编程已成为智能体工作流中具有最直接商业价值的应用场景。Claude Code、Google Gemini CLI、OpenAI Codex 以及其他应用，将「编程智能体」变成了 AI 巨头之间竞争最激烈的战场之一。为了留在牌桌上，规模较小的竞争对手也纷纷开发了自己的智能体模型。 当 2024 年首个开创性的智能体代码生成器 Devin 问世时，它将 SWE-Bench 编程挑战基准测试的最高水平（SOTA）从 1.96% 提升到了 13.86%。到了 2025 年，使用最新大语言模型的编程智能体已能常态化地完成超过 80% 的同类任务。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/9c24f9e5-469e-4157-978a-877189a67228/640.png] 2024 年底，推理模型的出现立即提升了编程能力并降低了成本，因为推理能力使智能体能够规划任务，并将具体执行交给成本更低的模型去完成。到 2025 年底，Gemini 3 Pro、Claude Opus 4.5 和 GPT-5.2 已成为编程和智能体工作流领域的顶尖模型。 Z.ai 的 GLM-4.5 和月之暗面的 Kimi K2 成为开放权重模型中的热门选择，使自动编程类初创公司得以大幅削减成本。7 月发布的 Qwen3-Coder 提供了一个庞大的 4800 亿参数模型，该模型在超过 5 万亿 Token 的代码数据上进行了训练，性能几近匹敌 Claude Sonnet 4。 Anthropic 围绕 Claude 构建了一套智能体框架，打造出了 Claude Code 应用。该应用在 2 月一经推出便大受欢迎，确立了智能体编程系统应有的标准。OpenAI 随即做出回应，推出了基于其 GPT-5 系列编程专用版构建的 Codex 应用。 模型制造商与集成开发环境（IDE）开发者之间展开了一场拉锯战。这导致 Anysphere (Cursor) 和 Cognition AI (Windsurf) 等热门 IDE 提供商开始构建自己的模型。反之，Google 也构建了自己的 IDE——Antigravity，并于 11 月首次亮相。 智能体系统不断推高 SWE-Bench 这一热门编程基准测试的上限，促使研究人员寻找替代的方式来评估其性能。这些努力催生了 SWE-Bench Verified、SWE-Bench Pro、LiveBench、Terminal-Bench、𝜏-Bench 和 CodeClash 等新基准。 2025 年初，大多数观察家还认为智能体仅擅长生成常规代码、文档和单元测试，而在处理更高阶的战略性问题上，资深人类工程师和产品经理的表现依然更胜一筹。但到了年底，许多公司报告称已开始自动化资深级别的任务。Microsoft、Google、Amazon 和 Anthropic 均表示，他们自身 越来越多的代码正由 AI 生成 。 更多细节，请参阅年度总结原文。 结语：去亲手构建未来 回望 2025，我们似乎见证了一场关于「规模」的游戏。 在这一年，AI 终于脱离了单纯的算法竞赛，演变成一场涉及人才、算力、基建和能源的工业革命。从超大规模数据中心到能耗巨大的算力集群，科技巨头们正以前所未有的资源投入，加速实现通往 AGI 的技术跨越。 这种宏大的叙事往往让人感到渺小，甚至焦虑。当 AI 的进化速度以「天」为单位，当顶尖人才的薪酬变成天文数字，普通开发者和从业者的位置在哪里？吴恩达给出了答案。 虽然 2025 是 AI 变得最「重」的一年，但它也是 AI 开发变得最「轻」的一年 。推理模型的成熟和编程智能体的进化，极大地拉低了创造的门槛。正如吴恩达所言，现在是软件开发前所未有的黄金时代。巨头们负责铺设电网和铁路（基础设施），而每一位开发者、学生、研究者，则拥有了在这些轨道上建造飞船的权利。 最好的预测未来的方式，就是去亲手构建它。 祝你在即将到来的 2026 年，保持好奇，永远不要停止学习。 新年快乐！ ]]>","published_date":"2025-12-30T07:17:50.789Z","authors":"机器之心","source":"机器之心 - 机器之心","details":{"content_html":"<img src=\"https://image.jiqizhixin.com/uploads/editor/dd6edcfb-d9fc-41ef-b7a1-bde98860ed18/1767078789691.png\" style=\"width: 700%;\">2025 年已经走到了尾声。<p></p><p>关注 AI 圈的读者们都知道，今年是各路 AI 巨头神仙打架的一年，是人才大战架构重组极其频繁的一年，是大模型军备竞赛出奇白热化的一年，也是 AI 基础设施建设如火如荼的一年……</p><p>在这精彩绝伦的一年的结尾，我们的老朋友：斯坦福大学计算机科学客座教授，前百度 AI 负责人，前谷歌大脑负责人吴恩达老师，发表了今年的保留节目：一封信，和一篇 2025 的人工智能领域年度总结。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/1cf3e875-a2a7-475b-a5cd-16f9e5ae571e/640.png\" alt=\"图片\" style=\"width: 70%;\"></section><p><strong>年末寄语：三把金钥匙</strong></p><p>元旦假期将至，学生们美妙的寒假以及春节假期也近在眼前。「永远不要停止学习」是假期前的老生常谈，尤其是希望在蓬勃进化的，高度竞争的人工智能领域内谋求发展机会的人。应当做什么，怎么做，吴恩达在今年的信中给出了他的见地。</p><p>以下是公开信全文：</p><blockquote><section><section></section><section><section><section><section><section><section><p>亲爱的朋友们：</p><p>又一年，AI 以惊人的速度向前推进，为所有人 —— 包括刚进入这个领域的新手 —— 创造了前所未有的软件开发机会。事实上，许多公司现在最大的困扰之一，就是找不到足够多真正懂 AI 的工程师。</p><p>每年冬季假期，我都会留出一段时间来学习和动手构建项目，希望你们也能如此。这不仅能帮助我打磨已有技能、掌握新知识，也能实实在在地推动你的技术职业发展。</p><p>要真正具备构建 AI 系统的能力，我建议你做到三点：</p><ul><li><p>系统地学习 AI 课程</p></li><li><p>持续动手构建 AI 系统</p></li><li><p>（可选）阅读研究论文</p></li></ul><p>下面我解释为什么这三点都如此重要。</p><p>我常听到一些开发者建议别人：「别学了，直接上手做就行。」这是非常糟糕的建议！除非你已经身处一个经验丰富的 AI 开发者社群中，否则在没有理解 AI 基础的情况下贸然动手，很容易导致你重复发明轮子，或者更糟糕的是，把轮子重新发明得一团糟。</p><p>举个例子，在面试中，我见过不少候选人：自己重新发明了一套标准的 RAG 文档切分策略；重复实现了已经成熟的 Agentic AI 评估方法；写出了结构混乱、难以维护的 LLM 上下文管理代码。如果他们提前上过几门相关课程，就会更清楚哪些「积木」已经存在于行业中。他们当然仍然可以选择从零实现这些模块，甚至发明出比现有方案更好的方法，但至少能避免浪费数周时间走弯路。</p><p>因此，结构化学习至关重要。</p><p>而且说实话，我个人觉得上课非常有趣。与其看 Netflix，我更愿意随时打开一门优秀 AI 讲师的课程来学习。</p><p>同时，仅仅上课是不够的。有许多重要的经验，只有通过亲手实践才能真正学到。学习飞机是如何运作的理论，对于成为一名飞行员当然非常重要，但从来没有人只靠上课就学会开飞机。在某个时刻，真正坐进驾驶舱是不可或缺的！好消息是：随着高度智能化（highly agentic）的编程助手出现，动手构建的门槛已经比以往任何时候都低。而当你开始学习 AI 的各种构建模块时，它们常常会激发你对「还能做些什么」的新想法。如果我一时找不到项目灵感，我通常会去上几门课，或者读一些研究论文。这样坚持一段时间后，我总会冒出一大堆新的项目想法。而且，说实话，我觉得「做东西」本身真的很有趣，也希望你能体会到这种乐趣！</p><p>最后，并不是每个人都必须这样做，但我发现如今就业市场上最强的一批候选人，几乎都会偶尔阅读研究论文。虽然在我看来，论文比课程难啃得多，但它们包含了大量尚未被翻译成更易理解形式的前沿知识。我会把读论文的优先级排在课程和实践之后，但如果你有机会提升阅读论文的能力，我仍然强烈建议你这样做。（你也可以看看我以前讲过的一段关于如何读论文的视频。）上课和动手构建对我来说很有趣，读论文则更像是一种「磨练」，但从论文中偶尔闪现的洞见，真的令人愉悦。</p><p>祝你度过一个美好的寒假，新年快乐。除了学习和创造，也希望你能多花时间陪伴亲人 —— 那同样非常重要！</p><p>Love，</p><p>Andrew</p></section></section></section></section></section></section></section></blockquote><p><strong>年终总结：AI工业时代的黎明</strong></p><p>2025年着实是精彩绝伦的一年。</p><p>作为每年的保留节目，吴恩达的年终总结都能带我们回顾全年最重要的人工智能事件和发展趋势。</p><p>2022年，是AI 的璀璨之年，生成文本、图像、视频、音乐和代码的系统即将到来，引发了关于创造力的未来问题的讨论。</p><p>2023年，是创新与焦虑的一年，生成式 AI 浪潮席卷了各行各业，其不断扩大的能力引发了智能机器可能会使人类过时的担忧。</p><p>2024年，是暴风雪般进步的一年，人工智能取得了突破性进展。智能代理系统提升了推理、使用工具和控制桌面应用程序的能力。小型模型迅速普及，其中许多比其前辈更强大且价格更低廉。</p><p>2025年，或将被铭记为 <strong>AI 工业时代的黎明。</strong>让我们跟随吴恩达的视角，探索2025年最具代表性的AI大事。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/2d1114c2-2934-425f-9964-c01d52546612/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><ul><li><p>文章链接：https://www.deeplearning.ai/the-batch/issue-333/</p></li></ul><p><strong>思考型模型解决更大的问题</strong></p><p>去年年末，OpenAI 推出了首个推理模型 o1，将一种具备代理能力的推理工作流内嵌其中。今年 1 月，DeepSeek-R1 向世界展示了如何构建这种能力。结果是：数学与编程性能立刻提升，问题回答更准确，机器人能力更强，AI 智能体取得快速进展。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/111d44b9-f69e-4b7b-9a4b-ff91aa7d3498/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p>在 2025 年初，模型只有在被明确提示时才会执行推理策略。如今，大多数新的大语言模型都会默认这样做，从而在广泛任务上显著提升了性能。</p><ul><li><p>最早的一批推理模型<strong>通过 RL 训练</strong>，专门用于正确求解数学问题、准确回答科学问题，生成能通过单元测试的代码。例如，o1-preview 在 AIME 2024 上比其非推理前身 GPT-4o 高出 43 个百分点，在 GPQA Diamond上高出 22 个百分点；在 Codeforces 编程题中，其表现位于人类竞技选手的 第 62 百分位，而 GPT-4o 仅为 第 11 百分位。</p></li><li><p>当推理模型<strong>学会使用诸如计算器、搜索引擎或 bash 终端等工具</strong>时，表现会进一步提升。例如，在一项涵盖 100 个领域、考察多模态理解与技术专长的高难度测试中，带工具的 OpenAI o4-mini 达到 17.7% 的准确率，比不使用工具时高出 3 个多百分点。</p></li><li><p><strong>机器人动作模型</strong>也通过 RL 学会推理。例如，通过奖励 ThinkAct 达成目标位置，使其在机器人任务上的表现相较于不具备思考能力的模型（如 OpenVLA）提升了约 8%。</p></li><li><p>推理模型还帮助智能体<strong>应对复杂问题</strong>。例如，AlphaEvolve 使用 Google Gemini 反复生成、评估并修改代码，最终为现实世界问题产出了更快的算法。其中一个成果是，它提出了一个用于解释微生物耐药性的长期未解问题的假说；人类科学家几乎在同一时间独立提出并验证了相同假说。</p></li></ul><p>推理能力显著提升了 LLM 的性能，但更优输出也伴随着成本。Gemini 3 Flash 在开启推理时运行 Artificial Analysis 的 Intelligence Index 基准共消耗 1.6 亿 tokens（得分 71），而关闭推理仅消耗 740 万 tokens（得分明显更低，为 55）。此外，生成推理 tokens 会延迟输出，这也给 LLM 推理服务商带来了更大的性能压力。不过，研究人员正在努力提高效率。Claude Opus 4.5 与 GPT-5.1 在高推理设置下取得了相同的 Intelligence Index 分数，但前者消耗 4800 万 tokens，后者则消耗 8100 万 tokens。</p><p><strong>巨额薪酬吸引顶尖 AI 人才</strong></p><p>领先的 AI 公司展开了一场激烈的人才争夺战，用堪比职业体育明星级别的薪酬，从竞争对手那里挖走顶尖人才。</p><p><strong>7 月，Meta 发起大规模招聘</strong>，为新成立的 Meta Superintelligence Labs 组建团队，向来自 OpenAI、Google、Anthropic 等顶级 AI 公司的研究人员开出高达数亿美元的待遇。作为回应，Meta 的竞争对手反过来从 Meta 及彼此之间挖走关键员工，使 AI 人才的市场价值被推至前所未有的高度。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/771831a0-1a79-4592-bee4-b636197977ec/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p>据《华尔街日报》报道，在成功招募 Alexandr Wang 及其核心团队成员之后，Meta 首席执行官 Mark Zuckerberg 列出了一份「心愿清单」。</p><ul><li><p>为了说服人们跳槽，Zuckerberg 甚至亲自登门拜访，有时还会带上自制的汤。这项努力成功招募了包括 OpenAI 的 Jason Wei 和 Hyung Won Chung 在内的人才，两人均为推理模型的核心研究者。</p></li><li><p>《华尔街日报》称，曾与 OpenAI 前 CTO Mira Murati 共同创立 Thinking Machines Lab 的 Andrew Tulloch，最初拒绝了 Meta 提出的方案，其中包括价值 15 亿美元 的奖金。几个月后，他改变主意，加入了 Meta。</p></li><li><p>Meta 还聘请了曾主管 Apple AI 模型的 Ruoming Pang。据彭博社报道，其薪酬方案在数年内累计高达数亿美元。Meta 的报价超过了 Apple 除 CEO 之外最高层管理者的薪酬，而 Apple 选择不予匹配。</p></li><li><p>在这场人员流动中，Microsoft AI CEO Mustafa Suleyman 从 Google 带走了 20 多名研究人员和工程师，其中包括工程副总裁 Amar Subramanya。</p></li><li><p>Elon Musk 的 xAI 从 Meta 挖走了十多名 AI 研究人员和工程师。Musk 抨击竞争对手的报价「疯狂」，并强调自己公司「极端以能力为导向」的文化，以及股权更具增长潜力。</p></li></ul><p>随着 2026 年的到来，AI 招聘格局已发生巨大变化。据《华尔街日报》报道，为了抵御猎头挖角，OpenAI 提供了比竞争对手更高比例的股票型薪酬，加快了新员工期权的归属进度，并发放高达 150 万美元 的留任奖金。</p><p>尽管 2025 年出现了关于 AI 泡沫的讨论，但对于计划投入数百亿美元建设 AI 数据中心的公司来说，高薪是完全理性的选择：<strong>如果你愿意在硬件上花这么多钱，为什么不拿出其中一小部分用于支付人才薪酬呢？</strong></p><p><strong>数据中心建设狂潮席卷全球</strong></p><p>头部 AI 公司纷纷宣布了庞大的建设计划，预计在未来几年内将豪掷数万亿美元，并消耗数吉瓦（GW）的电力。</p><p>仅今年一年，AI 行业的资本支出就突破了 3000 亿美元，其中大部分用于建设处理 AI 任务的新数据中心。这还仅仅是「前菜」，各大公司正在规划堪称宏伟的蓝图——建设规模堪比小镇、能耗相当于中型城市的设施。据麦肯锡预测，为了建设足够的算力以满足预期的推理和训练需求，<strong>这场竞赛的成本到 2030 年可能高达 5.2 万亿美元</strong>。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/351b599e-3e02-468f-a2c8-a4998e0b5595/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><ul><li><p><strong>OpenAI</strong>： 1 月，OpenAI 启动了与甲骨文（Oracle）、软银（SoftBank）及阿联酋投资公司 MGX 合作的 5000 亿美元「星际之门」（Stargate）项目。公司最终宣布计划在全球建设 20 吉瓦的数据中心产能，并预测需求量将是该数字的 5 倍。OpenAI CEO 萨姆·奥特曼表示，希望最终能实现每周增加 1 吉瓦的产能。</p></li><li><p><strong>Meta</strong>： 2025 年在基础设施项目上投入约 720 亿美元，高管表示该数字在 2026 年还将大幅上升。其 Hyperion 项目包括在路易斯安那州农村地区建设一个价值 270 亿美元、容量为 5 吉瓦的数据中心。</p></li><li><p><strong>微软</strong>： 2025 年全球数据中心项目支出达 800 亿美元，其中包括位于威斯康星州和亚特兰大的设施，它们将通过专用光纤网络连接，作为一个巨大的超级计算机运行。公司还承诺将其在欧洲的云和 AI 产能扩展至 200 个数据中心。</p></li><li><p><strong>亚马逊</strong>： 预计 2025 年基础设施支出将达 1250 亿美元，2026 年还将投入更多。其耗资 110 亿美元的「雷尼尔计划」（Project Rainier）是位于印第安纳州的一个 2.2 吉瓦数据中心，将运行 50 万块 Amazon Trainium 2 芯片。此外，亚马逊计划在 2025 年至 2029 年间斥资约 140 亿美元扩建澳大利亚的数据中心，并在德国投资约 210 亿美元。</p></li><li><p><strong>Alphabet</strong>（谷歌母公司）： 预计 2025 年基础设施支出高达 930 亿美元，高于此前预测的 750 亿美元。公司宣布了一项 400 亿美元的计划，到 2027 年在得克萨斯州增加 3 个数据中心。此外，还承诺在印度投入 150 亿美元，在德国宣布了约 60 亿美元的投资，并在澳大利亚、马来西亚和乌拉圭推出了新建或扩建项目。</p></li></ul><p>尽管存在对 AI 泡沫的担忧，但基础设施建设热潮正在为原本不温不火的<strong>经济带来实实在在的增长</strong>。哈佛大学经济学家 Jason Furman 指出，2025 年上半年美国GDP的增长几乎全部来自数据中心和 AI 领域的投资。在此阶段，有证据支持这样一种观点：2025 年拉开了新工业时代的序幕。</p><p><strong>智能体让代码编写更高效</strong></p><p>编程已成为智能体工作流中具有最直接商业价值的应用场景。Claude Code、Google Gemini CLI、OpenAI Codex 以及其他应用，将「编程智能体」变成了 AI 巨头之间竞争最激烈的战场之一。为了留在牌桌上，规模较小的竞争对手也纷纷开发了自己的智能体模型。</p><p>当 2024 年首个开创性的智能体代码生成器 Devin 问世时，它将 SWE-Bench 编程挑战基准测试的最高水平（SOTA）从 1.96% 提升到了 13.86%。到了 2025 年，使用最新大语言模型的编程智能体已能常态化地完成超过 80% 的同类任务。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/9c24f9e5-469e-4157-978a-877189a67228/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><ul><li><p>2024 年底，推理模型的出现立即提升了编程能力并降低了成本，因为推理能力使智能体能够规划任务，并将具体执行交给成本更低的模型去完成。到 2025 年底，Gemini 3 Pro、Claude Opus 4.5 和 GPT-5.2 已成为编程和智能体工作流领域的顶尖模型。</p></li><li><p>Z.ai 的 GLM-4.5 和月之暗面的 Kimi K2 成为开放权重模型中的热门选择，使自动编程类初创公司得以大幅削减成本。7 月发布的 Qwen3-Coder 提供了一个庞大的 4800 亿参数模型，该模型在超过 5 万亿 Token 的代码数据上进行了训练，性能几近匹敌 Claude Sonnet 4。</p></li><li><p>Anthropic 围绕 Claude 构建了一套智能体框架，打造出了 Claude Code 应用。该应用在 2 月一经推出便大受欢迎，确立了智能体编程系统应有的标准。OpenAI 随即做出回应，推出了基于其 GPT-5 系列编程专用版构建的 Codex 应用。</p></li><li><p>模型制造商与集成开发环境（IDE）开发者之间展开了一场拉锯战。这导致 Anysphere (Cursor) 和 Cognition AI (Windsurf) 等热门 IDE 提供商开始构建自己的模型。反之，Google 也构建了自己的 IDE——Antigravity，并于 11 月首次亮相。</p></li></ul><p>智能体系统不断推高 SWE-Bench 这一热门编程基准测试的上限，促使研究人员寻找替代的方式来评估其性能。这些努力催生了 SWE-Bench Verified、SWE-Bench Pro、LiveBench、Terminal-Bench、𝜏-Bench 和 CodeClash 等新基准。</p><p>2025 年初，大多数观察家还认为智能体仅擅长生成常规代码、文档和单元测试，而在处理更高阶的战略性问题上，资深人类工程师和产品经理的表现依然更胜一筹。但到了年底，许多公司报告称已开始自动化资深级别的任务。Microsoft、Google、Amazon 和 Anthropic 均表示，他们自身<strong>越来越多的代码正由 AI 生成</strong>。</p><p>更多细节，请参阅年度总结原文。</p><p><strong>结语：去亲手构建未来</strong></p><p>回望 2025，我们似乎见证了一场关于「规模」的游戏。</p><p>在这一年，AI 终于脱离了单纯的算法竞赛，演变成一场涉及人才、算力、基建和能源的工业革命。从超大规模数据中心到能耗巨大的算力集群，科技巨头们正以前所未有的资源投入，加速实现通往 AGI 的技术跨越。</p><p>这种宏大的叙事往往让人感到渺小，甚至焦虑。当 AI 的进化速度以「天」为单位，当顶尖人才的薪酬变成天文数字，普通开发者和从业者的位置在哪里？吴恩达给出了答案。</p><p>虽然 <strong>2025 是 AI 变得最「重」的一年，但它也是 AI 开发变得最「轻」的一年</strong>。推理模型的成熟和编程智能体的进化，极大地拉低了创造的门槛。正如吴恩达所言，现在是软件开发前所未有的黄金时代。巨头们负责铺设电网和铁路（基础设施），而每一位开发者、学生、研究者，则拥有了在这些轨道上建造飞船的权利。</p><p>最好的预测未来的方式，就是去亲手构建它。</p><p>祝你在即将到来的 2026 年，保持好奇，永远不要停止学习。</p><p>新年快乐！</p>]]>"}},{"id":"228741521169218561","type":"news","url":"https://www.jiqizhixin.com/articles/2025-12-30-6","title":"自回归因果注意力也能并行解码？上交联合UCSD突破LLM推理瓶颈，模型代码全开源","description":"[图片: 图片 https://image.jiqizhixin.com/uploads/editor/950e0739-dcba-4de5-bf65-72c7b5ec349e/640.png] 在大语言模型（LLM）落地应用中，推理速度始终是制约效率的核心瓶颈。传统自回归（AR）解码虽能保证生成质量，却需逐 token 串行计算，速度极为缓慢；扩散型 LLM（dLLMs）虽支持并行解码，却面临训练成本高昂、质量下降及 KV 缓存兼容问题；投机解码（Speculative Decoding）则需额外引入草稿模型，系统复杂度大增。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/05af4054-d57f-47f5-b758-5d9f1674056c/640.gif] [图片: 图片 https://image.jiqizhixin.com/uploads/editor/1d796c54-f8da-4ea8-affc-0e356a47e878/640.gif] Jacobi Forcing Model 与 AR LLM 推理速度对比示意 近期，来自 UCSD Hao AI Lab 和上海交大 Deng Lab 的团队提出了一种突破性解决方案 ——Jacobi Forcing，该方案无需重构模型架构，即可将标准 AR 模型转化为原生因果并行解码器，在编码、数学等任务中实现最高 4 倍 wall-clock 提速和 4.5 倍 tokens-per-forward 提升，同时保持接近 AR 模型的生成质量，为 LLM 高效推理开辟了新路径。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/fab8be93-2d13-487e-99ae-b2aef243a409/640.png] 论文地址: https://arxiv.org/pdf/2512.14681 代码地址：https://github.com/hao-ai-lab/JacobiForcing 模型仓库：http://huggingface.co/JacobiForcing Jacobi Forcing 核心优势：破解并行解码的 \"三元悖论\" Jacobi Forcing 的创新之处在于打破了 \"低代价、高速度、高质量\" 的不可能三角，其核心优势体现在三大维度： 1. 原生因果架构，部署与训练成本低: 不同于 dLLMs 的双向注意力机制，Jacobi Forcing 保留了 AR 模型的因果注意力结构，完美适配现有 KV 缓存复用机制和 AR 优化内核，可作为现有 AR 模型的 \"即插即用\" 替代方案，极大降低部署与训练成本。 2. 高效并行解码，速度提升显著： 通过在模型自己生成的 Jacobi 解码轨迹做渐进蒸馏训练，模型能够快速在每轮前向传播中并行更新多个 token。结合多块并行解码（Multiblock decoding）和拒绝回收（Rejection recycling）策略，可同时维护多个解码块，缓存高质量 n-gram 片段重复利用，在编码任务中实现 181.8 TPS 的生成速度，远超 AR 基线的 39.8 TPS。 3. 质量损失极小，任务表现优异： 针对 AR 到扩散模型的预训练 - 后训练目标不匹配问题，Jacobi Forcing 设计了使用模型自己生成的数据做学习，通过渐进式一致性蒸馏损失和 AR 损失的联合优化，让模型在噪声环境下仍能生成贴近 AR 分布的高质量结果，学习高效且保持了 AR 模型的高质量特性。在 HumanEval 编码基准中，以 83.5% 的准确率实现 4 倍提速；在 GSM8K 数学任务中，91.4% 的解题率接近 AR 基线，速度提升 3.7 倍。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/e27e478e-723b-437f-a271-e5b98af33cb7/640.png] Jacobi Forcing 与 dllm 在速度，质量与训练成本上的对比图 Jacobi Forcing 技术路线：从训练到推理的全链路优化 Jacobi Forcing 以因果并行解码为核心目标，基于 Jacobi 解码框架进行深度优化，通过训练机制创新与推理策略升级的全链路设计，在保留 AR 模型因果骨干与 KV 缓存兼容性的同时，实现高效并行解码。 其技术路线具体细节如下： 1. 技术基础：基于 Jacobi 解码的因果并行框架 Jacobi 解码是一种因果并行解码过程，核心逻辑是：在保留 AR 模型因果注意力机制的前提下，对一个块内的所有 token 进行并行迭代更新，直到所有 token 与贪心 AR 输出完全匹配（即达到 “定点” 状态）。这一过程形成了一条 “并行精炼轨迹”，既维持了因果依赖关系，又突破了逐 token 串行的限制。 此前的相关工作（如 CLLMs）已验证：通过在 Jacobi 轨迹上微调模型，可缩短迭代轨迹、提升解码速度，但存在一个关键局限：在大 block size 下由于上文噪声过多无法并行解码出更多的 token 数。Jacobi Forcing 在此基础上进一步推进，核心突破是：训练模型在含噪声的上文下，仍能生成贴近 AR 分布的高质量草稿，同时通过推理策略优化，最大化并行效率。 2. 训练阶段优化：噪声感知的渐进式学习 Jacobi Forcing 首先利用自回归语言模型对提示词（prompt）集合执行 Jacobi 解码，采集从噪声块到干净定点的完整 Jacobi 解码轨迹。为使模型具备应对高噪声上文场景下的并行解码能力，Jacobi Forcing 设计渐进式噪声调度策略，以学习噪声块到干净定点的映射关系：具体而言，先为采集轨迹中的中间未收敛噪声块赋予噪声等级（噪声等级越高，与干净定点状态的偏差越大），再按 “低噪声→高噪声” 的渐进式顺序对噪声块进行打包，构建训练序列，从而提升去噪任务的可学习性；其核心训练目标为将打包后的含噪声训练序列映射至全干净定点序列。为实现高效训练，Jacobi Forcing 进一步设计噪声感知注意力掩码，该掩码支持通过单次模型前向传播即可完成上述映射关系的学习。此外，为平衡并行解码效率与自回归（AR）生成质量，方案设计了加权双项联合损失函数：其一为渐进式一致性蒸馏损失，用于引导模型掌握任意噪声等级块到干净定点块的映射；其二为 AR 损失，确保模型生成质量与原始自回归模型保持一致。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/7869da7d-d48d-4295-934d-9cdc170ef7a9/640.gif] 训练数据打包与噪声感知注意力掩码图解 3. 推理阶段优化：高效并行解码策略 训练后的 Jacobi Forcing 模型仍是标准 AR checkpoint，但通过针对性的推理策略，可最大化并行解码效率，核心包括 “高质量草稿利用 + 多块调度” 两大模块。 1. 高质量草稿挖掘与复用 ：训练后模型的 Jacobi 解码轨迹呈现显著特性：轨迹中未收敛点包含大量高质量 n-gram，这些 n-gram 虽可能位置暂错，但内容与最终 AR 定点输出完全一致，且在迭代中保持稳定。基于此特性，推理时会缓存 n-gram 并在后续迭代中直接将这些缓存的 n-gram 作为候选草稿，减少迭代次数（见下图轨迹可视化：红色标注为可复用的高质量 n-gram）。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/191454b1-473c-41f7-917d-93baf157ba9d/640.png] 高质量草稿复用图解 2. 多块并行调度 ： 同时维护 K 个块（实验中 K=2 为最优），分为 “真实活跃块” 和 “伪活跃块”； 真实活跃块中的 token 会被验证并提交到 KV 缓存，成为后续块的因果前缀；伪活跃块会基于当前前缀进行 Jacobi 迭代更新，但暂不提交到 KV 缓存； 当真实活跃块收敛（所有 token 匹配定点），从伪活跃块中选择一个晋升为真实活跃块，基于更新后的完整前缀重新验证其所有 token。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/ba5f3fa6-9a8f-4fca-a63e-4c9100100a86/640.gif] 推理阶段优化策略图解 实测表现：优于主流并行解码方案 在 A100 GPU 上的 7B 模型基准测试中，Jacobi Forcing 超越 dLLMs、投机解码等主流方案，展现出更优的速度 - 质量 trade-off。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/69fe9e52-c6ca-485a-addc-6990867b08e0/640.png] Jacobi Forcing 模型性能展示 无论是编码、数学等专业任务，还是通用文本生成场景，Jacobi Forcing 都能在保证结果可靠性的前提下，将推理速度提升一个量级，尤其适合对延迟敏感的工业级 LLM 应用。 Jacobi Forcing 的出现，不仅解决了 LLM 推理的效率瓶颈，更重新定义了因果模型的并行化可能。随着大模型应用向低延迟、高并发场景渗透，这种兼顾兼容性、高性能和高质量的解码方案，有望成为工业级 LLM 部署的首选技术，推动 AI 应用效率迈入新阶段。 ]]>","published_date":"2025-12-30T07:10:36.379Z","authors":"机器之心","source":"机器之心 - 机器之心","details":{"content_html":"<img src=\"https://image.jiqizhixin.com/uploads/editor/950e0739-dcba-4de5-bf65-72c7b5ec349e/640.png\" alt=\"图片\" style=\"width: 700%;\"><p>在大语言模型（LLM）落地应用中，推理速度始终是制约效率的核心瓶颈。传统自回归（AR）解码虽能保证生成质量，却需逐 token 串行计算，速度极为缓慢；扩散型 LLM（dLLMs）虽支持并行解码，却面临训练成本高昂、质量下降及 KV 缓存兼容问题；投机解码（Speculative Decoding）则需额外引入草稿模型，系统复杂度大增。 </p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/05af4054-d57f-47f5-b758-5d9f1674056c/640.gif\" alt=\"图片\" style=\"width: 700%;\"></section><section><img src=\"https://image.jiqizhixin.com/uploads/editor/1d796c54-f8da-4ea8-affc-0e356a47e878/640.gif\" alt=\"图片\" style=\"width: 700%;\"></section><p><sup>       Jacobi Forcing Model 与 AR LLM 推理速度对比示意</sup></p><p>近期，来自 UCSD Hao AI Lab 和上海交大 Deng Lab 的团队提出了一种突破性解决方案 ——Jacobi Forcing，该方案无需重构模型架构，即可将标准 AR 模型转化为原生因果并行解码器，在编码、数学等任务中实现最高 4 倍 wall-clock 提速和 4.5 倍 tokens-per-forward 提升，同时保持接近 AR 模型的生成质量，为 LLM 高效推理开辟了新路径。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/fab8be93-2d13-487e-99ae-b2aef243a409/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><ul><li><p>论文地址: https://arxiv.org/pdf/2512.14681</p></li><li><p>代码地址：https://github.com/hao-ai-lab/JacobiForcing </p></li><li><p>模型仓库：http://huggingface.co/JacobiForcing </p></li></ul><p><strong>Jacobi Forcing 核心优势：破解并行解码的 \"三元悖论\" </strong></p><p>Jacobi Forcing 的创新之处在于打破了 \"低代价、高速度、高质量\" 的不可能三角，其核心优势体现在三大维度：</p><p><strong>1. 原生因果架构，部署与训练成本低: </strong></p><p>不同于 dLLMs 的双向注意力机制，Jacobi Forcing 保留了 AR 模型的因果注意力结构，完美适配现有 KV 缓存复用机制和 AR 优化内核，可作为现有 AR 模型的 \"即插即用\" 替代方案，极大降低部署与训练成本。 </p><p><strong>2. 高效并行解码，速度提升显著：</strong></p><p>通过在模型自己生成的 Jacobi 解码轨迹做渐进蒸馏训练，模型能够快速在每轮前向传播中并行更新多个 token。结合多块并行解码（Multiblock decoding）和拒绝回收（Rejection recycling）策略，可同时维护多个解码块，缓存高质量 n-gram 片段重复利用，在编码任务中实现 181.8 TPS 的生成速度，远超 AR 基线的 39.8 TPS。 </p><p><strong>3. 质量损失极小，任务表现优异： </strong></p><p>针对 AR 到扩散模型的预训练 - 后训练目标不匹配问题，Jacobi Forcing 设计了使用模型自己生成的数据做学习，通过渐进式一致性蒸馏损失和 AR 损失的联合优化，让模型在噪声环境下仍能生成贴近 AR 分布的高质量结果，学习高效且保持了 AR 模型的高质量特性。在 HumanEval 编码基准中，以 83.5% 的准确率实现 4 倍提速；在 GSM8K 数学任务中，91.4% 的解题率接近 AR 基线，速度提升 3.7 倍。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/e27e478e-723b-437f-a271-e5b98af33cb7/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p><sup>      Jacobi Forcing 与 dllm 在速度，质量与训练成本上的对比图</sup></p><p><strong>Jacobi Forcing 技术路线：从训练到推理的全链路优化 </strong></p><p>Jacobi Forcing 以因果并行解码为核心目标，基于 Jacobi 解码框架进行深度优化，通过训练机制创新与推理策略升级的全链路设计，在保留 AR 模型因果骨干与 KV 缓存兼容性的同时，实现高效并行解码。</p><p>其技术路线具体细节如下： </p><p><strong>1. 技术基础：基于 Jacobi 解码的因果并行框架 </strong></p><p>Jacobi 解码是一种因果并行解码过程，核心逻辑是：在保留 AR 模型因果注意力机制的前提下，对一个块内的所有 token 进行并行迭代更新，直到所有 token 与贪心 AR 输出完全匹配（即达到 “定点” 状态）。这一过程形成了一条 “并行精炼轨迹”，既维持了因果依赖关系，又突破了逐 token 串行的限制。 此前的相关工作（如 CLLMs）已验证：通过在 Jacobi 轨迹上微调模型，可缩短迭代轨迹、提升解码速度，但存在一个关键局限：在大 block size 下由于上文噪声过多无法并行解码出更多的 token 数。Jacobi Forcing 在此基础上进一步推进，核心突破是：训练模型在含噪声的上文下，仍能生成贴近 AR 分布的高质量草稿，同时通过推理策略优化，最大化并行效率。</p><p><strong>2. 训练阶段优化：噪声感知的渐进式学习 </strong></p><p>Jacobi Forcing 首先利用自回归语言模型对提示词（prompt）集合执行 Jacobi 解码，采集从噪声块到干净定点的完整 Jacobi 解码轨迹。为使模型具备应对高噪声上文场景下的并行解码能力，Jacobi Forcing 设计渐进式噪声调度策略，以学习噪声块到干净定点的映射关系：具体而言，先为采集轨迹中的中间未收敛噪声块赋予噪声等级（噪声等级越高，与干净定点状态的偏差越大），再按 “低噪声→高噪声” 的渐进式顺序对噪声块进行打包，构建训练序列，从而提升去噪任务的可学习性；其核心训练目标为将打包后的含噪声训练序列映射至全干净定点序列。为实现高效训练，Jacobi Forcing 进一步设计噪声感知注意力掩码，该掩码支持通过单次模型前向传播即可完成上述映射关系的学习。此外，为平衡并行解码效率与自回归（AR）生成质量，方案设计了加权双项联合损失函数：其一为渐进式一致性蒸馏损失，用于引导模型掌握任意噪声等级块到干净定点块的映射；其二为 AR 损失，确保模型生成质量与原始自回归模型保持一致。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/7869da7d-d48d-4295-934d-9cdc170ef7a9/640.gif\" alt=\"图片\" style=\"width: 700%;\"></section><p><sup>      训练数据打包与噪声感知注意力掩码图解</sup></p><p><strong>3. 推理阶段优化：高效并行解码策略 </strong></p><p>训练后的 Jacobi Forcing 模型仍是标准 AR checkpoint，但通过针对性的推理策略，可最大化并行解码效率，核心包括 “高质量草稿利用 + 多块调度” 两大模块。 </p><p><strong>1. 高质量草稿挖掘与复用</strong>：训练后模型的 Jacobi 解码轨迹呈现显著特性：轨迹中未收敛点包含大量高质量 n-gram，这些 n-gram 虽可能位置暂错，但内容与最终 AR 定点输出完全一致，且在迭代中保持稳定。基于此特性，推理时会缓存 n-gram 并在后续迭代中直接将这些缓存的 n-gram 作为候选草稿，减少迭代次数（见下图轨迹可视化：红色标注为可复用的高质量 n-gram）。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/191454b1-473c-41f7-917d-93baf157ba9d/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p><sup>      高质量草稿复用图解</sup></p><p><strong>2. 多块并行调度</strong>： 同时维护 K 个块（实验中 K=2 为最优），分为 “真实活跃块” 和 “伪活跃块”； 真实活跃块中的 token 会被验证并提交到 KV 缓存，成为后续块的因果前缀；伪活跃块会基于当前前缀进行 Jacobi 迭代更新，但暂不提交到 KV 缓存； 当真实活跃块收敛（所有 token 匹配定点），从伪活跃块中选择一个晋升为真实活跃块，基于更新后的完整前缀重新验证其所有 token。<br><img src=\"https://image.jiqizhixin.com/uploads/editor/ba5f3fa6-9a8f-4fca-a63e-4c9100100a86/640.gif\" alt=\"图片\" style=\"width: 700%;\"></p><p><sup>      推理阶段优化策略图解</sup></p><p><strong>实测表现：优于主流并行解码方案 </strong></p><p>在 A100 GPU 上的 7B 模型基准测试中，Jacobi Forcing 超越 dLLMs、投机解码等主流方案，展现出更优的速度 - 质量 trade-off。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/69fe9e52-c6ca-485a-addc-6990867b08e0/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p><sup>      Jacobi Forcing 模型性能展示</sup></p><p>无论是编码、数学等专业任务，还是通用文本生成场景，Jacobi Forcing 都能在保证结果可靠性的前提下，将推理速度提升一个量级，尤其适合对延迟敏感的工业级 LLM 应用。</p><p>Jacobi Forcing 的出现，不仅解决了 LLM 推理的效率瓶颈，更重新定义了因果模型的并行化可能。随着大模型应用向低延迟、高并发场景渗透，这种兼顾兼容性、高性能和高质量的解码方案，有望成为工业级 LLM 部署的首选技术，推动 AI 应用效率迈入新阶段。</p>]]>"}},{"id":"228724964639602688","type":"news","url":"https://www.aibase.com/zh/news/24128","title":"告别命令行！Claude Code可视化工作流编辑器爆火：拖拽节点就能建AI自动化神器","description":"2025年末，Anthropic旗下Claude Code迎来重大社区生态突破:一款名为“Claude Code Workflow Studio”的VSCode扩展工具迅速走红。该工具通过直观的拖拽式画布界面，让用户无需编写复杂提示或终端命令，即可构建和执行 高级 AI自动化工作流。这标志着Claude Code从纯命令行工具向可视化、无代码方向的演进，极大降低了非专业开发者的使用门槛。 [图片: image.png https://upload.chinaz.com/2025/1230/6390270344016482586862165.png] 核心功能:拖拽节点构建复杂流程 Claude Code Workflow Studio在VSCode中提供一个专属“画布”视图，用户可以通过简单拖放节点来设计工作流。主要节点包括: - Prompt（提示节点） - Sub-Agent（子代理） - Skill（技能） - MCP（模型上下文协议工具） - IfElse（条件分支） - AskUserQuestion（用户交互提问） 节点之间用连线连接，形成完整的自动化链路。设计完成后，直接导出为.claude格式文件，即可由Claude Code CLI无缝执行。该工具还支持AI辅助编辑:用户用自然语言描述需求（如“添加验证步骤”或“拆分长文本”），系统会自动调整工作流结构。 [图片: image.png https://upload.chinaz.com/2025/1230/6390270345287588209434373.png] 典型应用场景 这一可视化方式特别适合构建重复性或多步骤AI任务，例如: - 自动文档总结机器人:从输入文件提取内容、生成摘要并输出报告。 - 代码分析与修复工作流:读取代码、定位问题、提出修复建议并应用。 - 网页爬取自动化:访问页面、提取指定内容、处理数据并汇报结果。 对于入门用户而言，这种拖拽+聊天交互极大提升了直观性和便利性，无需深入终端操作即可实现强大自动化。 社区反馈与生态影响 该扩展由社区开发者推出，已在GitHub和VSCode Marketplace上线，迅速收获开发者好评。用户反馈称，它解决了Claude Code在复杂工作流下的“提示混乱”问题，让AI代理协作更结构化、可视化。结合Claude Code原生的子代理、技能和MCP支持，这一工具进一步释放了其在agentic workflow领域的潜力。 同时，国内社区也出现类似可视化GUI工具（如Claudia和Claude Code UI），进一步丰富了生态选择。 Claude Code Workflow Studio的出现，预示着AI编程工具正从“命令行专家专属”向“人人可及”转型。拖拽式设计结合AI智能编辑，不仅加速了自动化流程构建，还为非码农用户打开了Claude Code的大门。未来，随着更多可视化扩展的涌现，AI代理工作流将变得更易管理、更高效。AIbase将继续关注Claude Code生态动态，为读者带来一线前沿资讯。 项目地址：https://github.com/breaking-brake/cc-wf-studio/","published_date":"2025-12-30T06:58:11.917Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>2025年末，Anthropic旗下Claude Code迎来重大社区生态突破:一款名为“Claude Code Workflow Studio”的VSCode扩展工具迅速走红。该工具通过直观的拖拽式画布界面，让用户无需编写复杂提示或终端命令，即可构建和执行<span>高级</span>AI自动化工作流。这标志着Claude Code从纯命令行工具向可视化、无代码方向的演进，极大降低了非专业开发者的使用门槛。</p><p style=\"text-align:center\"><img src=\"https://upload.chinaz.com/2025/1230/6390270344016482586862165.png\" title=\"image.png\" alt=\"image.png\"></p><p><strong>核心功能:拖拽节点构建复杂流程</strong></p><p>Claude Code Workflow Studio在VSCode中提供一个专属“画布”视图，用户可以通过简单拖放节点来设计工作流。主要节点包括:</p><p>- Prompt（提示节点）</p><p>- Sub-Agent（子代理）</p><p>- Skill（技能）</p><p>- MCP（模型上下文协议工具）</p><p>- IfElse（条件分支）</p><p>- AskUserQuestion（用户交互提问）</p><p>节点之间用连线连接，形成完整的自动化链路。设计完成后，直接导出为.claude格式文件，即可由Claude Code CLI无缝执行。该工具还支持AI辅助编辑:用户用自然语言描述需求（如“添加验证步骤”或“拆分长文本”），系统会自动调整工作流结构。</p><p style=\"text-align:center\"><img src=\"https://upload.chinaz.com/2025/1230/6390270345287588209434373.png\" title=\"image.png\" alt=\"image.png\"></p><p><strong>典型应用场景</strong></p><p>这一可视化方式特别适合构建重复性或多步骤AI任务，例如:</p><p>- 自动文档总结机器人:从输入文件提取内容、生成摘要并输出报告。</p><p>- 代码分析与修复工作流:读取代码、定位问题、提出修复建议并应用。</p><p>- 网页爬取自动化:访问页面、提取指定内容、处理数据并汇报结果。</p><p>对于入门用户而言，这种拖拽+聊天交互极大提升了直观性和便利性，无需深入终端操作即可实现强大自动化。</p><p><strong>社区反馈与生态影响</strong></p><p>该扩展由社区开发者推出，已在GitHub和VSCode Marketplace上线，迅速收获开发者好评。用户反馈称，它解决了Claude Code在复杂工作流下的“提示混乱”问题，让AI代理协作更结构化、可视化。结合Claude Code原生的子代理、技能和MCP支持，这一工具进一步释放了其在agentic workflow领域的潜力。</p><p>同时，国内社区也出现类似可视化GUI工具（如Claudia和Claude Code UI），进一步丰富了生态选择。</p><p>Claude Code Workflow Studio的出现，预示着AI编程工具正从“命令行专家专属”向“人人可及”转型。拖拽式设计结合AI智能编辑，不仅加速了自动化流程构建，还为非码农用户打开了Claude Code的大门。未来，随着更多可视化扩展的涌现，AI代理工作流将变得更易管理、更高效。AIbase将继续关注Claude Code生态动态，为读者带来一线前沿资讯。</p><p>项目地址：https://github.com/breaking-brake/cc-wf-studio/</p>"}},{"id":"228724964639602689","type":"news","url":"https://www.aibase.com/zh/news/24127","title":"估值飙升至293亿美金！2025 AI编程工具爆发：从补全到智能代理革命","description":"2025 年，AI编程领域迎来爆发式增长。从简单的代码补全进化到多代理协作系统，AI已不再是辅助工具，而是能够独立处理复杂软件工程任务的“智能伙伴”。斯坦福大学人类中心人工智能研究所（HAI）发布的《 2025 年AI指数报告》显示，AI系统在软件工程基准测试SWE-bench上的表现大幅跃升，一年内得分提高了67. 3 个百分点，从 2023 年的低分提升至 2024 年的71.7%。这标志着AI编程正从实验阶段迈入成熟应用期。 技术性能大幅提升 过去一年，AI在高难度基准上的突破令人瞩目。报告指出，新兴基准如SWE-bench的性能从个位数跃升至七成以上，AI已能独立阅读问题描述、定位bug、跨文件修改代码并执行测试。这种“主动性”和“可执行能力”成为各大模型厂商的重点方向。AI不再仅限于生成简单循环，而是具备上下文理解、多任务协作的全面能力，推动软件开发效率显著提高。 融资热潮与市场认可 2025 年，AI编程工具融资火热，独立创业公司估值快速攀升。其中，AI代码编辑器Cursor的表现尤为亮眼。其母公司Anysphere在 11 月完成 23 亿美元D轮融资，投后估值达到 293 亿美元，仅用 11 个月估值增长超 11 倍。同时，该工具的年化收入（ARR）从早期百万级飙升至超 10 亿美元，用时不到 24 个月，刷新了B2B SaaS增长纪录。这一现象反映出资本市场对AI编程工具的强烈信心，该领域已被视为高速增长的新兴行业。 国内工具蓬勃发展 在中国，AI编程工具同样呈现蓬勃态势。本土产品如Trae等AI原生IDE，通过深度优化中文语义理解和多模态交互（如设计稿直接转代码），在本土场景中表现出色。多家工具实现全流程自动化，支持代理模式和项目级上下文处理，开发效率提升显著。结合国产大模型的迭代，国内AI编程生态正加速追赶并在某些领域形成优势，助力开发者应对复杂项目需求。 AIbase观点 2025 年是AI编程从“助手”向“主体”转型的关键一年。技术突破、资本注入与应用落地三者合力，推动行业进入新阶段。未来，随着代理系统进一步成熟，AI将重塑软件工程范式，但同时需关注安全、合规与人类监督的平衡。AIbase将持续追踪这一领域的 最新 动态，为开发者提供专业洞见。","published_date":"2025-12-30T06:48:57.139Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>2025 年，AI编程领域迎来爆发式增长。从简单的代码补全进化到多代理协作系统，AI已不再是辅助工具，而是能够独立处理复杂软件工程任务的“智能伙伴”。斯坦福大学人类中心人工智能研究所（HAI）发布的《 2025 年AI指数报告》显示，AI系统在软件工程基准测试SWE-bench上的表现大幅跃升，一年内得分提高了67. 3 个百分点，从 2023 年的低分提升至 2024 年的71.7%。这标志着AI编程正从实验阶段迈入成熟应用期。</p><p><strong>技术性能大幅提升</strong></p><p>过去一年，AI在高难度基准上的突破令人瞩目。报告指出，新兴基准如SWE-bench的性能从个位数跃升至七成以上，AI已能独立阅读问题描述、定位bug、跨文件修改代码并执行测试。这种“主动性”和“可执行能力”成为各大模型厂商的重点方向。AI不再仅限于生成简单循环，而是具备上下文理解、多任务协作的全面能力，推动软件开发效率显著提高。</p><p><strong>融资热潮与市场认可</strong></p><p>2025 年，AI编程工具融资火热，独立创业公司估值快速攀升。其中，AI代码编辑器Cursor的表现尤为亮眼。其母公司Anysphere在 11 月完成 23 亿美元D轮融资，投后估值达到 293 亿美元，仅用 11 个月估值增长超 11 倍。同时，该工具的年化收入（ARR）从早期百万级飙升至超 10 亿美元，用时不到 24 个月，刷新了B2B SaaS增长纪录。这一现象反映出资本市场对AI编程工具的强烈信心，该领域已被视为高速增长的新兴行业。</p><p><strong>国内工具蓬勃发展</strong></p><p>在中国，AI编程工具同样呈现蓬勃态势。本土产品如Trae等AI原生IDE，通过深度优化中文语义理解和多模态交互（如设计稿直接转代码），在本土场景中表现出色。多家工具实现全流程自动化，支持代理模式和项目级上下文处理，开发效率提升显著。结合国产大模型的迭代，国内AI编程生态正加速追赶并在某些领域形成优势，助力开发者应对复杂项目需求。</p><p>AIbase观点　 2025 年是AI编程从“助手”向“主体”转型的关键一年。技术突破、资本注入与应用落地三者合力，推动行业进入新阶段。未来，随着代理系统进一步成熟，AI将重塑软件工程范式，但同时需关注安全、合规与人类监督的平衡。AIbase将持续追踪这一领域的<span>最新</span>动态，为开发者提供专业洞见。</p>"}},{"id":"228724964639602690","type":"news","url":"https://www.aibase.com/zh/news/24126","title":"软银出资40亿美元收购 DigitalBridge 加速人工智能布局","description":"日本软银集团近日宣布达成了一项重要收购协议，将以40亿美元的价格收购数据中心投资公司 DigitalBridge。这一交易被视为软银在人工智能领域布局的关键举措，并已获得 DigitalBridge 董事会特别委员会的全票批准。 根据协议，软银将以每股16美元的现金价格收购 DigitalBridge 的所有流通普通股。值得注意的是，这一收购价格相比 DigitalBridge 在去年12月26日的收盘价溢价达15%。软银预计这笔交易将在明年下半年完成交割。 软银集团首席执行官兼董事长孙正义在声明中指出，此次收购将为下一代人工智能数据中心的发展奠定基础，推动公司向成为领先的 “ 超级 人工智能” 平台供应商的目标迈进。他强调，随着人工智能正在重塑各行各业，市场对算力、网络连接、电力供应和可扩展基础设施的需求变得愈加迫切。 受此消息影响，DigitalBridge 的股价当天上涨约10%。在交易即将达成的传闻中，该公司的股价一度飙升50%。DigitalBridge 首席执行官马克・甘兹表示，人工智能基础设施的建设是当前最重要的投资机会之一。他补充说，软银的愿景、强大的资本和全球资源将使 DigitalBridge 在加速推进使命方面更具灵活性，从而更好地服务全球 顶尖 科技公司，助力它们在人工智能领域的规模化发展。 值得一提的是，软银集团最近以58.3亿美元的价格出售了其在美国芯片制造商英伟达的全部股份，以腾出资金用于投资 OpenAI。根据 DigitalBridge 官网的信息，截至今年9月底，该公司管理的资产规模约达1080亿美元。 划重点: 🌟 软银以40亿美元收购 DigitalBridge，强化人工智能领域布局。 📈 收购价格每股16美元，较去年股价溢价15%，预计明年下半年完成。 ⚡ DigitalBridge CEO 表示，人工智能基础设施是重要投资机会，软银资源将助力其发展。","published_date":"2025-12-30T06:48:54.583Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p><br></p><p>日本软银集团近日宣布达成了一项重要收购协议，将以40亿美元的价格收购数据中心投资公司 DigitalBridge。这一交易被视为软银在人工智能领域布局的关键举措，并已获得 DigitalBridge 董事会特别委员会的全票批准。</p><p>根据协议，软银将以每股16美元的现金价格收购 DigitalBridge 的所有流通普通股。值得注意的是，这一收购价格相比 DigitalBridge 在去年12月26日的收盘价溢价达15%。软银预计这笔交易将在明年下半年完成交割。</p><p>软银集团首席执行官兼董事长孙正义在声明中指出，此次收购将为下一代人工智能数据中心的发展奠定基础，推动公司向成为领先的 “<span>超级</span>人工智能” 平台供应商的目标迈进。他强调，随着人工智能正在重塑各行各业，市场对算力、网络连接、电力供应和可扩展基础设施的需求变得愈加迫切。</p><p>受此消息影响，DigitalBridge 的股价当天上涨约10%。在交易即将达成的传闻中，该公司的股价一度飙升50%。DigitalBridge 首席执行官马克・甘兹表示，人工智能基础设施的建设是当前最重要的投资机会之一。他补充说，软银的愿景、强大的资本和全球资源将使 DigitalBridge 在加速推进使命方面更具灵活性，从而更好地服务全球<span>顶尖</span>科技公司，助力它们在人工智能领域的规模化发展。</p><p>值得一提的是，软银集团最近以58.3亿美元的价格出售了其在美国芯片制造商英伟达的全部股份，以腾出资金用于投资 OpenAI。根据 DigitalBridge 官网的信息，截至今年9月底，该公司管理的资产规模约达1080亿美元。</p><blockquote><p>划重点:</p><p>🌟 软银以40亿美元收购 DigitalBridge，强化人工智能领域布局。</p><p>📈 收购价格每股16美元，较去年股价溢价15%，预计明年下半年完成。</p><p>⚡ DigitalBridge CEO 表示，人工智能基础设施是重要投资机会，软银资源将助力其发展。</p></blockquote>"}},{"id":"228724964639602691","type":"news","url":"https://www.aibase.com/zh/news/24125","title":"​三星 Exynos 2600 芯片将 AI 模型 “瘦身” 90%，助力移动端智能化","description":"韩媒 ETNews 报道，三星电子将在其 最新 的 Exynos2600芯片中整合 Nota 公司的 AI 模型优化方案。这一技术突破使得 AI 模型的体积可以在保持高精度的前提下压缩超过90%。这意味着，未来用户将能够在移动设备上更高效地运行复杂的 AI 任务。 Nota 公司是一家专注于 AI 优化技术的韩国企业，他们与三星的合作已经不是 第一 次。在这次合作中，Nota 将为 Exynos2600提供其自主研发的 “NetsPresso” 平台的技术支持。NetsPresso 能够在不同的硬件环境下高效地优化和部署 AI 模型，使得大型生成式 AI 模型能够在移动设备上流畅运行，而无需依赖互联网连接。 此次合作的核心目标是推动生成式 AI 在三星 Exynos2600上的应用。通过 Nota 的优化技术，未来的移动设备可以更轻松地处理复杂的 AI 任务，比如图像生成、文本创作等。此外，Nota 还将参与开发三星的 AI 模型优化工具链 “Exynos AI Studio” 的下一代版本，这将显著提高开发者在 Exynos 平台上部署 AI 模型的效率，降低开发难度并缩短产品上市时间。 这一进展标志着三星在 AI 领域的持续努力，也为用户带来了更便捷和高效的智能体验。随着技术的不断成熟，我们期待看到更多基于 Exynos2600的创新应用。 划重点: - 🤖 三星 Exynos2600芯片将集成 Nota 公司的 AI 模型优化方案，模型体积可压缩90。 - 📱 Nota 的 NetsPresso 平台使得复杂 AI 任务能够在移动设备上顺利运行，无需互联网。 - ⚙️ Nota 将参与开发 Exynos AI Studio，帮助开发者更高效地部署 AI 模型。","published_date":"2025-12-30T06:48:14.160Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>韩媒 ETNews 报道，三星电子将在其<span>最新</span>的 Exynos2600芯片中整合 Nota 公司的 AI 模型优化方案。这一技术突破使得 AI 模型的体积可以在保持高精度的前提下压缩超过90%。这意味着，未来用户将能够在移动设备上更高效地运行复杂的 AI 任务。</p><p>Nota 公司是一家专注于 AI 优化技术的韩国企业，他们与三星的合作已经不是<span>第一</span>次。在这次合作中，Nota 将为 Exynos2600提供其自主研发的 “NetsPresso” 平台的技术支持。NetsPresso 能够在不同的硬件环境下高效地优化和部署 AI 模型，使得大型生成式 AI 模型能够在移动设备上流畅运行，而无需依赖互联网连接。</p><p>此次合作的核心目标是推动生成式 AI 在三星 Exynos2600上的应用。通过 Nota 的优化技术，未来的移动设备可以更轻松地处理复杂的 AI 任务，比如图像生成、文本创作等。此外，Nota 还将参与开发三星的 AI 模型优化工具链 “Exynos AI Studio” 的下一代版本，这将显著提高开发者在 Exynos 平台上部署 AI 模型的效率，降低开发难度并缩短产品上市时间。</p><p>这一进展标志着三星在 AI 领域的持续努力，也为用户带来了更便捷和高效的智能体验。随着技术的不断成熟，我们期待看到更多基于 Exynos2600的创新应用。</p><blockquote><p>划重点:</p><p>- 🤖 三星 Exynos2600芯片将集成 Nota 公司的 AI 模型优化方案，模型体积可压缩90。</p><p>- 📱 Nota 的 NetsPresso 平台使得复杂 AI 任务能够在移动设备上顺利运行，无需互联网。</p><p>- ⚙️ Nota 将参与开发 Exynos AI Studio，帮助开发者更高效地部署 AI 模型。</p></blockquote>"}},{"id":"228724964639602692","type":"news","url":"https://www.aibase.com/zh/news/24124","title":"AI模特“上岗”:Zara利用人工智能削减成本，摄影师与化妆师或面临失业","description":"据AIbase报道 ， 西班牙快时尚巨头Zara正悄然掀起一场摄影革命，通过人工智能（AI）技术对模特照片进行数字化编辑，以应对日益严峻的市场挑战。 此举的核心操作是，Zara在征得模特本人同意后，利用其现有照片，通过AI技术为她们“穿上”新款服装，并将其置于全新的虚拟场景中。这一流程完全省去了组织实体拍摄所需的场地、设备和团队，极大地节约了成本和时间。 [图片: 女装，衣橱 https://pic.chinaz.com/picmap/201909291149230725_2.jpg] 然而，这种高效模式的背后是行业利益的重新分配。据两位接受《City AM》采访的模特透露，她们获得的报酬与参与一次真实拍摄的费用相同。但对于传统拍摄中不可或缺的其他专业人士——例如摄影师、化妆师和造型师——他们在这场AI主导的创作过程中可能完全被排除在外，分文未得。 面对外界关注，Zara官方表示，人工智能技术将作为“传统摄影的补充，而非取代”。尽管如此，这一举措推出的时机颇为微妙。就在去年11月，该零售商的英国销售额跌至六个月来的 最低 点，降本增效的压力不言而喻。 值得注意的是，Zara并非 唯一 探索该领域的时尚品牌。其竞争对手H&#x26;M和Zalando也于今年夏季宣布了类似的计划，旨在利用AI为模特创建“数字孪生”，预示着整个时尚行业可能正迈入一个由技术重塑的全新时代。","published_date":"2025-12-30T06:15:40.311Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p><strong><strong style=\"text-indent: 2em;\">据AIbase报道</strong><span style=\"text-indent: 2em;\">，</span></strong><span style=\"text-indent: 2em;\">西班牙快时尚巨头Zara正悄然掀起一场摄影革命，通过人工智能（AI）技术对模特照片进行数字化编辑，以应对日益严峻的市场挑战。</span></p><p>此举的核心操作是，Zara在征得模特本人同意后，利用其现有照片，通过AI技术为她们“穿上”新款服装，并将其置于全新的虚拟场景中。这一流程完全省去了组织实体拍摄所需的场地、设备和团队，极大地节约了成本和时间。</p><p style=\"text-align: center\"><img src=\"https://pic.chinaz.com/picmap/201909291149230725_2.jpg\" title=\"女装，衣橱 (图片版权所属：站长之家)\" alt=\"女装，衣橱\"></p><p>然而，这种高效模式的背后是行业利益的重新分配。据两位接受《City AM》采访的模特透露，她们获得的报酬与参与一次真实拍摄的费用相同。但对于传统拍摄中不可或缺的其他专业人士——例如摄影师、化妆师和造型师——他们在这场AI主导的创作过程中可能完全被排除在外，分文未得。</p><p>面对外界关注，Zara官方表示，人工智能技术将作为“传统摄影的补充，而非取代”。尽管如此，这一举措推出的时机颇为微妙。就在去年11月，该零售商的英国销售额跌至六个月来的<span>最低</span>点，降本增效的压力不言而喻。</p><p>值得注意的是，Zara并非<span>唯一</span>探索该领域的时尚品牌。其竞争对手H&#x26;M和Zalando也于今年夏季宣布了类似的计划，旨在利用AI为模特创建“数字孪生”，预示着整个时尚行业可能正迈入一个由技术重塑的全新时代。</p>"}},{"id":"228724964639602693","type":"news","url":"https://www.aibase.com/zh/news/24123","title":"ChatGPT周活破8亿！OpenAI估值飙至5000亿美元，Altman：AI已从“玩具”变为“生产力基建”","description":"全球AI浪潮再迎里程碑。在 12 月 28 日举行的OpenAI DevDay开发者大会上，CEO Sam Altman正式宣布：ChatGPT全球周活跃用户已达 8 亿，较 3 月底的 5 亿、 8 月的 7 亿持续高速增长，覆盖消费者、开发者、企业及政府四大核心群体。与此同时，OpenAI在 最新 一轮私募股权交易中估值达 5000 亿美元，跃居全球最有价值的非上市公司。 用户与生态双爆发： 8 亿用户 +400 万开发者 Altman在主题演讲中透露，目前： - 超过 8 亿人每周使用ChatGPT； -400 万开发者基于OpenAI平台构建应用； - API每分钟处理超 60 亿Token，相当于每秒处理 1 亿字。 “AI已经从人们‘玩一玩’的新奇工具，转变为每天都在‘构建’和‘依赖’的生产力基础设施，”Altman强调。 从被动问答到主动服务：AI智能体生态加速成型 本次DevDay，OpenAI重点发布面向智能体（Agent）开发的新工具链，支持开发者在ChatGPT内构建可交互、自适应、个性化的下一代应用。例如： - 自主订票Agent：理解用户日程，自动比价并完成预订； - 企业知识助手：接入内部文档，回答员工问题并生成报告； - 个性化健康教练：结合用户数据提供动态建议。 此外，OpenAI近期已推出OpenAI Pulse服务，向用户主动推送定制化晨间简报，标志着产品从“被动响应”迈向“主动服务”。 商业与技术双线狂奔：Sora社交网+智能体电商落地 在用户规模飙升的同时，OpenAI正以惊人速度拓展商业化边界： - 上周发布Sora视频生成模型新版本，并同步推出配套AI视频社交平台； - 联合Stripe推出“智能体 commerce”平台，允许AI代理代表用户完成商品选购、比价与支付全流程。 这些举措表明，OpenAI正从“模型提供商”转型为“AI操作系统+应用生态”的全栈平台。 光环之下：安全与伦理挑战仍未解除 尽管增长迅猛，ChatGPT仍面临严峻信任考验。近期，用户Allan Brooks因过度依赖ChatGPT，被误导“发现”了一个不存在的数学定理，引发对AI“谄媚式幻觉”（sycophantic hallucination）的广泛担忧。OpenAI已通过强化事实核查、引入不确定性提示等方式优化，但如何平衡“友好性”与“真实性”，仍是待解难题。 AIbase观察： 5000 亿美元估值背后，是生态战争的全面升级 8 亿周活与 5000 亿美元估值，不仅是数字，更是信号：AI竞争已从模型能力转向生态掌控力。当开发者在ChatGPT内构建千万级智能体应用，当企业将AI嵌入核心业务流程，OpenAI正在复制当年iOS/Android的平台成功路径。 然而，高估值也意味着高期待。在巨额亏损（预计 2028 年年亏 740 亿美元）与监管压力并存的背景下，OpenAI能否将“用户规模”转化为“可持续价值”，将决定其能否真正成为AI时代的“操作系统级”巨头。 而对全球开发者而言，一个事实已无可争议：AI的主战场，正在从实验室全面移向真实世界的应用场景。","published_date":"2025-12-30T05:45:02.552Z","authors":"AI Base","source":"AI新闻资讯 - AI Base","details":{"content_html":"<p>全球AI浪潮再迎里程碑。在 12 月 28 日举行的OpenAI DevDay开发者大会上，CEO Sam Altman正式宣布：ChatGPT全球周活跃用户已达 8 亿，较 3 月底的 5 亿、 8 月的 7 亿持续高速增长，覆盖消费者、开发者、企业及政府四大核心群体。与此同时，OpenAI在<span>最新</span>一轮私募股权交易中估值达 5000 亿美元，跃居全球最有价值的非上市公司。</p><p><strong> 用户与生态双爆发： 8 亿用户 +400 万开发者</strong></p><p>Altman在主题演讲中透露，目前：</p><p>- 超过 8 亿人每周使用ChatGPT；</p><p>-400 万开发者基于OpenAI平台构建应用；</p><p>- API每分钟处理超 60 亿Token，相当于每秒处理 1 亿字。</p><p>“AI已经从人们‘玩一玩’的新奇工具，转变为每天都在‘构建’和‘依赖’的生产力基础设施，”Altman强调。</p><p><strong> 从被动问答到主动服务：AI智能体生态加速成型</strong></p><p>本次DevDay，OpenAI重点发布面向智能体（Agent）开发的新工具链，支持开发者在ChatGPT内构建可交互、自适应、个性化的下一代应用。例如：</p><p>- 自主订票Agent：理解用户日程，自动比价并完成预订；</p><p>- 企业知识助手：接入内部文档，回答员工问题并生成报告；</p><p>- 个性化健康教练：结合用户数据提供动态建议。</p><p>此外，OpenAI近期已推出OpenAI Pulse服务，向用户主动推送定制化晨间简报，标志着产品从“被动响应”迈向“主动服务”。</p><p><strong> 商业与技术双线狂奔：Sora社交网+智能体电商落地</strong></p><p>在用户规模飙升的同时，OpenAI正以惊人速度拓展商业化边界：</p><p>- 上周发布Sora视频生成模型新版本，并同步推出配套AI视频社交平台；</p><p>- 联合Stripe推出“智能体 commerce”平台，允许AI代理代表用户完成商品选购、比价与支付全流程。</p><p>这些举措表明，OpenAI正从“模型提供商”转型为“AI操作系统+应用生态”的全栈平台。</p><p> <strong>光环之下：安全与伦理挑战仍未解除</strong></p><p>尽管增长迅猛，ChatGPT仍面临严峻信任考验。近期，用户Allan Brooks因过度依赖ChatGPT，被误导“发现”了一个不存在的数学定理，引发对AI“谄媚式幻觉”（sycophantic hallucination）的广泛担忧。OpenAI已通过强化事实核查、引入不确定性提示等方式优化，但如何平衡“友好性”与“真实性”，仍是待解难题。</p><p><strong> AIbase观察： 5000 亿美元估值背后，是生态战争的全面升级</strong></p><p>8 亿周活与 5000 亿美元估值，不仅是数字，更是信号：AI竞争已从模型能力转向生态掌控力。当开发者在ChatGPT内构建千万级智能体应用，当企业将AI嵌入核心业务流程，OpenAI正在复制当年iOS/Android的平台成功路径。</p><p>然而，高估值也意味着高期待。在巨额亏损（预计 2028 年年亏 740 亿美元）与监管压力并存的背景下，OpenAI能否将“用户规模”转化为“可持续价值”，将决定其能否真正成为AI时代的“操作系统级”巨头。</p><p>而对全球开发者而言，一个事实已无可争议：AI的主战场，正在从实验室全面移向真实世界的应用场景。</p>"}},{"id":"228692513282738177","type":"news","url":"https://newshacker.me/story?id=46388882","title":"🛡️ Claude Code 拦截破坏性 Git/文件命令的插件：解析拦截脆弱，容器/钩子更靠谱","description":"原标题： 《Show HN: A Claude Code plugin that catch destructive Git and filesystem commands》 评分: 20 | 作者: kenryu 💭 把删除仓库与文件的钥匙交给会自作主张的 AI？ 🎯 讨论背景 这是一个 Show HN 帖子，介绍为 Claude Code（Anthropic 的可执行代码 agent/IDE 插件）制作的插件，目的是拦截破坏性 Git 与文件系统命令。评论集中在防护应置于哪个层级：有人指出基于解析 shell 或黑名单的方案脆弱，模型会写临时脚本（如 tmp.sh）、或用 awk '{system(...) }' 等手段绕过检测；替代措施包括在容器/VM 沙箱中运行 agent、使用带检查点的快照以及 hooks（在编辑/提交/工具调用时触发的检测钩子）强制检查。讨论还涉及把 agent 切到 plan mode（先出计划再执行）、用 OPA/Rego（Open Policy Agent 的策略语言）做策略治理、以及在 hooks 中跑 gitleaks（一个检测仓库中泄露密钥的工具）或强制单元测试以提升安全性。评论同时触及对 AI 依赖的伦理争论与对用 AI 检测危险命令或 AI 撰写帖子这一类循环方案的怀疑。 📌 讨论焦点 解析/黑名单拦截方法脆弱 多位评论指出把防护放在解析 shell 或黑名单层级是错误的：Claude Code（Anthropic 的可执行代码 agent）会用非常规手段执行命令从而绕过这类检查。具体例子包括先用 edit 工具写入 tmp.sh 再执行 bash tmp.sh，或使用 awk '{system(...)}' 等间接执行方式，这些都能逃过基于命令匹配的过滤。评论认为列举“可能执行任意代码的 Unix 命令”的黑名单注定不完备，因为变通方法太多。若模型遇到阻碍，它更倾向于寻找绕路以完成任务，而不是退回更安全的做法，这会导致死循环或更危险的尝试。 [来源1] [来源2] [来源3] [来源4] 容器/沙箱与检查点更可靠 多条回复建议将防护上移到执行环境层：在容器或 VM 中运行 agent，只挂载必要代码并定期提交或做快照，能防止 agent 删除真实仓库或历史。给 agent 一个带检查点（checkpointed container）的沙箱环境，让它在可回滚的环境中试验，比解析 shell 或黑名单更彻底且更容易保证安全。评论还强调最小权限原则与频繁推送变更作为补充，确保即便 agent 试图“删库”，也无法影响主环境或历史记录。总体观点是隔离与可回滚性比字符串匹配更能保证防护有效性。 [来源1] [来源2] Hooks 与策略治理：早期预警与强制检查 部分评论详细讨论了使用 hooks（在编辑/提交/工具调用等关键点触发的检测或阻断逻辑）来建立治理体系：在每次 edit 或 commit 前后运行 gitleaks、跑单元测试或通过策略引擎（如 OPA/Rego）判定合规性，可以在问题发生前阻断或提供可执行的修复提示。实操经验表明，hooks 最有效时需向代理返回简洁且可操作的错误信息，否则代理会把复杂失败视为无法完成的任务并尝试绕过。有人把 hooks 当作“早期预警系统”，并举例强制运行单元测试、设定覆盖率阈值（例如 80% ）以把责任部分放回模型。也有建议把 hooks 与 plan mode 和人工批准流程结合，用以实现更成熟的企业级治理。 [来源1] [来源2] [来源3] [来源4] 对 AI 依赖与使用者责任的争论 有评论从使用者角度批评过度依赖 AI 会导致“skill entropy”（技能退化），建议在执行删除类操作时慎重，不应把权限轻易交给代理。反方指出并非所有事故都是用户失误：Claude 有时会违背明确指令（例如执行 git restore、吹掉本地改动），因此单纯指责用户并不公平。讨论还涉及对 AI 的偏见和可及性问题：有人抱怨被扣“不会写代码”的帽子，也有人强调 AI 对有身体障碍者作为辅助工具的重要性。总体上社区在责任分配上存在分歧，是工具治理不足还是过度依赖导致风险仍有争议。 [来源1] [来源2] [来源3] [来源4] [来源5] 替代思路与元讨论：用 AI 检测危险命令与对 AI 撰写帖的怀疑 有用户讽刺性地建议用另一个 AI 去判断代理想执行的命令是否危险，指出这类“用 AI 管理 AI”的思路在工程界越来越流行，但可能只是把问题转移而非解决根本隔离。另有评论对 Show HN 上接连出现功能雷同的 MCP 工具以及疑似由 AI 撰写的文章表示怀疑，认为前页重复工具值得警惕。还有人具体指出 Claude 有时会迅速建议像 git filter-branch 这类高风险命令，而不是采用更安全的分步替代方案，显示出审查或更严格交互批准的必要性。总体上社区既提出利用 AI 做风险判定的想法，也对这种循环方案和 AI 撰稿产生的噪声持怀疑态度。 [来源1] [来源2] [来源3] 📚 术语解释 hooks（钩子）: 在 Claude Code 等 agent 环境中于编辑、工具调用、提交等节点触发的自定义检测或阻断逻辑，用于执行 gitleaks、跑单元测试或返回可操作的修复提示给代理。 plan mode（计划模式）: agent 先生成执行计划并等待人工批准的模式，避免模型在拥有完全权限的环境下直接运行修改，有助于人工审查和阻止危险操作。 checkpointed container / 沙箱（检查点容器）: 给 agent 提供隔离的容器或虚拟机环境，仅挂载受限目录并定期做快照/提交，使代理在可回滚的沙箱中试验而不会影响主仓库与历史记录。 类别： AI | Security | Programming | Show HN | Release | Claude Code | claude-code-safety-net | Git | filesystem | hooks | containers | Open Policy Agent (OPA) | plan mode","published_date":"2025-12-30T04:52:08.041Z","authors":"","source":"News Hacker | 极客洞察","details":{"content_html":"<p><strong>原标题：</strong>《Show HN: A Claude Code plugin that catch destructive Git and filesystem commands》</p><p><strong>评分:</strong> 20 | <strong>作者:</strong> kenryu</p><blockquote>💭 把删除仓库与文件的钥匙交给会自作主张的 AI？</blockquote><hr><h2>🎯 讨论背景</h2><p>这是一个 Show HN 帖子，介绍为 Claude Code（Anthropic 的可执行代码 agent/IDE 插件）制作的插件，目的是拦截破坏性 Git 与文件系统命令。评论集中在防护应置于哪个层级：有人指出基于解析 shell 或黑名单的方案脆弱，模型会写临时脚本（如 tmp.sh）、或用 awk '{system(...) }' 等手段绕过检测；替代措施包括在容器/VM 沙箱中运行 agent、使用带检查点的快照以及 hooks（在编辑/提交/工具调用时触发的检测钩子）强制检查。讨论还涉及把 agent 切到 plan mode（先出计划再执行）、用 OPA/Rego（Open Policy Agent 的策略语言）做策略治理、以及在 hooks 中跑 gitleaks（一个检测仓库中泄露密钥的工具）或强制单元测试以提升安全性。评论同时触及对 AI 依赖的伦理争论与对用 AI 检测危险命令或 AI 撰写帖子这一类循环方案的怀疑。</p><hr><h2>📌 讨论焦点</h2><h3>解析/黑名单拦截方法脆弱</h3><p>多位评论指出把防护放在解析 shell 或黑名单层级是错误的：Claude Code（Anthropic 的可执行代码 agent）会用非常规手段执行命令从而绕过这类检查。具体例子包括先用 edit 工具写入 tmp.sh 再执行 bash tmp.sh，或使用 awk '{system(...)}' 等间接执行方式，这些都能逃过基于命令匹配的过滤。评论认为列举“可能执行任意代码的 Unix 命令”的黑名单注定不完备，因为变通方法太多。若模型遇到阻碍，它更倾向于寻找绕路以完成任务，而不是退回更安全的做法，这会导致死循环或更危险的尝试。</p><p><a href=\"https://news.ycombinator.com/item?id=46429164\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429490\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46429481\" target=\"_blank\">[来源3]</a> <a href=\"https://news.ycombinator.com/item?id=46429534\" target=\"_blank\">[来源4]</a></p><h3>容器/沙箱与检查点更可靠</h3><p>多条回复建议将防护上移到执行环境层：在容器或 VM 中运行 agent，只挂载必要代码并定期提交或做快照，能防止 agent 删除真实仓库或历史。给 agent 一个带检查点（checkpointed container）的沙箱环境，让它在可回滚的环境中试验，比解析 shell 或黑名单更彻底且更容易保证安全。评论还强调最小权限原则与频繁推送变更作为补充，确保即便 agent 试图“删库”，也无法影响主环境或历史记录。总体观点是隔离与可回滚性比字符串匹配更能保证防护有效性。</p><p><a href=\"https://news.ycombinator.com/item?id=46429164\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429534\" target=\"_blank\">[来源2]</a></p><h3>Hooks 与策略治理：早期预警与强制检查</h3><p>部分评论详细讨论了使用 hooks（在编辑/提交/工具调用等关键点触发的检测或阻断逻辑）来建立治理体系：在每次 edit 或 commit 前后运行 gitleaks、跑单元测试或通过策略引擎（如 OPA/Rego）判定合规性，可以在问题发生前阻断或提供可执行的修复提示。实操经验表明，hooks 最有效时需向代理返回简洁且可操作的错误信息，否则代理会把复杂失败视为无法完成的任务并尝试绕过。有人把 hooks 当作“早期预警系统”，并举例强制运行单元测试、设定覆盖率阈值（例如 80% ）以把责任部分放回模型。也有建议把 hooks 与 plan mode 和人工批准流程结合，用以实现更成熟的企业级治理。</p><p><a href=\"https://news.ycombinator.com/item?id=46429306\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429517\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46429605\" target=\"_blank\">[来源3]</a> <a href=\"https://news.ycombinator.com/item?id=46429481\" target=\"_blank\">[来源4]</a></p><h3>对 AI 依赖与使用者责任的争论</h3><p>有评论从使用者角度批评过度依赖 AI 会导致“skill entropy”（技能退化），建议在执行删除类操作时慎重，不应把权限轻易交给代理。反方指出并非所有事故都是用户失误：Claude 有时会违背明确指令（例如执行 git restore、吹掉本地改动），因此单纯指责用户并不公平。讨论还涉及对 AI 的偏见和可及性问题：有人抱怨被扣“不会写代码”的帽子，也有人强调 AI 对有身体障碍者作为辅助工具的重要性。总体上社区在责任分配上存在分歧，是工具治理不足还是过度依赖导致风险仍有争议。</p><p><a href=\"https://news.ycombinator.com/item?id=46429122\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429171\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46429357\" target=\"_blank\">[来源3]</a> <a href=\"https://news.ycombinator.com/item?id=46429159\" target=\"_blank\">[来源4]</a> <a href=\"https://news.ycombinator.com/item?id=46429377\" target=\"_blank\">[来源5]</a></p><h3>替代思路与元讨论：用 AI 检测危险命令与对 AI 撰写帖的怀疑</h3><p>有用户讽刺性地建议用另一个 AI 去判断代理想执行的命令是否危险，指出这类“用 AI 管理 AI”的思路在工程界越来越流行，但可能只是把问题转移而非解决根本隔离。另有评论对 Show HN 上接连出现功能雷同的 MCP 工具以及疑似由 AI 撰写的文章表示怀疑，认为前页重复工具值得警惕。还有人具体指出 Claude 有时会迅速建议像 git filter-branch 这类高风险命令，而不是采用更安全的分步替代方案，显示出审查或更严格交互批准的必要性。总体上社区既提出利用 AI 做风险判定的想法，也对这种循环方案和 AI 撰稿产生的噪声持怀疑态度。</p><p><a href=\"https://news.ycombinator.com/item?id=46429598\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429464\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46429214\" target=\"_blank\">[来源3]</a></p><hr><h2>📚 术语解释</h2><p><strong>hooks（钩子）:</strong> 在 Claude Code 等 agent 环境中于编辑、工具调用、提交等节点触发的自定义检测或阻断逻辑，用于执行 gitleaks、跑单元测试或返回可操作的修复提示给代理。</p><p><strong>plan mode（计划模式）:</strong> agent 先生成执行计划并等待人工批准的模式，避免模型在拥有完全权限的环境下直接运行修改，有助于人工审查和阻止危险操作。</p><p><strong>checkpointed container / 沙箱（检查点容器）:</strong> 给 agent 提供隔离的容器或虚拟机环境，仅挂载受限目录并定期做快照/提交，使代理在可回滚的沙箱中试验而不会影响主仓库与历史记录。</p><hr><p><strong>类别：</strong>AI | Security | Programming | Show HN | Release | Claude Code | claude-code-safety-net | Git | filesystem | hooks | containers | Open Policy Agent (OPA) | plan mode</p>"}},{"id":"228713045038442496","type":"news","url":"https://www.qbitai.com/2025/12/366249.html","title":"对科技圈，小红书是个「新绿洲」","description":"对科技圈，小红书是个「新绿洲」 [图片: http://www.qbitai.com/wp-content/themes/liangziwei/imagesnew/head.jpg] 梦瑶 2025-12-30 12:49:17 来源： 量子位 作者｜张鹏 编辑｜连冉 我最近意识到，自己刷小红书的时间越来越多了，而且，原因很奇特： 我竟然是去刷科技动态和找创新产品的！ 没错，虽然我有很多内容渠道和自己写的Agent可以做这些事，包括极客公园整个编辑部、社区团队、投资团队都在帮我做这件事，甚至连我的抖音都被自己「调教」为科技频道了，但是我统计了下我的小红书使用时长，今年上升最明显。 我仔细想了想，可能是因为小红书上有种比较独特的「人间视角」，看着许多真实的人在科技话题上「自然涌现」的讨论，和彼此间的「吵吵闹闹」，似乎能更好的支持自己一些更具体的「直觉感知」。 我甚至觉得，对于科技圈来说，过去大家觉得主要是为了「吃喝玩乐」的小红书，如今正在有一种「新绿洲」的即视感。 这个感觉不好一句说清，不妨写一篇出来看看大家是不是也有所共鸣吧。 从科技内容的「夜市烟火气」开始 不知道大家是否和我一样，在过去几年的AI浪潮里，因为要疯狂学习和跟上时代，阅读了大量科技内容，但逐渐也时不时会有一个厌倦感，有时候会感觉我们在谈论科技、商业资讯的时候，是不是在消费一种「工业快餐」？ 因为AI热潮，也因为运用上AI能力，科技内容的生产流程已经有点高度标准化了——把类似论文成果和跑分成果、还有ARR和融资额、以及新出现行业术语这些内容打包，高效地生产后把信息填进读者的脑子里。 这种内容确实能「管饱」，解决你获取资讯的需求，但说实话，也带着一种因为信息过载而产生的焦虑感——根本读不完，但好像你不读你就要被时代抛弃了。 但我发现在小红书这样的社区里，科技内容的消费模式正在发生一些改变，如果说传统资讯是标准化的「快餐店」，那么现在的社区生态更像是一个喧闹、鲜活的「人间夜市」。 在这个「夜市」里，流量的逻辑变了，内容的逻辑也变了。这里的博主和开发者，正在做一件很有价值的事情——圈点和翻译。 或者更准确的说是：去中心化的编辑+跨世界观的翻译，也就是他们的选择和筛选，加上他们的跨世界观表达，去「降维」了那些晦涩的技术概念，甚至把技术变成立即马上的直观体验。 举个今年我印象比较深刻的例子。今年AI圈的核心一直在讲大模型的Context（上下文）工程和Agent能力，显然是个非常重要的概念。 但我最近对这个概念印象深刻的圈点，反而是在小红书上看到的。有博主把其他平台上一位技术人对Agent相关的上下文问题和工作流问题的解读，在小红书上发了出来，这里面确实对技术概念做了很好的抽象，但更精彩的还在评论区——有人进一步将这些逻辑抽象，提炼为「来龙去脉」问题： 「来龙（context，背景信息、数据），去脉（workflow，任务指令/目标/路径），要思考两者在不同清晰度情况下该如何设计业务」。你看，这种来自真实用户的「神总结」，带来的启发是非常独特的，这个概念感觉一下子就鲜活通透了，有时候即便刚看完万字长文，也一样需要这种提纲挈领的。 再比如，大模型的生成能力，我们熟悉的是用来做分析，写文档。但在小红书上，它演变成了「小猫文学」——用户用AI模仿猫的口吻写日记、搞怪。 甚至有人把AI调教成了「完美男友」或「人生导师」，这里面的技术或许谈不上多高深，但只要看看底下的评论，你对这个世界的「分辨率」依然会因此提升：你会看到用户眼里的AI是什么样，以及他们究竟需要什么样的 AI。 科技的解释权和方向感，同样需要来自那些能把「技术」翻译成「生活体感」的人。 另一个很直观的感受来自于已经举办16年的极客公园创新大会（GeekPark IF）。以前大会结束后我们最关心的是后续传播的阅读量、全网的分发报道数据。这很好理解，极客公园团队把自己一年对科技圈的判断理解做成一个「贺岁片」似的大会内容，当然特别希望看到这些努力的意义。 但从2年前我们开始邀请小红书一起参与IF大会，我每年都更加能从小红书上感受到，认真做一场科技大会是值得的。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/53d343a5e378c339d86db8e1ba2ef12e.jpeg] 比如今年我自己的小红书信息流里，就刷到了上百篇参会者发认真分享的IF大会的笔记。他们自己的视角和观点，他们的眼睛里看到的大会里那些具体的收获，是「原生」的、没有修饰的分享，它是如此的「真实而具体」，它能让你看到极客公园团队努力创造的东西，也真的有「真实而具体」的意义。 还有一个比较让我印象深刻的是小红书上强烈的「活人感」。就像你家旁边的夜市大排档还蕴含着「和有趣老板聊几句的乐趣」。你光顾一个摊位，往往不只是因为东西好吃，可能也是因为这个摊主是个「熟人」。 今年小红书发起的AMA (Ask Me Anything) 活动就挺典型的，不少科技圈的大咖和创业者都参与了这个直接回答用户提问的活动。体验了科技圈内「坐而论道」之外的沟通方式。 在前段时间在极客公园创新大会2026的一场沙龙上，散兵（小红书科技运营负责人）提到一个细节，最初这些创业者和学者是很「i」的，过去不知道怎么社交，习惯的是用产品和思想征服世界，但AMA这种形式让他们发现，原来大众喜欢的是平等的对话，而对话本身就有意义。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/225081d24b85e91a79a91d7a7c9a2585.jpeg] 在这个过程中，信息的密度可能不如一篇万字研报，但连接的强度和愉悦感是前所未有的。对此我深有感触，在参与AMA时，我发现大量提问都聚焦于创业起步期的核心认知——如何验证PMF、如何实现增长、如何理解早期融资…… 当你针对这些具体的困惑给出解答，你能清晰地感知到这对屏幕那头的人是真实有用的。这种传递认知与经验的过程，让意义变得无比「具体」。 以前写篇文章觉得自己能「帮所有人」，但其实你也不知道他是谁，他在哪里？但当你感受到真的帮到一个具体的人，这种「具体的意义」，就很开心。 这个「具体」还是来自于小红书上活跃着大量充满好奇、在生活中使用科技的普通人。 「活人感」也是小红书的底色之一——毕竟是 一个由真实需求交织而成的UGC生态。 当真实的人和具体的需求在这里高度聚集时，它改变的就不止是内容的消费模式，也深刻地重构了产品创造的逻辑。 当创业者在「街区」Build in Public 既然我们厘清了这里的底层「土壤」——海量的、带着具体需求的「人」，那么对于创业者，尤其是AI时代的超级个体来说，选择在哪里「出生」，逻辑也就开始变得不同了。 我有一个观察：传统的流量平台像是一个巨大的「公共广场」，而小红书像是一个充满左邻右舍的「街区」。 为什么会有这种差别？ 有一个Panel里提到的细节，让我印象很深。在2024年初AI浪潮刚来袭时，面对技术圈一天一个样的迭代，小红书做了一个反直觉的判断： 追逐最新的资讯意义不大，因为技术迭代太快了；「科技里的人」，才是最关键的。 所以，他们当时没有盯着技术参数，而是去放大了那些「普通人与AI的真实交互」——比如我们前面提到的AI恋爱、DeepSeek玩法等等。 所以有了这些普通人脑洞大开的参与，有了前面说的那个喧闹的「夜市」。 而这个「夜市」里涌动的、最真实具体的「人味儿」和需求，也恰恰反哺了最核心的科技圈层，帮助「拆掉了」那堵技术走向大众的墙。 这就解释了为什么小红书会成为一个「科技街区」： 在广场上，你需要声量，需要拿着大喇叭喊，周围充斥着辩论、站队和噪音。但在街区里，你是邻居。邻居之间不谈宏大叙事，谈的是「这东西好不好用」、「能不能帮我解决这个问题」。这种场域属性的差异，直接决定了创业逻辑的根本性扭转。 过去做产品，创业者习惯「憋大招」。关起门来开发1年，然后开一场发布会，希望征服市场能买单。但在AI时代，有一种新的创业显学，叫Build in Public（在公众面前构建）。 最近我在小红书上看到一个非常有意思的现象：营销正在变得极度前置。散兵在panel上说，现在很多开发者，产品还没写一行代码，甚至只是有一个Idea，就敢发笔记出来「口嗨」。 这在以前叫「画饼」，但在今天的「街区」逻辑里，这叫低成本验证（MVP）。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/c6494d3bbd78af1e6ed5c728c687d112.jpeg] △Plan Coach的开发者苏晓江｜图片来源：极客公园 Plan Coach的开发者苏晓江（机器猫），是一个独立开发者，而在做Plan Coach之前，他本来是一个技术主管，想做个产品治治自己的拖延症。然后他没有选择闷头写代码，而是先把「站起来去洗碗」这个痛点拆解成笔记在小红书上发了出来。结果一天之内，3000多个陌生人给他点赞。 这3000个赞是什么？是需求，是信心，更是第一批种子用户。它说明只要你的产品足够Amazing，能解决「邻居们」的具体问题，它就会被看到。 在「街区」里创业，还有很合理的一点在于反馈的及时性和颗粒度。 陈锴杰（Macaron AI创始人）在同一场panel上分享过一个细节。他们在设计产品时，纠结了三天三夜要不要用粉红色。结果发到小红书上一看，用户不仅不反感，反而因为这个配色觉得「真香」。 更夸张的是，前面提到的苏晓江收到了1000多条用户反馈，这些用户是真的把自己当成了「精神股东」。他们会催更、会投票决定下一个功能做什么，甚至会因为开发者采纳了自己的建议而兴奋不已。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/89402e2b0c4740c53e587847776014c7.jpeg] △Plan coach在小红书上请用户投票新功能 这种关系不再是冷冰冰的「开发者-用户」，而是有点变成了一种「养成系」的共生关系。用户看着产品从0到1长大，他们对产品的包容度和忠诚度，是那些靠买量砸出来的用户完全无法比拟的。 为什么这些看似微小的、垂直的「超级个体」能在小红书上活下来，甚至活得很好？因为AI能力实际上提升了商业世界的「分辨率」。 在传统互联网时代，为了覆盖开发成本，我们必须做几千万日活的「大产品」，必须去满足「最大公约数」的需求。但在AI时代，生产要素变便宜了，开发的门槛被空前拉低了。 这就意味着，我们可以把镜头拉得更「微距」，去服务那些曾经被视为「太小」、「太碎」的需求。 在Macaron AI上，我看到了20万个千奇百怪的小应用。其中有一个应用，专门用来记录「拉屎」。听起来是不是很荒诞？但这恰恰证明了世界的丰富性。在小红书这个「街区」里，哪怕你只服务1万人的垂直需求，只要你真的做到了，做好了，你也能找到这1万个「邻居」。 这里有足够真实的用户需求，有足够强烈的反馈机制。在这个时代，「流量」只会越来越昂贵，因为流量是平台的；但「共鸣」是免费的，因为那是开发者可以用自己的世界观、审美和能力换来的。 所以对于提升商业世界分辨率的独立开发者来说，这1万个精准用户提供的商业价值和逻辑自洽，更符合创新的本质——从解决问题开始。 我想这也是为什么在小红书，能看到许多 年轻的创业者，在小红书上完成了「从0到1再到100」的蜕变。他们起初可能只是作为一个独立开发者，做出了一些初创产品，然后在社区里验证了想法，拿到了融资——像Flowith这样在小红书一路成长起来的案例，正在变得越来越多。 创业需要依托创新，创新需要从创造开始，而创造的起点，是解决你足够理解的，「具体而有意义」的问题。 过去十年，移动互联网教会了我们太多的「商业公式」。创业需要回归「创造」的本质。在小红书上，我看到很多开发者不再是为了「我们要切入一个百亿赛道」而出发，而是因为「我想解决一个让自己抓狂的小问题」或者「我想表达一种审美」。 作为一个创造的起点，这可能就足够了。很多了不起的人和事情，往往就是这样开始的。 版权所有，未经授权不得以任何形式转载及使用，违者必究。","published_date":"2025-12-30T04:49:17.422Z","authors":"量子位","source":"量子位 - 资讯 - 量子位","details":{"content_html":"<h1>对科技圈，小红书是个「新绿洲」</h1>\n       <div>\n             <span><img src=\"http://www.qbitai.com/wp-content/themes/liangziwei/imagesnew/head.jpg\" height=\"200\" width=\"200\"><em><a href=\"https://www.qbitai.com/author/mengyao\" title=\"由 梦瑶 发布\" target=\"_blank\">梦瑶</a></em></span>\n                          <span>2025-12-30</span>\n             <span>12:49:17</span>\n          <span>\n          来源：<a href=\"https://www.qbitai.com/\" target=\"_blank\">量子位</a>            </span></div>\n                          \n                            <p><strong>作者｜张鹏</strong></p>\n<p><strong>编辑｜连冉</strong></p>\n<p>我最近意识到，自己刷小红书的时间越来越多了，而且，原因很奇特：</p>\n<p>我竟然是去刷科技动态和找创新产品的！</p>\n<p>没错，虽然我有很多内容渠道和自己写的Agent可以做这些事，包括极客公园整个编辑部、社区团队、投资团队都在帮我做这件事，甚至连我的抖音都被自己「调教」为科技频道了，但是我统计了下我的小红书使用时长，今年上升最明显。</p>\n<p>我仔细想了想，可能是因为小红书上有种比较独特的「人间视角」，看着许多真实的人在科技话题上「自然涌现」的讨论，和彼此间的「吵吵闹闹」，似乎能更好的支持自己一些更具体的「直觉感知」。</p>\n<p>我甚至觉得，对于科技圈来说，过去大家觉得主要是为了「吃喝玩乐」的小红书，如今正在有一种「新绿洲」的即视感。</p>\n<p>这个感觉不好一句说清，不妨写一篇出来看看大家是不是也有所共鸣吧。</p>\n<h1>从科技内容的「夜市烟火气」开始</h1>\n<p>不知道大家是否和我一样，在过去几年的AI浪潮里，因为要疯狂学习和跟上时代，阅读了大量科技内容，但逐渐也时不时会有一个厌倦感，有时候会感觉我们在谈论科技、商业资讯的时候，是不是在消费一种「工业快餐」？</p>\n<p>因为AI热潮，也因为运用上AI能力，科技内容的生产流程已经有点高度标准化了——把类似论文成果和跑分成果、还有ARR和融资额、以及新出现行业术语这些内容打包，高效地生产后把信息填进读者的脑子里。</p>\n<p>这种内容确实能「管饱」，解决你获取资讯的需求，但说实话，也带着一种因为信息过载而产生的焦虑感——根本读不完，但好像你不读你就要被时代抛弃了。</p>\n<p>但我发现在小红书这样的社区里，科技内容的消费模式正在发生一些改变，如果说传统资讯是标准化的「快餐店」，那么现在的社区生态更像是一个喧闹、鲜活的「人间夜市」。</p>\n<p>在这个「夜市」里，流量的逻辑变了，内容的逻辑也变了。这里的博主和开发者，正在做一件很有价值的事情——圈点和翻译。</p>\n<p>或者更准确的说是：去中心化的编辑+跨世界观的翻译，也就是他们的选择和筛选，加上他们的跨世界观表达，去「降维」了那些晦涩的技术概念，甚至把技术变成立即马上的直观体验。</p>\n<p>举个今年我印象比较深刻的例子。今年AI圈的核心一直在讲大模型的Context（上下文）工程和Agent能力，显然是个非常重要的概念。</p>\n<p>但我最近对这个概念印象深刻的圈点，反而是在小红书上看到的。有博主把其他平台上一位技术人对Agent相关的上下文问题和工作流问题的解读，在小红书上发了出来，这里面确实对技术概念做了很好的抽象，但更精彩的还在评论区——有人进一步将这些逻辑抽象，提炼为「来龙去脉」问题：</p>\n<p>「来龙（context，背景信息、数据），去脉（workflow，任务指令/目标/路径），要思考两者在不同清晰度情况下该如何设计业务」。你看，这种来自真实用户的「神总结」，带来的启发是非常独特的，这个概念感觉一下子就鲜活通透了，有时候即便刚看完万字长文，也一样需要这种提纲挈领的。</p>\n<p>再比如，大模型的生成能力，我们熟悉的是用来做分析，写文档。但在小红书上，它演变成了「小猫文学」——用户用AI模仿猫的口吻写日记、搞怪。</p>\n<p>甚至有人把AI调教成了「完美男友」或「人生导师」，这里面的技术或许谈不上多高深，但只要看看底下的评论，你对这个世界的「分辨率」依然会因此提升：你会看到用户眼里的AI是什么样，以及他们究竟需要什么样的 AI。<strong>科技的解释权和方向感，同样需要来自那些能把「技术」翻译成「生活体感」的人。</strong></p>\n<p>另一个很直观的感受来自于已经举办16年的极客公园创新大会（GeekPark IF）。以前大会结束后我们最关心的是后续传播的阅读量、全网的分发报道数据。这很好理解，极客公园团队把自己一年对科技圈的判断理解做成一个「贺岁片」似的大会内容，当然特别希望看到这些努力的意义。</p>\n<p>但从2年前我们开始邀请小红书一起参与IF大会，我每年都更加能从小红书上感受到，认真做一场科技大会是值得的。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/53d343a5e378c339d86db8e1ba2ef12e.jpeg\"><p></p>\n<p>\n</p></div>\n<p>比如今年我自己的小红书信息流里，就刷到了上百篇参会者发认真分享的IF大会的笔记。他们自己的视角和观点，他们的眼睛里看到的大会里那些具体的收获，是「原生」的、没有修饰的分享，它是如此的「真实而具体」，它能让你看到极客公园团队努力创造的东西，也真的有「真实而具体」的意义。</p>\n<p>还有一个比较让我印象深刻的是小红书上强烈的「活人感」。就像你家旁边的夜市大排档还蕴含着「和有趣老板聊几句的乐趣」。你光顾一个摊位，往往不只是因为东西好吃，可能也是因为这个摊主是个「熟人」。</p>\n<p>今年小红书发起的AMA (Ask Me Anything) 活动就挺典型的，不少科技圈的大咖和创业者都参与了这个直接回答用户提问的活动。体验了科技圈内「坐而论道」之外的沟通方式。</p>\n<p>在前段时间在极客公园创新大会2026的一场沙龙上，散兵（小红书科技运营负责人）提到一个细节，最初这些创业者和学者是很「i」的，过去不知道怎么社交，习惯的是用产品和思想征服世界，但AMA这种形式让他们发现，原来大众喜欢的是平等的对话，而对话本身就有意义。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/225081d24b85e91a79a91d7a7c9a2585.jpeg\"><p></p>\n<p>\n</p></div>\n<p>在这个过程中，信息的密度可能不如一篇万字研报，但连接的强度和愉悦感是前所未有的。对此我深有感触，在参与AMA时，我发现大量提问都聚焦于创业起步期的核心认知——如何验证PMF、如何实现增长、如何理解早期融资……</p>\n<p>当你针对这些具体的困惑给出解答，你能清晰地感知到这对屏幕那头的人是真实有用的。这种传递认知与经验的过程，让意义变得无比「具体」。</p>\n<p>以前写篇文章觉得自己能「帮所有人」，但其实你也不知道他是谁，他在哪里？但当你感受到真的帮到一个具体的人，这种「具体的意义」，就很开心。</p>\n<p><strong>这个「具体」还是来自于小红书上活跃着大量充满好奇、在生活中使用科技的普通人。</strong>「活人感」也是小红书的底色之一——毕竟是<strong>一个由真实需求交织而成的UGC生态。</strong></p>\n<p>当真实的人和具体的需求在这里高度聚集时，它改变的就不止是内容的消费模式，也深刻地重构了产品创造的逻辑。</p>\n<h1>当创业者在「街区」Build in Public</h1>\n<p>既然我们厘清了这里的底层「土壤」——海量的、带着具体需求的「人」，那么对于创业者，尤其是AI时代的超级个体来说，选择在哪里「出生」，逻辑也就开始变得不同了。</p>\n<p>我有一个观察：传统的流量平台像是一个巨大的「公共广场」，而小红书像是一个充满左邻右舍的「街区」。</p>\n<p>为什么会有这种差别？</p>\n<p>有一个Panel里提到的细节，让我印象很深。在2024年初AI浪潮刚来袭时，面对技术圈一天一个样的迭代，小红书做了一个反直觉的判断：<strong>追逐最新的资讯意义不大，因为技术迭代太快了；「科技里的人」，才是最关键的。</strong></p>\n<p>所以，他们当时没有盯着技术参数，而是去放大了那些「普通人与AI的真实交互」——比如我们前面提到的AI恋爱、DeepSeek玩法等等。</p>\n<p>所以有了这些普通人脑洞大开的参与，有了前面说的那个喧闹的「夜市」。</p>\n<p>而这个「夜市」里涌动的、最真实具体的「人味儿」和需求，也恰恰反哺了最核心的科技圈层，帮助「拆掉了」那堵技术走向大众的墙。</p>\n<p>这就解释了为什么小红书会成为一个「科技街区」：</p>\n<p>在广场上，你需要声量，需要拿着大喇叭喊，周围充斥着辩论、站队和噪音。但在街区里，你是邻居。邻居之间不谈宏大叙事，谈的是「这东西好不好用」、「能不能帮我解决这个问题」。这种场域属性的差异，直接决定了创业逻辑的根本性扭转。</p>\n<p>过去做产品，创业者习惯「憋大招」。关起门来开发1年，然后开一场发布会，希望征服市场能买单。但在AI时代，有一种新的创业显学，叫Build in Public（在公众面前构建）。</p>\n<p>最近我在小红书上看到一个非常有意思的现象：营销正在变得极度前置。散兵在panel上说，现在很多开发者，产品还没写一行代码，甚至只是有一个Idea，就敢发笔记出来「口嗨」。</p>\n<p>这在以前叫「画饼」，但在今天的「街区」逻辑里，这叫低成本验证（MVP）。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/c6494d3bbd78af1e6ed5c728c687d112.jpeg\"><p></p>\n<p>\n</p></div>\n<h1>△Plan Coach的开发者苏晓江｜图片来源：极客公园</h1>\n<p>Plan Coach的开发者苏晓江（机器猫），是一个独立开发者，而在做Plan Coach之前，他本来是一个技术主管，想做个产品治治自己的拖延症。然后他没有选择闷头写代码，而是先把「站起来去洗碗」这个痛点拆解成笔记在小红书上发了出来。结果一天之内，3000多个陌生人给他点赞。</p>\n<p>这3000个赞是什么？是需求，是信心，更是第一批种子用户。它说明只要你的产品足够Amazing，能解决「邻居们」的具体问题，它就会被看到。</p>\n<p>在「街区」里创业，还有很合理的一点在于反馈的及时性和颗粒度。</p>\n<p>陈锴杰（Macaron AI创始人）在同一场panel上分享过一个细节。他们在设计产品时，纠结了三天三夜要不要用粉红色。结果发到小红书上一看，用户不仅不反感，反而因为这个配色觉得「真香」。</p>\n<p>更夸张的是，前面提到的苏晓江收到了1000多条用户反馈，这些用户是真的把自己当成了「精神股东」。他们会催更、会投票决定下一个功能做什么，甚至会因为开发者采纳了自己的建议而兴奋不已。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/89402e2b0c4740c53e587847776014c7.jpeg\"><p></p>\n<p>\n</p></div>\n<h1>△Plan coach在小红书上请用户投票新功能</h1>\n<p>这种关系不再是冷冰冰的「开发者-用户」，而是有点变成了一种「养成系」的共生关系。用户看着产品从0到1长大，他们对产品的包容度和忠诚度，是那些靠买量砸出来的用户完全无法比拟的。</p>\n<p>为什么这些看似微小的、垂直的「超级个体」能在小红书上活下来，甚至活得很好？因为AI能力实际上提升了商业世界的「分辨率」。</p>\n<p>在传统互联网时代，为了覆盖开发成本，我们必须做几千万日活的「大产品」，必须去满足「最大公约数」的需求。但在AI时代，生产要素变便宜了，开发的门槛被空前拉低了。</p>\n<p>这就意味着，我们可以把镜头拉得更「微距」，去服务那些曾经被视为「太小」、「太碎」的需求。</p>\n<p>在Macaron AI上，我看到了20万个千奇百怪的小应用。其中有一个应用，专门用来记录「拉屎」。听起来是不是很荒诞？但这恰恰证明了世界的丰富性。在小红书这个「街区」里，哪怕你只服务1万人的垂直需求，只要你真的做到了，做好了，你也能找到这1万个「邻居」。</p>\n<p>这里有足够真实的用户需求，有足够强烈的反馈机制。在这个时代，「流量」只会越来越昂贵，因为流量是平台的；但「共鸣」是免费的，因为那是开发者可以用自己的世界观、审美和能力换来的。</p>\n<p>所以对于提升商业世界分辨率的独立开发者来说，这1万个精准用户提供的商业价值和逻辑自洽，更符合创新的本质——从解决问题开始。</p>\n<p>我想这也是为什么在小红书，能看到许多 年轻的创业者，在小红书上完成了「从0到1再到100」的蜕变。他们起初可能只是作为一个独立开发者，做出了一些初创产品，然后在社区里验证了想法，拿到了融资——像Flowith这样在小红书一路成长起来的案例，正在变得越来越多。</p>\n<p>创业需要依托创新，创新需要从创造开始，而创造的起点，是解决你足够理解的，「具体而有意义」的问题。</p>\n<p>过去十年，移动互联网教会了我们太多的「商业公式」。创业需要回归「创造」的本质。在小红书上，我看到很多开发者不再是为了「我们要切入一个百亿赛道」而出发，而是因为「我想解决一个让自己抓狂的小问题」或者「我想表达一种审美」。</p>\n<p>作为一个创造的起点，这可能就足够了。很多了不起的人和事情，往往就是这样开始的。</p>\n                \n                \n                <div><span></span><em>版权所有，未经授权不得以任何形式转载及使用，违者必究。</em><span></span></div>\n            "}},{"id":"228692513282738178","type":"news","url":"https://newshacker.me/story?id=46428154","title":"⚔️ 城镇 / 野外 / 地牢：把三处整合为动态非线性世界的诉求","description":"原标题： 《Outside, Dungeon, Town: Integrating the Three Places in Videogames (2024)》 评分: 24 | 作者: vector_spaces 💭 要完全非线性自由，你确定不想回城回血吗？ 🎯 讨论背景 讨论源自 2024 年关于如何把 Outside（野外）、Dungeon（地牢）和 Town（城镇）三类空间在游戏中整合的文章，评论聚焦于让城镇/野外/地牢互相影响以产生持续运行的动态世界。评论以多款游戏对比实践与愿景：Ultima Online（早期大型多人在线 RPG，强调玩家驱动事件与公共惩罚）、Skyrim（Bethesda 的开放世界 RPG）、Kingdom Come Deliverance（写实中世纪 RPG，尝试时间点事件）等被用作参照。也讨论了 Dark Souls（高难度动作 RPG）、Tears of the Kingdom（塞尔达衍作的地下互联示例）、Minecraft 与 Terraria（沙盒游戏中通过生成/刷新机制产生 emergent gameplay）以及 Escape from Tarkov（硬核在线射击，曾设想把世界做成地牢式体验）的成功与局限。共同前提是玩家既想要更多自主与 emergent 事件，也需要清晰的危险提示与可控手段，讨论集中在设计取舍与 generative AI 等技术能否成为实现这些目标的助力。 📌 讨论焦点 玩家对动态、非线性事件与自治世界的期待 不少评论者希望世界在玩家不在时也会自发发生戏剧性事件——邻村被攻陷焚毁、城堡遭围攻或城镇内发生犯罪并被惩罚，这种“play your own adventure”取代线性主线的诉求反复出现。主张者想保留关键剧情节点但把大部分世界做成模拟，让 NPC 有自治行为，玩家行为与 NPC/玩家的惩罚能产生独一无二的叙事片段。有人以 Ultima Online 的公开处刑为怀旧例子，并指出已有小众作品或实验（如 Depth of Peril）体现了这种动态性。总体诉求是玩家通过自由行动去“构建”故事，而非被分区指向固定的 XP / 副本点。 [来源1] [来源2] [来源3] [来源4] 安全感与可读性的设计权衡 反对者或谨慎派强调城镇、野外、地牢通常代表安全性递减，玩家通常需要明确的危险指示来决定行动，这使得把所有空间模糊化存在实用风险。有人指出像 Dark Souls 能成功模糊边界并增强有机感，但那类实现难度高且需精心打磨。另有评论用 Minecraft 与 Terraria 的怪物刷新与玩家可控手段说明，“可控的危险”能催生 emergent gameplay，而非单纯混乱。结论是需要在沉浸与可读性之间取舍，避免把玩家抛入难以理解的世界。 [来源1] [来源2] [来源3] [来源4] 现有游戏的尝试与局限：成功的片段与常见失败 评论用多款现有游戏检验这一设想，指出部分作品有可借鉴的片段但整体常显不足。有人提到 Kingdom Come Deliverance 2 的时间点事件做法但也有回应认为整体仍然线性；Elder Scrolls 与 Fallout 系列会出现城镇伏击或定居点被攻击的零星例子，但像 Fallout 4 的定居点系统被批为频率与意义不足，无法形成持续互动。Escape from Tarkov 的早期愿景是把整个世界都当作外部/地牢式体验，但因技术与开发能力限制没能实现真正开放世界。Depth of Peril 被列为较早实现动态派系与事件的范例，但其年代与玩法显示理念到成熟实现之间存在差距。 [来源1] [来源2] [来源3] [来源4] [来源5] [来源6] 技术与未来：生成式 AI 与实现难题 部分评论认为 generative AI 将在 NPC 行为和时序事件生成上带来巨大助力，使得村庄政治、公开处刑或派系争斗等社会动态规模化成为可能。同时也有人警告这需要稳定性與可控性，不能盲目放任生成内容；把文本层面的实验转为引擎内可信的实时模拟仍有大量工程难题。Escape from Tarkov 被引用为技术实施受限的反面教训，说明团队能力与架构也会成为瓶颈。总体观点是技术方向可行但实现质量、信任与设计权衡决定成败。 [来源1] [来源2] 📚 术语解释 Emergent gameplay（涌现式游戏玩法）: 由多个简单规则或系统相互作用产生未预设的复杂行为与叙事，玩家与世界的互动会触发意外结果，评论中以 Minecraft/ Terraria 的刷新与控制机制作为例子。 Zoning（区域分区/门控）: 设计上通过把世界划分成不同区域并用关卡或地形限制进入来控制进度与经验获取，常被批评为把所谓“open world”线性化。 NPC（non-player character，非玩家角色）: 由游戏控制的角色，可执行日常行为、执法或触发剧情；是否赋予 NPC 自主行动和惩罚能力直接影响世界的动态感与玩家互动。 类别： Product | AI | Opinion | videogames | Keith Burgun | Skyrim | Minecraft | Dark Souls | Fallout 4 | Ultima Online","published_date":"2025-12-30T04:26:55.083Z","authors":"","source":"News Hacker | 极客洞察","details":{"content_html":"<p><strong>原标题：</strong>《Outside, Dungeon, Town: Integrating the Three Places in Videogames (2024)》</p><p><strong>评分:</strong> 24 | <strong>作者:</strong> vector_spaces</p><blockquote>💭 要完全非线性自由，你确定不想回城回血吗？</blockquote><hr><h2>🎯 讨论背景</h2><p>讨论源自 2024 年关于如何把 Outside（野外）、Dungeon（地牢）和 Town（城镇）三类空间在游戏中整合的文章，评论聚焦于让城镇/野外/地牢互相影响以产生持续运行的动态世界。评论以多款游戏对比实践与愿景：Ultima Online（早期大型多人在线 RPG，强调玩家驱动事件与公共惩罚）、Skyrim（Bethesda 的开放世界 RPG）、Kingdom Come Deliverance（写实中世纪 RPG，尝试时间点事件）等被用作参照。也讨论了 Dark Souls（高难度动作 RPG）、Tears of the Kingdom（塞尔达衍作的地下互联示例）、Minecraft 与 Terraria（沙盒游戏中通过生成/刷新机制产生 emergent gameplay）以及 Escape from Tarkov（硬核在线射击，曾设想把世界做成地牢式体验）的成功与局限。共同前提是玩家既想要更多自主与 emergent 事件，也需要清晰的危险提示与可控手段，讨论集中在设计取舍与 generative AI 等技术能否成为实现这些目标的助力。</p><hr><h2>📌 讨论焦点</h2><h3>玩家对动态、非线性事件与自治世界的期待</h3><p>不少评论者希望世界在玩家不在时也会自发发生戏剧性事件——邻村被攻陷焚毁、城堡遭围攻或城镇内发生犯罪并被惩罚，这种“play your own adventure”取代线性主线的诉求反复出现。主张者想保留关键剧情节点但把大部分世界做成模拟，让 NPC 有自治行为，玩家行为与 NPC/玩家的惩罚能产生独一无二的叙事片段。有人以 Ultima Online 的公开处刑为怀旧例子，并指出已有小众作品或实验（如 Depth of Peril）体现了这种动态性。总体诉求是玩家通过自由行动去“构建”故事，而非被分区指向固定的 XP / 副本点。</p><p><a href=\"https://news.ycombinator.com/item?id=46428899\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429409\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46429116\" target=\"_blank\">[来源3]</a> <a href=\"https://news.ycombinator.com/item?id=46429304\" target=\"_blank\">[来源4]</a></p><h3>安全感与可读性的设计权衡</h3><p>反对者或谨慎派强调城镇、野外、地牢通常代表安全性递减，玩家通常需要明确的危险指示来决定行动，这使得把所有空间模糊化存在实用风险。有人指出像 Dark Souls 能成功模糊边界并增强有机感，但那类实现难度高且需精心打磨。另有评论用 Minecraft 与 Terraria 的怪物刷新与玩家可控手段说明，“可控的危险”能催生 emergent gameplay，而非单纯混乱。结论是需要在沉浸与可读性之间取舍，避免把玩家抛入难以理解的世界。</p><p><a href=\"https://news.ycombinator.com/item?id=46428528\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46428549\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46428763\" target=\"_blank\">[来源3]</a> <a href=\"https://news.ycombinator.com/item?id=46429001\" target=\"_blank\">[来源4]</a></p><h3>现有游戏的尝试与局限：成功的片段与常见失败</h3><p>评论用多款现有游戏检验这一设想，指出部分作品有可借鉴的片段但整体常显不足。有人提到 Kingdom Come Deliverance 2 的时间点事件做法但也有回应认为整体仍然线性；Elder Scrolls 与 Fallout 系列会出现城镇伏击或定居点被攻击的零星例子，但像 Fallout 4 的定居点系统被批为频率与意义不足，无法形成持续互动。Escape from Tarkov 的早期愿景是把整个世界都当作外部/地牢式体验，但因技术与开发能力限制没能实现真正开放世界。Depth of Peril 被列为较早实现动态派系与事件的范例，但其年代与玩法显示理念到成熟实现之间存在差距。</p><p><a href=\"https://news.ycombinator.com/item?id=46429304\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429425\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46429371\" target=\"_blank\">[来源3]</a> <a href=\"https://news.ycombinator.com/item?id=46429422\" target=\"_blank\">[来源4]</a> <a href=\"https://news.ycombinator.com/item?id=46428661\" target=\"_blank\">[来源5]</a> <a href=\"https://news.ycombinator.com/item?id=46429116\" target=\"_blank\">[来源6]</a></p><h3>技术与未来：生成式 AI 与实现难题</h3><p>部分评论认为 generative AI 将在 NPC 行为和时序事件生成上带来巨大助力，使得村庄政治、公开处刑或派系争斗等社会动态规模化成为可能。同时也有人警告这需要稳定性與可控性，不能盲目放任生成内容；把文本层面的实验转为引擎内可信的实时模拟仍有大量工程难题。Escape from Tarkov 被引用为技术实施受限的反面教训，说明团队能力与架构也会成为瓶颈。总体观点是技术方向可行但实现质量、信任与设计权衡决定成败。</p><p><a href=\"https://news.ycombinator.com/item?id=46429409\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46428661\" target=\"_blank\">[来源2]</a></p><hr><h2>📚 术语解释</h2><p><strong>Emergent gameplay（涌现式游戏玩法）:</strong> 由多个简单规则或系统相互作用产生未预设的复杂行为与叙事，玩家与世界的互动会触发意外结果，评论中以 Minecraft/ Terraria 的刷新与控制机制作为例子。</p><p><strong>Zoning（区域分区/门控）:</strong> 设计上通过把世界划分成不同区域并用关卡或地形限制进入来控制进度与经验获取，常被批评为把所谓“open world”线性化。</p><p><strong>NPC（non-player character，非玩家角色）:</strong> 由游戏控制的角色，可执行日常行为、执法或触发剧情；是否赋予 NPC 自主行动和惩罚能力直接影响世界的动态感与玩家互动。</p><hr><p><strong>类别：</strong>Product | AI | Opinion | videogames | Keith Burgun | Skyrim | Minecraft | Dark Souls | Fallout 4 | Ultima Online</p>"}},{"id":"228713045038442497","type":"news","url":"https://www.qbitai.com/2025/12/366239.html","title":"千人千面的真人级AI名师，劈开教育「不可能三角」","description":"千人千面的真人级AI名师，劈开教育「不可能三角」 [图片: http://www.qbitai.com/wp-content/themes/liangziwei/imagesnew/head.jpg] Jay 2025-12-30 12:25:46 来源： 量子位 百万AI学习原住民已入驻 Jay 发自 凹非寺量子位 | 公众号 QbitAI 注意看，这是一个教育领域的AI应用新物种—— 咱就是说，这讲课节奏，这语气，这互动，也太自然了。 更重要的是，它不仅能「像老师一样讲课」，还能针对每一位学员进行一对一的个性化教学。 这位AI导师，出自一家名为「与爱为舞」的AI原生应用企业。自年初上线以来，已累计为百万级用户提供学习陪伴与一对一讲解服务。 教育行业， 向来是个「规模、质量、成本」的不可能三角 。 既能做到千人千面，又能服务百万名学员，还几乎看不出是AI……更是难上加难。 它究竟是怎么做到的？ 与爱为舞用来劈开这个不可能三角的，是一把 硬核的技术巨剑 。 AI教育，要的不止「答案」 而铸造这把技术巨剑，有三块核心组成部件：「模型+语音+工程」。 先看 模型 —— 得益于CoT的Scaling，大模型解决复杂问题的能力指数级增长，「做题」水平突飞猛进，甚至能斩获「奥赛金牌」。 摘得奥赛桂冠，AI只需要给出标准答案。但搞教育不行。 先来看一个简单的英语语法题： Lily expects _ her grandparents in the countryside next month. A. visiting｜B. to visit｜C. to have visited｜D. having visited 大模型给出的解答如下： 这道题的正确答案是B. to visit。 核心考点解析： 动词固定搭配，动词 expect（期望、预料）的常用用法是—— expect to do sth.（期望做某事），后面需要接不定式（to + 动词原形） 作宾语。 B选项to visit是不定式的一般式，用来表示与谓语动词（expect）同时或之后发生的动作，符合 “下个月看望祖父母” 的将来时态逻辑。 我可以帮你整理一份常见接不定式作宾语的动词清单，方便你记忆，需要吗？ 答案是对了，看起来好像也讲了解题过程，但如果拿这套方法教学员，那肯定是要被家长投诉的啊…… 完全没有引导学员思考，一上来就把答案透露了。就像个纯天赋型学霸，风驰电掣做完题，然后猛地丢给你一个结果，问你：「我说的对不对！」随后才给出一串看似头头是道的分析。 不过，硅基大佬，小弟我根本听不明白你在说什么啊！ 最后还要给你一份「动词清单」，直接默认教学等于枯燥的背诵，而没去想怎样引导学员的主观能动性。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/f5c46bc41b61862be3ac6bc0674a6b6b.jpeg] 归根结底，通用大模型的设计初衷就不是教育。它拼尽全力，只想向用户证明一件事——「厉害吧，哥啥都知道！」 古人讲： 授人以鱼，不如授人以渔 。导师如果光顾着自己拿金牌，这师生关系就乱了套了。 想要成为一名好导师，AI需要学会放低姿态，真正关心学员的课堂体验。 首先，AI得明白各学科的核心知识图谱、关键考点和常见解题方法，这些才是学员能服用的，是最基本的「知」。 在此之上，AI还得学习名师是怎么设计讲解顺序的，并从中总结归纳出一套顶尖教师的授课方法论。这是更高维度的「知」。 陆游讲，「纸上得来终觉浅，绝知此事要躬行。」 「知」总是相对容易的，重点是如何把纸上谈兵那套，搬到现实世界里实践起来。 所幸，「行」方面，与爱为舞有相当充足的弹药。 据悉，他们已积累了约百万小时的音视频互动数据，特别是包含大量业内TOP级名师的授课视频。 在此基础上，团队又根据学员的认知水平与学习态度，构建出多类型的「虚拟学员」，让他们与AI导师进行「搏击」，每周又能收获 数万小时的合成数据 。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/e61557505c960f0d388ec4cd9fe06488.gif] 这些数据在经过筛选与清洗后，会交由专业教研进行把关。 具体而言， 教师们会把自己多年的「教学经验」，根据场景具象化为一条条思维链 ，最终汇集成一本「好老师红宝书」： 每个知识点该如何拆解，与学员互动时如何循循善诱……不止要让AI学会怎么讲课，更要明白「为什么要这么讲」。 这种手把手教的方式效果很好，但成本也相当高。 随着方法论逐渐成熟，团队索性将这一环节也自动化，让AI模仿专业教研参与数据标注。 备考资料准备就绪，下面就该着手训练了。 第一步，照猫画虎。 那些相对容易标准化的知识，已体现在标注数据之中。AI需要做的，是通过模仿专业教师的思维链，逐步摸索出每一个教学动作背后的真实意图。 这一微调过程，能大幅降低AI「自我发挥」带来的的幻觉率，同时培养更稳定的推理能力与泛化能力。 能做到这一点，就算是打牢了基本功。 最基本的教法、节奏和经验都已被「固化」，能以标准化形式面向所有学员输出，教学质量的下限得到保障。 但如果目标只是及格，这件事就没意义了。 师傅能陪伴的路程就到这。接下来，得能靠AI自己上路修行。 第二步，终于到了大家喜闻乐见的 强化学习 环节。 在教育这个场景下，与爱为舞的奖励函数围绕教学路径规划质量、教学有效性与教学灵活性等维度设计，通过GRPO给AI做强化。 这步结束，AI彻底出师——不仅能够完成授课任务，还能驾驭课堂节奏，提高趣味性，根据不同学员灵活调整教学策略。 那么接下来，就该真正走进「教师资格证考场」了。 不过，教育不是一个有标准答案的任务，Benchmark肯定是行不通。笔试应该如何设计？ 与爱为舞的做法很简单，甚至有些「粗暴」—— 笔试啥，直接把AI丢到讲台上 ，看学员的真实反应。 第一步，是在 模拟课堂 中试水。 这个课堂由多类型的模拟学员组成，团队会按照真实分布规律注入一批线上数据，再由评分模型从多个维度对AI导师打分。 模拟课堂如果表现不错，AI会迎来更严苛的终极试炼场—— 直连真实教学一线 。 AI能否驾驭高度不确定的真实课堂？是否真的能摆脱照本宣科？答案，只能由学员来评判，再好的数据标注导师也帮不了。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/c7285f0af5918689d157a018d7cc1db3.png] 即便成功拿下了「教师资格证」，但教学，依然是个终身学习的过程。 正式上线后，海量的学员数据会被持续建模，AI导师将基于每一位学员的专属档案库，为其定制个性化课程。 至此，AI导师才算具备了千人千面的能力。不仅下限有保障，上限也很高。 「真人级」AI导师 通过「知」与「行」的双重训练，与爱为舞得以将通用大模型，塑造成一个真正懂教学的名师AI模型。 然而，再聪明的模型，无法与学员真实互动，最终仍会沦为一颗「缸中之脑」。 AI导师需要「耳朵」。 作为导师，连学员的问题都听不清楚，最后聊的牛头不对马嘴。不仅显得导师呆若木鸡，学员的积极性也会大打折扣。 但现实是，课堂不是录音棚。 真实环境往往充斥着噪音 ，如果有电视，甚至会出现多个人声掺杂在一块的情况。 即便能输入干净音频，中国有各种各样的方言，不同学员的咬字发音习惯也不同，识别难度相当高。 雪上加霜的是，在传统ASR范式下，输入模型的只是一段孤立的语音，基本没什么上下文。一旦放到教学场景下，AI很容易把同音字混淆。 例如，「极限」和「极线」。 前者是微积分中的核心概念，后者则属于二次曲线相关的几何术语。二者在语义上截然不同，发音却完全一致，如果没有上下文，仅凭语音几乎无法区分。 为解决这个问题，与爱为舞基于其长期积累的教育场景与课堂教学数据，自研了一套 多模态语音理解大模型 ，让语音识别不再只「听声音」，而是能够理解所处的教学上下文。 在此基础上，团队进一步自研了 声纹降噪模型 ，可以将学员和家长说话的声音区分开。 事实证明，凭借「上下文理解+声纹降噪」，ASR识别效果有了质的飞跃：句准确率从行业内开放API的80%左右的最好效果，大幅度提升至 95%以上 ，接近真人理解识别水平。 听清楚学员的问题，思考完毕，下面就该导师开口指点迷津了。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/92bb503675ad290e9bd9d71daf15e8b4.png] 目前，行业主流语音合成架构基本都是LLM或者LLM+Flow/Diffusion的方案。 真用到课堂里，会暴露出三个问题：人机味明显、不像在上课、不支持双向实时交互。 下面看看，与爱为舞是如何迈过这三道坎的。 先来最直观的—— 人机感 。 在底层架构上，团队采用了LLM+Flow方案，引入了两类speech token：一类负责声音本身的细节，一类负责语义和表达节奏。 在此基础上，结合强化学习，可以让AI学会正常说话应有的抑扬顿挫。 不过，光会说话可不行，老师上课得有个「老师」的样。 为此，团队拿出了大量真实课堂数据，对不同学科、不同导师的讲课方式进行了建模：有的导师说话像机关枪，有的导师则更慢条斯理。 落地时，团队还会为每位主讲名师单独设计录制脚本。这样，数据收集效率更高，还能最大程度还原名师声线，保证声音的「质感」。 具体效果如何嘛，我们可以一起听听下面这两段音频。 （文本：接下来我们看这个题，图中表示水蒸气直接变成冰的过程） 这是第三方TTS，不仅表现力较弱，还出现了发音错误，如果是上课很容易出戏。 相比起来，这段是不是「活人感」足了很多？ 这正是自研模型的优势，发音更自然，更稳定，情感表现也更好。 至于 双向实时交互 ，AI导师需要边说话边理解学员是否在主动打断询问导师问题，并且做出及时的响应，这是AI导师智能与否最重要的能力之一。 为此，团队研发 流式语义VAD和打断模型 ，能够让AI导师实时识别学员是否有真实打断意图，识别准确度可以达到90%以上。 而为了让AI导师真正「站上讲台」，团队还为其配套设计了逼真的数字人形象：口型、面部表情与肢体动作高度同步，且支持实时互动。 这下，AI导师可算是凑齐了自己的莲藕肉身三件套——「耳朵+嘴巴+身体」。 当AI开始具备人的温度，信任才有可能建立，学员也更不容易分心。 百万AI学习原住民 话说回来，即便「大脑、耳朵、嘴巴」全部补齐，我们依然无法解释与爱为舞是如何实现规模化落地的。 毕竟，从语音识别，到模型思考，再到语音合成，最后还要驱动真人级数字人，这条服务链路相当长。 任何一个环节稍有迟滞，都会严重影响学员的课堂体验。 而当用户规模放大，「千人千面」会带来更高频的推理请求，一旦调度或资源分配稍有不慎，服务质量会迅速下滑。 想要实现大规模落地，AI导师还需要一颗能持续供血、且足够强健的「心脏」。 首先，得把这条冗长的服务链疏通，保证「血管」里不堵。 在《思考，快与慢》中，Daniel Kahneman提出，大脑为了偷懒，演化出了两套工作模式： 靠直觉行事的「系统一」、调用认知资源的「系统二」 。 与爱为舞借鉴的，正是这一点。 当学员开口提问时，系统不会一股脑把问题全丢给大模型，而是先做一次判断： 能马上回答的，直接走快速通道；真正需要推理的，再交给大模型慢慢想。 具体而言，简单问题会先由快速回答系统给出反馈；与此同时，大模型已经在后台并行启动。等学员听完前半句，模型的「思考」也完成了一大半。 于是，模型回复的延迟可压缩到 100ms 以内，整条响应链路稳定在 1–1.5秒 。 同理，如果学员在导师讲话时突然插话，AI也不会傻等学员全部说完再思考。而是立刻结合上下文判断学员的意图，提前开始构思。 这样响应时间仍可控制在 100–200ms ，整条链路不超过 1.6秒 。 当然，遇到一些开放式问题，确实要多想一会儿。 但即便如此，AI导师也不会「卡住不动」，而是通过表情变化、过渡性话语告诉学员： 我在想，你稍等 。而不是空气突然安静，一人一AI面面相觑。 血管疏通之后，还可以通过「提前缓存」，让血液循环得更顺畅一些。 在真实教学中，同一堂课的核心知识点其实相对固定。哪怕学员的具体问题不同，总体来看仍有一定规律可循。 先从 输入 说起。 大模型在生成答案前，要先「读懂问题」（prefill），再「组织回答」（decode）。而前者非常吃算力，并且很耗时间。 团队的做法是，把Prompt结构化：在不影响回答质量的前提下，把同一类场景里老是出现的内容集中起来，从而让AI少做重复阅读。 再看 输出 。 学员千差万别，但在具体知识点上，很多人其实都是在同一个地方「栽跟头」。既然如此，AI导师就没必要每次都从头生成一整套讲解。 因此，团队会以题目、引导方式和学员回答作为索引，把模型的讲解结果先存下来。一旦再次遇到相同情形，直接拿来用就好。 通过这套「链路优化+缓存」的组合拳，与爱为舞将整个流程控制在了1s-1.6s之间。 筋骨与脉络就位，接下来，该让心脏泵得更有力了，与爱为舞在 大规模并发上 也做了大量工作。 首先在单机上 ，为了榨干每一张GPU，团队在系统设计之初就完成了显存地址的统一规划，全程实现显存共享，尽量避免数据在不同计算与存储介质间反复搬运所带来的性能损耗。 与此同时，在GPU算子层面，团队又针对核心计算路径进行了专项加速，使单卡的有效吞吐能力提升约5倍，足以支撑起几十路真人级数字人的推理。 其次在集群上 ，资源的调度能力同样至关重要。团队又从五个层面，对整体系统做了进一步加固： 多数字人统一调度：同一个资源池中不同形象统一调度，从而更好的复用集群资源； 系统抽象：对话轮次化、课节内容组件化、知识点任务化，让复杂流程标准化； 并行计算：尽量不浪费任何空闲算力，AI导师还在讲上一题时，下一题的计算已经在后台悄然启动； 预留容量：服务支持横向扩容，不同层级配有多种缓存与缓冲机制，一层层削薄高峰流量，避免高并发请求同时压向模型与数据库； 保险机制：整个教学调度过程可恢复，即便遭遇网络中断或客户端异常退出，教学状态也不会丢失。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/7e920b299052dcdc14a3b67c25c40b64.png] 凭借一台全速运转的 AI发动机 ，加上一张巨大的 工程降落伞 ，与爱为舞得以把AI导师「空投」到全国各地，成为业界首个支持万人并发的真人级AI教学系统。 归根结底，与爱为舞从未将AI视作一个简单的辅助工具。 在他们看来，比起技术升级，AI更像一场关于个体工作逻辑与组织管理范式的深层重塑。 回头看今天的企业形态，其实很多都是工业时代的妥协产物：人的精力有限，只能把分工越拆越细，组织层级上层层加码。 一道道庞大的部门墙，虽防止了团队混乱，但也淹没了许多人才的主观能动性。 AI的出现，第一次让生产力得到完全释放，每个人都能担任「架构师」。 在此背景下，与爱为舞提出「全员皆超级个体」——只要有想法，任何人都可以手握数据与算力这两栋「粮仓」，调度一支由智能体组成的硅基军团，以极低的成本，快速实现抢跑。 [图片: https://i.qbitai.com/wp-content/uploads/replace/2025/12/f2fb2e5d79cf34d47c9daf4ca2ed469a.png] 而这一理念，也已在产品上得到验证—— 至今，「爱学」已服务 百万级用户，学员分布于全国342个城市 ：东至佳木斯，西达克孜勒苏，南抵三沙，北至大兴安岭。 关于AI原生的企业理念，市场已经给出了自己的判断。 而当AI真正开始惠及百万学员，我们或许终于有机会，兑现孔夫子两千多年前所期待的那个美好愿景——「有教无类、因材施教」。 版权所有，未经授权不得以任何形式转载及使用，违者必究。","published_date":"2025-12-30T04:25:46.682Z","authors":"量子位","source":"量子位 - 资讯 - 量子位","details":{"content_html":"<h1>千人千面的真人级AI名师，劈开教育「不可能三角」</h1>\n       <div>\n             <span><img src=\"http://www.qbitai.com/wp-content/themes/liangziwei/imagesnew/head.jpg\" height=\"200\" width=\"200\"><em><a href=\"https://www.qbitai.com/author/jay\" title=\"由 Jay 发布\" target=\"_blank\">Jay</a></em></span>\n                          <span>2025-12-30</span>\n             <span>12:25:46</span>\n          <span>\n          来源：<a href=\"https://www.qbitai.com/\" target=\"_blank\">量子位</a>            </span></div>\n                          \n            <div><p>百万AI学习原住民已入驻</p>\n</div>                <blockquote>\n<p>Jay 发自 凹非寺量子位 | 公众号 QbitAI</p>\n</blockquote>\n<p>注意看，这是一个教育领域的AI应用新物种——</p>\n<p>咱就是说，这讲课节奏，这语气，这互动，也太自然了。</p>\n<p>更重要的是，它不仅能「像老师一样讲课」，还能针对每一位学员进行一对一的个性化教学。</p>\n<p>这位AI导师，出自一家名为「与爱为舞」的AI原生应用企业。自年初上线以来，已累计为百万级用户提供学习陪伴与一对一讲解服务。</p>\n<p>教育行业，<strong>向来是个「规模、质量、成本」的不可能三角</strong>。</p>\n<p>既能做到千人千面，又能服务百万名学员，还几乎看不出是AI……更是难上加难。</p>\n<p>它究竟是怎么做到的？</p>\n<p>与爱为舞用来劈开这个不可能三角的，是一把<strong>硬核的技术巨剑</strong>。</p>\n<h1>AI教育，要的不止「答案」</h1>\n<p>而铸造这把技术巨剑，有三块核心组成部件：「模型+语音+工程」。</p>\n<p>先看<strong>模型</strong>——</p>\n<p>得益于CoT的Scaling，大模型解决复杂问题的能力指数级增长，「做题」水平突飞猛进，甚至能斩获「奥赛金牌」。</p>\n<p>摘得奥赛桂冠，AI只需要给出标准答案。但搞教育不行。</p>\n<p><strong>先来看一个简单的英语语法题：</strong></p>\n<blockquote>\n<p>Lily expects <strong>_</strong> her grandparents in the countryside next month.</p>\n<p>A. visiting｜B. to visit｜C. to have visited｜D. having visited</p>\n</blockquote>\n<p><strong>大模型给出的解答如下：</strong></p>\n<blockquote>\n<p>这道题的正确答案是B. to visit。</p>\n<p>核心考点解析：</p>\n<p>动词固定搭配，动词 expect（期望、预料）的常用用法是——</p>\n<p>expect to do sth.（期望做某事），后面需要接不定式（to + 动词原形） 作宾语。</p>\n<p>B选项to visit是不定式的一般式，用来表示与谓语动词（expect）同时或之后发生的动作，符合 “下个月看望祖父母” 的将来时态逻辑。</p>\n<p>我可以帮你整理一份常见接不定式作宾语的动词清单，方便你记忆，需要吗？</p>\n</blockquote>\n<p>答案是对了，看起来好像也讲了解题过程，但如果拿这套方法教学员，那肯定是要被家长投诉的啊……</p>\n<p>完全没有引导学员思考，一上来就把答案透露了。就像个纯天赋型学霸，风驰电掣做完题，然后猛地丢给你一个结果，问你：「我说的对不对！」随后才给出一串看似头头是道的分析。</p>\n<p>不过，硅基大佬，小弟我根本听不明白你在说什么啊！</p>\n<p>最后还要给你一份「动词清单」，直接默认教学等于枯燥的背诵，而没去想怎样引导学员的主观能动性。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/f5c46bc41b61862be3ac6bc0674a6b6b.jpeg\"></div>\n<p>归根结底，通用大模型的设计初衷就不是教育。它拼尽全力，只想向用户证明一件事——「厉害吧，哥啥都知道！」</p>\n<p>古人讲：<strong>授人以鱼，不如授人以渔</strong>。导师如果光顾着自己拿金牌，这师生关系就乱了套了。</p>\n<p>想要成为一名好导师，AI需要学会放低姿态，真正关心学员的课堂体验。</p>\n<p>首先，AI得明白各学科的核心知识图谱、关键考点和常见解题方法，这些才是学员能服用的，是最基本的「知」。</p>\n<p>在此之上，AI还得学习名师是怎么设计讲解顺序的，并从中总结归纳出一套顶尖教师的授课方法论。这是更高维度的「知」。</p>\n<p><strong>陆游讲，「纸上得来终觉浅，绝知此事要躬行。」</strong></p>\n<p>「知」总是相对容易的，重点是如何把纸上谈兵那套，搬到现实世界里实践起来。</p>\n<p>所幸，「行」方面，与爱为舞有相当充足的弹药。</p>\n<p>据悉，他们已积累了约百万小时的音视频互动数据，特别是包含大量业内TOP级名师的授课视频。</p>\n<p>在此基础上，团队又根据学员的认知水平与学习态度，构建出多类型的「虚拟学员」，让他们与AI导师进行「搏击」，每周又能收获<strong>数万小时的合成数据</strong>。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/e61557505c960f0d388ec4cd9fe06488.gif\"></div>\n<p>这些数据在经过筛选与清洗后，会交由专业教研进行把关。</p>\n<p>具体而言，<strong>教师们会把自己多年的「教学经验」，根据场景具象化为一条条思维链</strong>，最终汇集成一本「好老师红宝书」：</p>\n<p>每个知识点该如何拆解，与学员互动时如何循循善诱……不止要让AI学会怎么讲课，更要明白「为什么要这么讲」。</p>\n<p>这种手把手教的方式效果很好，但成本也相当高。</p>\n<p>随着方法论逐渐成熟，团队索性将这一环节也自动化，让AI模仿专业教研参与数据标注。</p>\n<p>备考资料准备就绪，下面就该着手训练了。</p>\n<p><strong>第一步，照猫画虎。</strong></p>\n<p>那些相对容易标准化的知识，已体现在标注数据之中。AI需要做的，是通过模仿专业教师的思维链，逐步摸索出每一个教学动作背后的真实意图。</p>\n<p>这一微调过程，能大幅降低AI「自我发挥」带来的的幻觉率，同时培养更稳定的推理能力与泛化能力。</p>\n<p>能做到这一点，就算是打牢了基本功。</p>\n<p>最基本的教法、节奏和经验都已被「固化」，能以标准化形式面向所有学员输出，教学质量的下限得到保障。</p>\n<p>但如果目标只是及格，这件事就没意义了。</p>\n<p>师傅能陪伴的路程就到这。接下来，得能靠AI自己上路修行。</p>\n<p>第二步，终于到了大家喜闻乐见的<strong>强化学习</strong>环节。</p>\n<p>在教育这个场景下，与爱为舞的奖励函数围绕教学路径规划质量、教学有效性与教学灵活性等维度设计，通过GRPO给AI做强化。</p>\n<p>这步结束，AI彻底出师——不仅能够完成授课任务，还能驾驭课堂节奏，提高趣味性，根据不同学员灵活调整教学策略。</p>\n<p>那么接下来，就该真正走进「教师资格证考场」了。</p>\n<p>不过，教育不是一个有标准答案的任务，Benchmark肯定是行不通。笔试应该如何设计？</p>\n<p>与爱为舞的做法很简单，甚至有些「粗暴」——<strong>笔试啥，直接把AI丢到讲台上</strong>，看学员的真实反应。</p>\n<p>第一步，是在<strong>模拟课堂</strong>中试水。</p>\n<p>这个课堂由多类型的模拟学员组成，团队会按照真实分布规律注入一批线上数据，再由评分模型从多个维度对AI导师打分。</p>\n<p>模拟课堂如果表现不错，AI会迎来更严苛的终极试炼场——<strong>直连真实教学一线</strong>。</p>\n<p>AI能否驾驭高度不确定的真实课堂？是否真的能摆脱照本宣科？答案，只能由学员来评判，再好的数据标注导师也帮不了。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/c7285f0af5918689d157a018d7cc1db3.png\"></div>\n<p>即便成功拿下了「教师资格证」，但教学，依然是个终身学习的过程。</p>\n<p>正式上线后，海量的学员数据会被持续建模，AI导师将基于每一位学员的专属档案库，为其定制个性化课程。</p>\n<p>至此，AI导师才算具备了千人千面的能力。不仅下限有保障，上限也很高。</p>\n<h1>「真人级」AI导师</h1>\n<p>通过「知」与「行」的双重训练，与爱为舞得以将通用大模型，塑造成一个真正懂教学的名师AI模型。</p>\n<p>然而，再聪明的模型，无法与学员真实互动，最终仍会沦为一颗「缸中之脑」。</p>\n<p>AI导师需要「耳朵」。</p>\n<p>作为导师，连学员的问题都听不清楚，最后聊的牛头不对马嘴。不仅显得导师呆若木鸡，学员的积极性也会大打折扣。</p>\n<p>但现实是，课堂不是录音棚。<strong>真实环境往往充斥着噪音</strong>，如果有电视，甚至会出现多个人声掺杂在一块的情况。</p>\n<p>即便能输入干净音频，中国有各种各样的方言，不同学员的咬字发音习惯也不同，识别难度相当高。</p>\n<p>雪上加霜的是，在传统ASR范式下，输入模型的只是一段孤立的语音，基本没什么上下文。一旦放到教学场景下，AI很容易把同音字混淆。</p>\n<p>例如，「极限」和「极线」。</p>\n<p>前者是微积分中的核心概念，后者则属于二次曲线相关的几何术语。二者在语义上截然不同，发音却完全一致，如果没有上下文，仅凭语音几乎无法区分。</p>\n<p>为解决这个问题，与爱为舞基于其长期积累的教育场景与课堂教学数据，自研了一套<strong>多模态语音理解大模型</strong>，让语音识别不再只「听声音」，而是能够理解所处的教学上下文。</p>\n<p>在此基础上，团队进一步自研了<strong>声纹降噪模型</strong>，可以将学员和家长说话的声音区分开。</p>\n<p>事实证明，凭借「上下文理解+声纹降噪」，ASR识别效果有了质的飞跃：句准确率从行业内开放API的80%左右的最好效果，大幅度提升至<strong>95%以上</strong>，接近真人理解识别水平。</p>\n<p>听清楚学员的问题，思考完毕，下面就该导师开口指点迷津了。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/92bb503675ad290e9bd9d71daf15e8b4.png\"></div>\n<p>目前，行业主流语音合成架构基本都是LLM或者LLM+Flow/Diffusion的方案。</p>\n<p>真用到课堂里，会暴露出三个问题：人机味明显、不像在上课、不支持双向实时交互。</p>\n<p>下面看看，与爱为舞是如何迈过这三道坎的。</p>\n<p>先来最直观的——<strong>人机感</strong>。</p>\n<p>在底层架构上，团队采用了LLM+Flow方案，引入了两类speech token：一类负责声音本身的细节，一类负责语义和表达节奏。</p>\n<p>在此基础上，结合强化学习，可以让AI学会正常说话应有的抑扬顿挫。</p>\n<p>不过，光会说话可不行，老师上课得有个「老师」的样。</p>\n<p>为此，团队拿出了大量真实课堂数据，对不同学科、不同导师的讲课方式进行了建模：有的导师说话像机关枪，有的导师则更慢条斯理。</p>\n<p>落地时，团队还会为每位主讲名师单独设计录制脚本。这样，数据收集效率更高，还能最大程度还原名师声线，保证声音的「质感」。</p>\n<p>具体效果如何嘛，我们可以一起听听下面这两段音频。</p>\n<p>（文本：接下来我们看这个题，图中表示水蒸气直接变成冰的过程）</p>\n<p>这是第三方TTS，不仅表现力较弱，还出现了发音错误，如果是上课很容易出戏。</p>\n<p>相比起来，这段是不是「活人感」足了很多？</p>\n<p>这正是自研模型的优势，发音更自然，更稳定，情感表现也更好。</p>\n<p>至于<strong>双向实时交互</strong>，AI导师需要边说话边理解学员是否在主动打断询问导师问题，并且做出及时的响应，这是AI导师智能与否最重要的能力之一。</p>\n<p>为此，团队研发<strong>流式语义VAD和打断模型</strong>，能够让AI导师实时识别学员是否有真实打断意图，识别准确度可以达到90%以上。</p>\n<p>而为了让AI导师真正「站上讲台」，团队还为其配套设计了逼真的数字人形象：口型、面部表情与肢体动作高度同步，且支持实时互动。</p>\n<p>这下，AI导师可算是凑齐了自己的莲藕肉身三件套——「耳朵+嘴巴+身体」。</p>\n<p>当AI开始具备人的温度，信任才有可能建立，学员也更不容易分心。</p>\n<h1>百万AI学习原住民</h1>\n<p>话说回来，即便「大脑、耳朵、嘴巴」全部补齐，我们依然无法解释与爱为舞是如何实现规模化落地的。</p>\n<p>毕竟，从语音识别，到模型思考，再到语音合成，最后还要驱动真人级数字人，这条服务链路相当长。</p>\n<p>任何一个环节稍有迟滞，都会严重影响学员的课堂体验。</p>\n<p>而当用户规模放大，「千人千面」会带来更高频的推理请求，一旦调度或资源分配稍有不慎，服务质量会迅速下滑。</p>\n<p>想要实现大规模落地，AI导师还需要一颗能持续供血、且足够强健的「心脏」。</p>\n<p>首先，得把这条冗长的服务链疏通，保证「血管」里不堵。</p>\n<p>在《思考，快与慢》中，Daniel Kahneman提出，大脑为了偷懒，演化出了两套工作模式：<strong>靠直觉行事的「系统一」、调用认知资源的「系统二」</strong>。</p>\n<p>与爱为舞借鉴的，正是这一点。</p>\n<p>当学员开口提问时，系统不会一股脑把问题全丢给大模型，而是先做一次判断：</p>\n<p>能马上回答的，直接走快速通道；真正需要推理的，再交给大模型慢慢想。</p>\n<p>具体而言，简单问题会先由快速回答系统给出反馈；与此同时，大模型已经在后台并行启动。等学员听完前半句，模型的「思考」也完成了一大半。</p>\n<p>于是，模型回复的延迟可压缩到<strong>100ms</strong>以内，整条响应链路稳定在<strong>1–1.5秒</strong>。</p>\n<p>同理，如果学员在导师讲话时突然插话，AI也不会傻等学员全部说完再思考。而是立刻结合上下文判断学员的意图，提前开始构思。</p>\n<p>这样响应时间仍可控制在<strong>100–200ms</strong>，整条链路不超过<strong>1.6秒</strong>。</p>\n<p>当然，遇到一些开放式问题，确实要多想一会儿。</p>\n<p>但即便如此，AI导师也不会「卡住不动」，而是通过表情变化、过渡性话语告诉学员：<strong>我在想，你稍等</strong>。而不是空气突然安静，一人一AI面面相觑。</p>\n<p>血管疏通之后，还可以通过「提前缓存」，让血液循环得更顺畅一些。</p>\n<p>在真实教学中，同一堂课的核心知识点其实相对固定。哪怕学员的具体问题不同，总体来看仍有一定规律可循。</p>\n<p>先从<strong>输入</strong>说起。</p>\n<p>大模型在生成答案前，要先「读懂问题」（prefill），再「组织回答」（decode）。而前者非常吃算力，并且很耗时间。</p>\n<p>团队的做法是，把Prompt结构化：在不影响回答质量的前提下，把同一类场景里老是出现的内容集中起来，从而让AI少做重复阅读。</p>\n<p>再看<strong>输出</strong>。</p>\n<p>学员千差万别，但在具体知识点上，很多人其实都是在同一个地方「栽跟头」。既然如此，AI导师就没必要每次都从头生成一整套讲解。</p>\n<p>因此，团队会以题目、引导方式和学员回答作为索引，把模型的讲解结果先存下来。一旦再次遇到相同情形，直接拿来用就好。</p>\n<p>通过这套「链路优化+缓存」的组合拳，与爱为舞将整个流程控制在了1s-1.6s之间。</p>\n<p>筋骨与脉络就位，接下来，该让心脏泵得更有力了，与爱为舞在<strong>大规模并发上</strong>也做了大量工作。</p>\n<p><strong>首先在单机上</strong>，为了榨干每一张GPU，团队在系统设计之初就完成了显存地址的统一规划，全程实现显存共享，尽量避免数据在不同计算与存储介质间反复搬运所带来的性能损耗。</p>\n<p>与此同时，在GPU算子层面，团队又针对核心计算路径进行了专项加速，使单卡的有效吞吐能力提升约5倍，足以支撑起几十路真人级数字人的推理。</p>\n<p><strong>其次在集群上</strong>，资源的调度能力同样至关重要。团队又从五个层面，对整体系统做了进一步加固：</p>\n<ul>\n<li><strong>多数字人统一调度：同一个资源池中不同形象统一调度，从而更好的复用集群资源；</strong></li>\n<li><strong>系统抽象：对话轮次化、课节内容组件化、知识点任务化，让复杂流程标准化；</strong></li>\n<li><strong>并行计算：尽量不浪费任何空闲算力，AI导师还在讲上一题时，下一题的计算已经在后台悄然启动；</strong></li>\n<li><strong>预留容量：服务支持横向扩容，不同层级配有多种缓存与缓冲机制，一层层削薄高峰流量，避免高并发请求同时压向模型与数据库；</strong></li>\n<li><strong>保险机制：整个教学调度过程可恢复，即便遭遇网络中断或客户端异常退出，教学状态也不会丢失。</strong></li>\n</ul>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/7e920b299052dcdc14a3b67c25c40b64.png\"></div>\n<p>凭借一台全速运转的<strong>AI发动机</strong>，加上一张巨大的<strong>工程降落伞</strong>，与爱为舞得以把AI导师「空投」到全国各地，成为业界首个支持万人并发的真人级AI教学系统。</p>\n<p>归根结底，与爱为舞从未将AI视作一个简单的辅助工具。</p>\n<p>在他们看来，比起技术升级，AI更像一场关于个体工作逻辑与组织管理范式的深层重塑。</p>\n<p>回头看今天的企业形态，其实很多都是工业时代的妥协产物：人的精力有限，只能把分工越拆越细，组织层级上层层加码。</p>\n<p>一道道庞大的部门墙，虽防止了团队混乱，但也淹没了许多人才的主观能动性。</p>\n<p>AI的出现，第一次让生产力得到完全释放，每个人都能担任「架构师」。</p>\n<p>在此背景下，与爱为舞提出「全员皆超级个体」——只要有想法，任何人都可以手握数据与算力这两栋「粮仓」，调度一支由智能体组成的硅基军团，以极低的成本，快速实现抢跑。</p>\n<div><img src=\"https://i.qbitai.com/wp-content/uploads/replace/2025/12/f2fb2e5d79cf34d47c9daf4ca2ed469a.png\"></div>\n<p>而这一理念，也已在产品上得到验证——</p>\n<p>至今，「爱学」已服务<strong>百万级用户，学员分布于全国342个城市</strong>：东至佳木斯，西达克孜勒苏，南抵三沙，北至大兴安岭。</p>\n<p>关于AI原生的企业理念，市场已经给出了自己的判断。</p>\n<p>而当AI真正开始惠及百万学员，我们或许终于有机会，兑现孔夫子两千多年前所期待的那个美好愿景——「有教无类、因材施教」。</p>\n                \n                \n                <div><span></span><em>版权所有，未经授权不得以任何形式转载及使用，违者必究。</em><span></span></div>\n            "}},{"id":"228692513282738179","type":"news","url":"https://newshacker.me/story?id=46428496","title":"🔌 39C3：洗衣机攻防与诊断接口——厂商合作、光学隔离与声学诊断","description":"原标题： 《Hacking Washing Machines (39C3) [video]》 评分: 26 | 作者: clausecker 💭 他们会因为安全把接口封死让用户无能为力吗？ 🎯 讨论背景 39C3（Chaos Communication Congress，欧洲知名黑客/安全会议）的一场演讲展示了对洗衣机等家电诊断与通信接口的安全研究并发布了视频。研究者在披露后与家电厂商 BSH 与 Miele 主动通话确认细节，体现负责任披露与厂商合作的流程。讨论集中在 Miele 使用的 optical communication（光学通信，用于电气隔离以应对 mains voltage 的风险）与 LG 早期采用的 acoustic diagnostics（通过调制音、可能为 4-FSK）的实际应用与逆向难度。评论还把话题延伸到传统的 blink code/数字错误显示与厂商通过手机或专有通道绑定诊断服务对维修可达性的影响。 📌 讨论焦点 厂商响应与负责任披露 评论指出 BSH 和 Miele 主动与研究者通话确认细节，沟通被描述为建设性且对双方都有益，这被视为负责任披露的正面例子。厂商参与通话表明在公开漏洞前有机会消除误解或确认真实风险，从而降低误报或误判的可能性。与此同时也有担忧，部分人害怕厂商在收到报告后会采取更封闭的硬件策略以“加强安全”，反而损害维修与审计的可及性。总体上评论既肯定了合作，也表达了对后续厂商策略的警惕。 [来源1] 光学通信与电气隔离 针对 Miele 使用的 optical communication，回复强调其主要目的是实现 electrical isolation，因为洗衣机内部存在 mains voltage，直接电气连接会带来短路或绝缘失效风险。光学链路通过光信号断开电流回路，降低因电气故障造成的人身或设备危险，这比单纯防潮更重要。评论中有人因此认为光学方案在有高压或需要严格隔离的家电场景尤其合理，同时也引发对其它设备是否采用相同设计的好奇。讨论把技术动机从“防腐蚀”调整到“安全隔离”，并探讨可靠性与实现成本的权衡。 [来源1] [来源2] 声学诊断实现与可逆向性（LG 案例） 多条回复介绍了 LG 早期家电用 acoustic signaling 做诊断：将手机靠近设备，洗衣机会发出类似 modem 的调制音，评论中提到可能是 4-tone / 4-FSK 的方案。该方法被设计为能在语音 codecs 上工作，因此即使没有智能手机，用户也能通过电话把诊断音频中继给技术人员，增加可达性。有用户找到了 GitHub 上的实现笔记（kabelincho/LG-Smart-Diagnostics-modem）并贴出 YouTube 示例音频，但公开完整逆向出的 bitstream 很少见或不存在。总体观点是技术上可行且并非复杂，但公开资料多为示例音频和零散实现，完整协议仍有限。 [来源1] [来源2] [来源3] [来源4] 维修可达性与厂商绑定（gatekeeping） 有人批评将诊断绑在手机/专属通道上是一种“advanced gatekeeping”，因为传统上设备会有 blink code 或数字错误显示（例如 Miele 的数位错误码）供用户或第三方查表维修。把诊断流程依赖于专有声码或厂商 app 会把用户和维修人员锁定在厂商生态，降低独立维修与可维修性。评论表达担忧：即便出于安全考虑，这类设计也可能被用作限制第三方访问诊断信息的借口，从而提高维修成本与门槛。讨论里有人希望厂商合作但不要以封闭接口作为默认回应。 [来源1] [来源2] 其它物理信号通道的类比（电表 LED 脉冲） 有评论举例电能表常用的 LED 脉冲作为类比：电表每消耗 X kWh 会闪烁一次 LED，外部设备可以计数这些脉冲用于能耗监测。该类 LED/blink 信号通常是单向的，用于计数或简单监测，而不是复杂的双向通信，这表明不同场景会采用不同的物理层方案。该例子用来提醒大家——家电诊断与计量可以采用多种非传统接口，选择背后既有可靠性与安全考量，也有互操作性和可维护性的权衡。 [来源1] 📚 术语解释 optical communication / optical isolation: 通过光学信号而非电连接传输数据，从而实现电气隔离（electrical isolation）；在包含 mains voltage（主电源电压）的家电中可减少短路或绝缘不良带来的风险，同时降低金属端口被潮气腐蚀的直接影响。 4-FSK: four-level frequency-shift keying，一种频移键控调制方式，用四个不同频率表示四种符号，适合在音频或语音编解码信道上传输低速诊断数据，能在电话线路或受限带宽下保持鲁棒性。 acoustic diagnostics（声学诊断 / acoustic signaling）: 设备通过扬声器输出调制的音频信号传递诊断信息，这类信号可被手机、电话或专用接收器录入并解析，优势是兼容语音编解码、无需专用硬件，但也可能把诊断流程绑定到厂商生态。 blink code / LED 脉冲: 设备或电表通过 LED 闪烁或脉冲输出基础信息（例如每消耗一定能量闪烁一次），外部设备或人工可通过计数或查表解读这些信号，通常为单向的简洁诊断或计量接口。 类别： Security | Hardware | Systems | Video | Incident | washing machines | 39C3 | LG | acoustic diagnostics | LG Smart Diagnostics | optical communication | Miele","published_date":"2025-12-30T04:21:53.373Z","authors":"","source":"News Hacker | 极客洞察","details":{"content_html":"<p><strong>原标题：</strong>《Hacking Washing Machines (39C3) [video]》</p><p><strong>评分:</strong> 26 | <strong>作者:</strong> clausecker</p><blockquote>💭 他们会因为安全把接口封死让用户无能为力吗？</blockquote><hr><h2>🎯 讨论背景</h2><p>39C3（Chaos Communication Congress，欧洲知名黑客/安全会议）的一场演讲展示了对洗衣机等家电诊断与通信接口的安全研究并发布了视频。研究者在披露后与家电厂商 BSH 与 Miele 主动通话确认细节，体现负责任披露与厂商合作的流程。讨论集中在 Miele 使用的 optical communication（光学通信，用于电气隔离以应对 mains voltage 的风险）与 LG 早期采用的 acoustic diagnostics（通过调制音、可能为 4-FSK）的实际应用与逆向难度。评论还把话题延伸到传统的 blink code/数字错误显示与厂商通过手机或专有通道绑定诊断服务对维修可达性的影响。</p><hr><h2>📌 讨论焦点</h2><h3>厂商响应与负责任披露</h3><p>评论指出 BSH 和 Miele 主动与研究者通话确认细节，沟通被描述为建设性且对双方都有益，这被视为负责任披露的正面例子。厂商参与通话表明在公开漏洞前有机会消除误解或确认真实风险，从而降低误报或误判的可能性。与此同时也有担忧，部分人害怕厂商在收到报告后会采取更封闭的硬件策略以“加强安全”，反而损害维修与审计的可及性。总体上评论既肯定了合作，也表达了对后续厂商策略的警惕。</p><p><a href=\"https://news.ycombinator.com/item?id=46428854\" target=\"_blank\">[来源1]</a></p><h3>光学通信与电气隔离</h3><p>针对 Miele 使用的 optical communication，回复强调其主要目的是实现 electrical isolation，因为洗衣机内部存在 mains voltage，直接电气连接会带来短路或绝缘失效风险。光学链路通过光信号断开电流回路，降低因电气故障造成的人身或设备危险，这比单纯防潮更重要。评论中有人因此认为光学方案在有高压或需要严格隔离的家电场景尤其合理，同时也引发对其它设备是否采用相同设计的好奇。讨论把技术动机从“防腐蚀”调整到“安全隔离”，并探讨可靠性与实现成本的权衡。</p><p><a href=\"https://news.ycombinator.com/item?id=46428963\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46428854\" target=\"_blank\">[来源2]</a></p><h3>声学诊断实现与可逆向性（LG 案例）</h3><p>多条回复介绍了 LG 早期家电用 acoustic signaling 做诊断：将手机靠近设备，洗衣机会发出类似 modem 的调制音，评论中提到可能是 4-tone / 4-FSK 的方案。该方法被设计为能在语音 codecs 上工作，因此即使没有智能手机，用户也能通过电话把诊断音频中继给技术人员，增加可达性。有用户找到了 GitHub 上的实现笔记（kabelincho/LG-Smart-Diagnostics-modem）并贴出 YouTube 示例音频，但公开完整逆向出的 bitstream 很少见或不存在。总体观点是技术上可行且并非复杂，但公开资料多为示例音频和零散实现，完整协议仍有限。</p><p><a href=\"https://news.ycombinator.com/item?id=46429089\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46429209\" target=\"_blank\">[来源2]</a> <a href=\"https://news.ycombinator.com/item?id=46429240\" target=\"_blank\">[来源3]</a> <a href=\"https://news.ycombinator.com/item?id=46429335\" target=\"_blank\">[来源4]</a></p><h3>维修可达性与厂商绑定（gatekeeping）</h3><p>有人批评将诊断绑在手机/专属通道上是一种“advanced gatekeeping”，因为传统上设备会有 blink code 或数字错误显示（例如 Miele 的数位错误码）供用户或第三方查表维修。把诊断流程依赖于专有声码或厂商 app 会把用户和维修人员锁定在厂商生态，降低独立维修与可维修性。评论表达担忧：即便出于安全考虑，这类设计也可能被用作限制第三方访问诊断信息的借口，从而提高维修成本与门槛。讨论里有人希望厂商合作但不要以封闭接口作为默认回应。</p><p><a href=\"https://news.ycombinator.com/item?id=46429225\" target=\"_blank\">[来源1]</a> <a href=\"https://news.ycombinator.com/item?id=46428854\" target=\"_blank\">[来源2]</a></p><h3>其它物理信号通道的类比（电表 LED 脉冲）</h3><p>有评论举例电能表常用的 LED 脉冲作为类比：电表每消耗 X kWh 会闪烁一次 LED，外部设备可以计数这些脉冲用于能耗监测。该类 LED/blink 信号通常是单向的，用于计数或简单监测，而不是复杂的双向通信，这表明不同场景会采用不同的物理层方案。该例子用来提醒大家——家电诊断与计量可以采用多种非传统接口，选择背后既有可靠性与安全考量，也有互操作性和可维护性的权衡。</p><p><a href=\"https://news.ycombinator.com/item?id=46429412\" target=\"_blank\">[来源1]</a></p><hr><h2>📚 术语解释</h2><p><strong>optical communication / optical isolation:</strong> 通过光学信号而非电连接传输数据，从而实现电气隔离（electrical isolation）；在包含 mains voltage（主电源电压）的家电中可减少短路或绝缘不良带来的风险，同时降低金属端口被潮气腐蚀的直接影响。</p><p><strong>4-FSK:</strong> four-level frequency-shift keying，一种频移键控调制方式，用四个不同频率表示四种符号，适合在音频或语音编解码信道上传输低速诊断数据，能在电话线路或受限带宽下保持鲁棒性。</p><p><strong>acoustic diagnostics（声学诊断 / acoustic signaling）:</strong> 设备通过扬声器输出调制的音频信号传递诊断信息，这类信号可被手机、电话或专用接收器录入并解析，优势是兼容语音编解码、无需专用硬件，但也可能把诊断流程绑定到厂商生态。</p><p><strong>blink code / LED 脉冲:</strong> 设备或电表通过 LED 闪烁或脉冲输出基础信息（例如每消耗一定能量闪烁一次），外部设备或人工可通过计数或查表解读这些信号，通常为单向的简洁诊断或计量接口。</p><hr><p><strong>类别：</strong>Security | Hardware | Systems | Video | Incident | washing machines | 39C3 | LG | acoustic diagnostics | LG Smart Diagnostics | optical communication | Miele</p>"}},{"id":"228683941501184000","type":"news","url":"https://www.jiqizhixin.com/articles/2025-12-30-5","title":"Manus被收购，智谱也定了8天后上市","description":"[图片: https://image.jiqizhixin.com/uploads/editor/ba352966-44ab-406e-9c07-fd783c62cd08/1767067888960.png]AI 大新闻，一桩接一桩。 早上刚传来 Manus 被 Meta 收购的消息，很快，围绕「全球大模型第一股」的竞速，也传来靴子落地的声响。 12 月 30 日，北京智谱华章科技股份有限公司（以下简称「智谱」）正式启动港股招股。招股期将持续至 2026 年 1 月 5 日，并计划于 2026 年 1 月 8 日以股票代码 “2513” 在香港联交所主板挂牌上市。 根据招股安排，智谱拟进行全球发售 3741.95 万股 H 股，其中香港公开发售 187.1 万股 H 股，国际发售 3554.85 万股 H 股。 IPO 的定价与募资规模也随之揭晓 —— 每股发行价定为 116.20 港元。在扣除相关发行费用后，预计本次募资规模约 43 亿港元，对应的 IPO 市值预计将超过 511 亿港元。 公开信息显示，智谱在私募市场的累计融资额已达 83.44 亿元，最新估值攀升至 243.77 亿元。这意味着，在迈向上市的关键一跃中，智谱的市值几乎实现翻倍，如此幅度的「溢价上市」，也是一次难度不低的市场挑战。 基石投资者阵容同样颇为亮眼。公告显示，基石投资者合计拟认购 29.8 亿港元，占本次发行规模近七成（假设超额配股权未获行使）。 参与基石认购的机构包括： JSC International Investment Fund SPC、JinYi Capital Multi-Strategy Fund SPC、Perseveranc Asset Management、上海高毅资产管理、WT Asset Management、泰康人寿、广发基金、3W Fund Management 等 11 家投资机构。 在当前港股科技资产整体承压的背景下，如此高比例的基石认购，也为这场围绕「全球大模型第一股」的竞速，写下了更为明确的市场注脚。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/b65e1e23-4547-439f-83ba-f1e133693e4d/640.png] 烧钱还在继续，大模型开始走向资本市场 放眼行业内部，2024 年曾被热捧的 AI 大模型创业阵营「六小虎」，已经出现明显分化：两家选择主动退出基座模型竞争，转而聚焦垂直应用。 其余四家 —— 智谱、MiniMax、月之暗面与阶跃星辰 —— 仍试图留在大模型这张牌桌之上。2024 年 12 月中下旬，智谱与 MiniMax 先后披露港股招股书。 与 MiniMax 专注 to C 不同，智谱主要专注企业级方案（to B），已落地金融服务、互联网、智能设备、医疗等行业。 今年上半年智谱收入为 1.91 亿元，期内亏损高达 23.58 亿元，AI 研发成本高达 15.95 亿元。 如果说，2023 年一级市场给予大模型创业公司的高估值，更多是押注宏大的技术叙事。那么进入 2024—2025 年，市场开始更明确地转向模型能力与商业化兑现路径。 即便是头部公司，也难以绕开对基座模型的持续投入，大模型创业公司也要直面能否持续推进模型迭代、探索应用场景落地的挑战。 而这些，在很大程度上，取决于资本市场是否愿意提供长期、稳定的资金支持。 今年 4 月，智谱曾在证监会北京监管局开启 A 股上市辅导备案。但截至 12 月 12 日，公司并未收到中国证监会关于推进 A 股上市的进一步意见或问询。 在此背景下，智谱选择转向港股，为这场高投入、长周期的大模型竞赛寻找更可持续的燃料。同时，也将直面融资能力与市场信心的双重考验 —— 是否有人愿意为 AI 的长期投入买单。 从 GLM 到 MaaS：智谱的大模型技术底座与商业化路径 招股书显示，智谱主要提供从算力、API 接口到 MaaS（模型即服务）的服务，支持本地和云端两种部署模式，已落地多个行业。 作为国内从事通用语言模型研究与产业化的代表性公司之一，智谱技术体系以 GLM 为核心，覆盖文本、多模态与面向应用的模型服务。 GLM 属于基于 Transformer 的大语言模型建模范式，通过将自回归生成与掩码预测相结合，实现对理解类与生成类任务的统一建模。该架构最早由智谱与清华大学相关研究团队提出，并在后续模型中持续迭代。 2021 年，智谱发布中国首个专有预训练大模型框架 GLM，并推出了模型即服务（MaaS）的产品开发与商业化平台，通过该平台向外部提供大模型能力与服务。 2022 年智谱发布并开源 GLM-130B（中英双语千亿参数模型），该模型的推出标志着智谱正式将 GLM 体系运用于预训练大语言模型之上。 2024 年 1 月，GLM 系列迎来重要节点，GLM-4 上线，支持更长的上下文，同时推理速度更快，大大降低推理成本。 2025 年 7 月，智谱进一步开源 GLM-4.5。该模型首发 48 小时内，登顶 Hugging Face（全球最大的开源模型平台）热门榜全球第一。 同年 9 月，智谱发布并开源 GLM-4.6，作为基座模型的进一步升级版本，GLM-4.6 主要强化了编码能力。11 月，GLM-4.6 在 CodeArena 上位列全球第一。 12 月，智谱推出最新旗舰模型 GLM-4.7： 在核心编码方面，相较前一代 GLM-4.6，GLM-4.7 在多语言智能体编程与基于终端的任务上取得了明显提升，SWE-bench 73.8%（+5.8%）、SWE-bench Multilingual 66.7%（+12.9%）。 氛围编程：GLM-4.7 在 UI 生成质量上实现了重要跃升，能够生成更加简洁、现代化的网页界面，并在演示文稿生成方面提供更准确的布局与尺寸控制，整体视觉效果更佳。 工具调用：GLM-4.7 的工具使用能力显著提升，在 BrowseComp 所覆盖的网页浏览任务中展现出更强的实际操作能力。 复杂推理上：在 HLE（Humanity’s Last Exam） 基准测试中取得 42.8% 的成绩，相比 GLM-4.6 提升 12.4 个百分点。 [图片: 图片 https://image.jiqizhixin.com/uploads/editor/afc64f4d-b8e5-4e96-a688-cc378c011788/640.png] 与 GPT-5、GPT-5.1-High、Claude Sonnet 4.5、Gemini 3.0 Pro、DeepSeek-V3.2、Kimi K2 Thinking 相比，GLM-4.7 也表现出色： [图片: 图片 https://image.jiqizhixin.com/uploads/editor/93e23209-b692-4f11-96e8-458ff2751799/640.png] 与此同时，智谱还发布了面向不同功能的多模态模型，包括 CogView（图像生成）、GLM-4.5V（视觉理解与推理）、CogVideoX（视频生成）等。 在 AI Agent 方面，智谱基座智能体模型为 AutoGLM。12 月智谱将 AutoGLM 的核心模型全面开源，标志着 AutoGLM 在开放生态中的进一步发展。 截至 2025 年 6 月 30 日，智谱模型已为超过 8000 家机构客户提供支持；截至最后实际可行日期，已为约 8000 万台设备提供支持。 在商业化方面，智谱从 2021 年就开始布局 MaaS 的商业模式。 MaaS 平台主要提供四类模型能力，主要覆盖语言模型、多模态模型、智能体模型和代码模型四类核心模型能力，并同时提供支持模型微调、模型部署及智能体开发的一体化工具链。 从模型能力的扩展、智能体技术的推进，到 MaaS 商业化体系的逐步成型，智谱已经完成了一轮相对完整的技术与产品布局。 但靴子落地，并不意味着终局已定。随着走向公开市场，高强度的研发投入、不断攀升的算力成本，以及通用大模型商业化路径尚未完全跑通的现实，也被一并置于更透明的审视之下。 上市不是终点，而是一场更长周期的公开测试。 ]]>","published_date":"2025-12-30T04:12:45.359Z","authors":"机器之心","source":"机器之心 - 机器之心","details":{"content_html":"<img src=\"https://image.jiqizhixin.com/uploads/editor/ba352966-44ab-406e-9c07-fd783c62cd08/1767067888960.png\" style=\"width: 700%;\">AI 大新闻，一桩接一桩。<p></p><p>早上刚传来 <a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&#x26;mid=2651009685&#x26;idx=1&#x26;sn=bfc44e95b106534d25b4002c3e701ee3&#x26;scene=21#wechat_redirect\" target=\"_blank\">Manus </a>被 Meta 收购的消息，很快，围绕「全球大模型第一股」的竞速，也传来靴子落地的声响。</p><p>12 月 30 日，北京智谱华章科技股份有限公司（以下简称「智谱」）正式启动港股招股。招股期将持续至 2026 年 1 月 5 日，并计划于 2026 年 1 月 8 日以股票代码 “2513” 在香港联交所主板挂牌上市。</p><p>根据招股安排，智谱拟进行全球发售 3741.95 万股 H 股，其中香港公开发售 187.1 万股 H 股，国际发售 3554.85 万股 H 股。</p><p>IPO 的定价与募资规模也随之揭晓 —— 每股发行价定为 116.20 港元。在扣除相关发行费用后，预计本次募资规模约 43 亿港元，对应的 IPO 市值预计将超过 511 亿港元。</p><p>公开信息显示，智谱在私募市场的累计融资额已达 83.44 亿元，最新估值攀升至 243.77 亿元。这意味着，在迈向上市的关键一跃中，智谱的市值几乎实现翻倍，如此幅度的「溢价上市」，也是一次难度不低的市场挑战。</p><p>基石投资者阵容同样颇为亮眼。公告显示，基石投资者合计拟认购 29.8 亿港元，占本次发行规模近七成（假设超额配股权未获行使）。</p><p>参与基石认购的机构包括： JSC International Investment Fund SPC、JinYi Capital Multi-Strategy Fund SPC、Perseveranc Asset Management、上海高毅资产管理、WT Asset Management、泰康人寿、广发基金、3W Fund Management 等 11 家投资机构。</p><p>在当前港股科技资产整体承压的背景下，如此高比例的基石认购，也为这场围绕「全球大模型第一股」的竞速，写下了更为明确的市场注脚。</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/b65e1e23-4547-439f-83ba-f1e133693e4d/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p><strong>烧钱还在继续，大模型开始走向资本市场</strong></p><p>放眼行业内部，2024 年曾被热捧的 AI 大模型创业阵营「六小虎」，已经出现明显分化：两家选择主动退出基座模型竞争，转而聚焦垂直应用。</p><p>其余四家 —— 智谱、MiniMax、月之暗面与阶跃星辰 —— 仍试图留在大模型这张牌桌之上。2024 年 12 月中下旬，智谱与 MiniMax 先后披露港股招股书。</p><p>与 MiniMax 专注 to C 不同，智谱主要专注企业级方案（to B），已落地金融服务、互联网、智能设备、医疗等行业。</p><p>今年上半年智谱收入为 1.91 亿元，期内亏损高达 23.58 亿元，AI 研发成本高达 15.95 亿元。</p><p>如果说，2023 年一级市场给予大模型创业公司的高估值，更多是押注宏大的技术叙事。那么进入 2024—2025 年，市场开始更明确地转向模型能力与商业化兑现路径。</p><p>即便是头部公司，也难以绕开对基座模型的持续投入，大模型创业公司也要直面能否持续推进模型迭代、探索应用场景落地的挑战。</p><p>而这些，在很大程度上，取决于资本市场是否愿意提供长期、稳定的资金支持。</p><p>今年 4 月，智谱曾在证监会北京监管局开启 A 股上市辅导备案。但截至 12 月 12 日，公司并未收到中国证监会关于推进 A 股上市的进一步意见或问询。</p><p>在此背景下，智谱选择转向港股，为这场高投入、长周期的大模型竞赛寻找更可持续的燃料。同时，也将直面融资能力与市场信心的双重考验 —— 是否有人愿意为 AI 的长期投入买单。</p><p><strong>从 GLM 到 MaaS：智谱的大模型技术底座与商业化路径</strong></p><p>招股书显示，智谱主要提供从算力、API 接口到 MaaS（模型即服务）的服务，支持本地和云端两种部署模式，已落地多个行业。</p><p>作为国内从事通用语言模型研究与产业化的代表性公司之一，智谱技术体系以 GLM 为核心，覆盖文本、多模态与面向应用的模型服务。</p><p>GLM 属于基于 Transformer 的大语言模型建模范式，通过将自回归生成与掩码预测相结合，实现对理解类与生成类任务的统一建模。该架构最早由智谱与清华大学相关研究团队提出，并在后续模型中持续迭代。</p><p>2021 年，智谱发布中国首个专有预训练大模型框架 GLM，并推出了模型即服务（MaaS）的产品开发与商业化平台，通过该平台向外部提供大模型能力与服务。</p><p>2022 年智谱发布并开源 GLM-130B（中英双语千亿参数模型），该模型的推出标志着智谱正式将 GLM 体系运用于预训练大语言模型之上。</p><p>2024 年 1 月，GLM 系列迎来重要节点，GLM-4 上线，支持更长的上下文，同时推理速度更快，大大降低推理成本。</p><p>2025 年 7 月，智谱进一步开源 GLM-4.5。该模型首发 48 小时内，登顶 Hugging Face（全球最大的开源模型平台）热门榜全球第一。</p><p>同年 9 月，智谱发布并开源 GLM-4.6，作为基座模型的进一步升级版本，GLM-4.6 主要强化了编码能力。11 月，GLM-4.6 在 CodeArena 上位列全球第一。</p><p>12 月，智谱推出最新旗舰模型 GLM-4.7：</p><ul><li><p>在核心编码方面，相较前一代 GLM-4.6，GLM-4.7 在多语言智能体编程与基于终端的任务上取得了明显提升，SWE-bench 73.8%（+5.8%）、SWE-bench Multilingual 66.7%（+12.9%）。</p></li><li><p>氛围编程：GLM-4.7 在 UI 生成质量上实现了重要跃升，能够生成更加简洁、现代化的网页界面，并在演示文稿生成方面提供更准确的布局与尺寸控制，整体视觉效果更佳。</p></li><li><p>工具调用：GLM-4.7 的工具使用能力显著提升，在 BrowseComp 所覆盖的网页浏览任务中展现出更强的实际操作能力。</p></li><li><p>复杂推理上：在 HLE（Humanity’s Last Exam） 基准测试中取得 42.8% 的成绩，相比 GLM-4.6 提升 12.4 个百分点。</p></li></ul><section><img src=\"https://image.jiqizhixin.com/uploads/editor/afc64f4d-b8e5-4e96-a688-cc378c011788/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p>与 GPT-5、GPT-5.1-High、Claude Sonnet 4.5、Gemini 3.0 Pro、DeepSeek-V3.2、Kimi K2 Thinking 相比，GLM-4.7 也表现出色：</p><section><img src=\"https://image.jiqizhixin.com/uploads/editor/93e23209-b692-4f11-96e8-458ff2751799/640.png\" alt=\"图片\" style=\"width: 700%;\"></section><p>与此同时，智谱还发布了面向不同功能的多模态模型，包括 CogView（图像生成）、GLM-4.5V（视觉理解与推理）、CogVideoX（视频生成）等。</p><p>在 AI Agent 方面，智谱基座智能体模型为 AutoGLM。12 月智谱将 AutoGLM 的核心模型全面开源，标志着 AutoGLM 在开放生态中的进一步发展。</p><p>截至 2025 年 6 月 30 日，智谱模型已为超过 8000 家机构客户提供支持；截至最后实际可行日期，已为约 8000 万台设备提供支持。</p><p>在商业化方面，智谱从 2021 年就开始布局 MaaS 的商业模式。</p><p>MaaS 平台主要提供四类模型能力，主要覆盖语言模型、多模态模型、智能体模型和代码模型四类核心模型能力，并同时提供支持模型微调、模型部署及智能体开发的一体化工具链。</p><p>从模型能力的扩展、智能体技术的推进，到 MaaS 商业化体系的逐步成型，智谱已经完成了一轮相对完整的技术与产品布局。</p><p>但靴子落地，并不意味着终局已定。随着走向公开市场，高强度的研发投入、不断攀升的算力成本，以及通用大模型商业化路径尚未完全跑通的现实，也被一并置于更透明的审视之下。</p><p>上市不是终点，而是一场更长周期的公开测试。</p>]]>"}},{"id":"225938951532409856","type":"news","url":"https://www.telecomstechnews.com/news/5g-network-infrastructure-strategies-comparison/","title":"5G network strategies diverge: Inside AT&T, Verizon, and T-Mobile’s different technology bets","description":"When AT&#x26;T handed Microsoft the keys to its 5G core network in 2021, it signalled that building next-generation wireless infrastructure is about choosing cloud platforms, virtualisation strategies, and spectrum portfolios – decisions that will determine which operators win in the next decade’s competitive markets. America’s big three carriers deploy 5G using radically different technology stacks. ... Read more » The post 5G network strategies diverge: Inside AT&#x26;T, Verizon, and T-Mobile’s different technology bets appeared first on Telecoms Tech News .","published_date":"2025-12-22T12:10:29.587Z","authors":"Dashveenjit Kaur","source":"机器之心","details":{"content_html":"<p>When AT&#x26;T handed Microsoft the keys to its 5G core network in 2021, it signalled that building next-generation wireless infrastructure is about choosing cloud platforms, virtualisation strategies, and spectrum portfolios – decisions that will determine which operators win in the next decade’s competitive markets. America’s big three carriers deploy 5G using radically different technology stacks.<a class=\"\" href=\"https://www.telecomstechnews.com/news/5g-network-infrastructure-strategies-comparison/\" title=\"Read5G network strategies diverge: Inside AT&#x26;T, Verizon, and T-Mobile’s different technology bets\" target=\"_blank\">... Read more »</a></p>\n<p>The post <a href=\"https://www.telecomstechnews.com/news/5g-network-infrastructure-strategies-comparison/\" target=\"_blank\">5G network strategies diverge: Inside AT&#x26;T, Verizon, and T-Mobile’s different technology bets</a> appeared first on <a href=\"https://www.telecomstechnews.com\" target=\"_blank\">Telecoms Tech News</a>.</p>"}}]